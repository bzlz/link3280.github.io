<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>时间与精神的小屋</title>
  
  <subtitle>专注思考的时候，时间仿佛也静下来了</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://link3280.github.io/"/>
  <updated>2020-01-16T12:25:31.468Z</updated>
  <id>https://link3280.github.io/</id>
  
  <author>
    <name>Paul Lin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink DataStream 关联维表实战</title>
    <link href="https://link3280.github.io/2020/01/16/Flink-DataStream-%E5%85%B3%E8%81%94%E7%BB%B4%E8%A1%A8%E5%AE%9E%E6%88%98/"/>
    <id>https://link3280.github.io/2020/01/16/Flink-DataStream-关联维表实战/</id>
    <published>2020-01-16T12:24:09.000Z</published>
    <updated>2020-01-16T12:25:31.468Z</updated>
    
    <content type="html"><![CDATA[<p>上篇博客提到 Flink SQL 如何 Join 两个数据流，有读者反馈说如果不打算用 SQL 或者想自己实现底层操作，那么如何基于 DataStream API 来关联维表呢？实际上由于 Flink DataStream API 的灵活性，实现这个需求的方式是非常多样的，但是大部分用户很难在设计架构时就考虑得很全面，可能会走不少弯路。针对于此，笔者根据工作经验以及社区资源整理了用 DataStream 实现 Join 维表的常见方式，并给每种的方式优劣和适用场景给出一点可作为参考的个人观点。</p><a id="more"></a><h1 id="衡量指标"><a href="#衡量指标" class="headerlink" title="衡量指标"></a>衡量指标</h1><p>总体来讲，关联维表有三个基础的方式：实时数据库查找关联（Per-Record Reference Data Lookup）、预加载维表关联（Pre-Loading of Reference Data）和维表变更日志关联（Reference Data Change Stream），而根据实现上的优化可以衍生出多种关联方式，且这些优化还可以灵活组合产生不同效果（不过为了简单性这里不讨论同时应用多种优化的实现方式）。对于不同的关联方式，我们可以从以下 7 个关键指标来衡量（每个指标的得分将以 1-5 五档来表示）:</p><ol><li>实现简单性: 设计是否足够简单，易于迭代和维护。</li><li>吞吐量: 性能是否足够好。</li><li>维表数据的实时性: 维度表的更新是否可以立刻对作业可见。</li><li>数据库的负载: 是否对外部数据库造成较大的负载（负载越低分越高）。</li><li>内存资源占用: 是否需要大量内存来缓存维表数据（内存占用越少分越高）。</li><li>可拓展性: 在更大规模的数据下会不会出现瓶颈。</li><li>结果确定性: 在数据延迟或者数据重放情况下，是否可以得到一致的结果。</li></ol><p>和大多数架构设计一样，这三类关联方式不存在绝对的好坏，更多的是针对业务场景在各指标上的权衡取舍，因此这里的得分也仅仅是针对通用场景来说。</p><h1 id="实时数据库查找关联"><a href="#实时数据库查找关联" class="headerlink" title="实时数据库查找关联"></a>实时数据库查找关联</h1><p>实时数据库查找关联是在 DataStream API 用户函数中直接访问数据库来进行关联的方式。这种方式通常开发量最小，但一般会给数据库带来很大的压力，而且因为关联是基于 Processing Time 的，如果数据有延迟或者重放，会得到和原来不一致的数据。</p><h2 id="同步数据库查找关联"><a href="#同步数据库查找关联" class="headerlink" title="同步数据库查找关联"></a>同步数据库查找关联</h2><p>同步实时数据库查找关联是最为简单的关联方式，只需要在一个 Map 或者 FlatMap 函数中访问数据库，处理好关联逻辑后，将结果数据输出。</p><p><center><img src="/img/flink-datastream-join/img1.sync-db-lookup.png" alt="图1.同步数据库查找关联架构" title="图1.同步数据库查找关联架构"></center></p><p>这种方式的主要优点在于实现简单、不需要额外内存且维表的更新延迟很低，然而缺点也很明显: 1. 因为每条数据都需要请求一次数据库，给数据库造成的压力很大；2. 访问数据库是同步调用，导致 subtak 线程会被阻塞，影响吞吐量；3. 关联是基于 Processing Time 的，结果并不具有确定性；4. 瓶颈在数据库端，但实时计算的流量通常远大于普通数据库的设计流量，因此可拓展性比较低。</p><p><center><img src="/img/flink-datastream-join/img2.sync-db-lookup.png" alt="图2.同步数据库查找关联关键指标" title="图2.同步数据库查找关联关键指标"></center></p><p>从应用场景来说，同步数据库查找关联可以用于流量比较低的作业，但通常不是最好的选择。</p><h2 id="异步数据库查找关联"><a href="#异步数据库查找关联" class="headerlink" title="异步数据库查找关联"></a>异步数据库查找关联</h2><p>异步数据库查找关联是通过 AsyncIO[2]来访问外部数据库的方式。利用数据库提供的异步客户端，AsyncIO 可以并发地处理多个请求，很大程度上减少了对 subtask 线程的阻塞。</p><p>因为数据库请求响应时长是不确定的，可能导致后输入的数据反而先完成计算，所以 AsyncIO 提供有序和无序两种输出模式，前者会按请求返回顺序输出数据，后者则会缓存提前完成计算的数据，并按输入顺序逐个输出结果。</p><p><center><img src="/img/flink-datastream-join/img3.async-db-lookup.png" alt="图3.异步数据库查找关联架构" title="图3.异步数据库查找关联架构"></center></p><p>比起同步数据库查找关联，异步数据库查找关联稍微复杂一点，但是大部分的逻辑都由 Flink AsyncIO API 封装，因此总体来看还是比较简单。然而，有序输出模式下的 AsyncIO 会需要缓存数据，且这些数据会被写入 checkpoint，因此在内容资源方面的得分会低一点。另一方面，同步数据库查找关联的吞吐量问题得到解决，但仍不可避免地有数据库负载高和结果不确定两个问题。</p><p><center><img src="/img/flink-datastream-join/img4.async-db-lookup.png" alt="图4.异步数据库查找关联关键指标" title="图4.异步数据库查找关联关键指标"></center></p><p>从应用场景来说，异步数据库查找关联比较适合流量低的实时计算。</p><h2 id="带缓存的数据库查找关联"><a href="#带缓存的数据库查找关联" class="headerlink" title="带缓存的数据库查找关联"></a>带缓存的数据库查找关联</h2><p>为了解决上述两种关联方式对数据库造成太大压力的问题，可以引入一层缓存来减少直接对数据库的请求。缓存并一般不需要通过 checkpoint 机制持久化，因此简单地用一个 WeakHashMap 或者 Guava Cache 就可以实现。</p><p><center><img src="/img/flink-datastream-join/img5.cached-db-lookup.png" alt="图5.带缓存的数据库查找关联架构" title="图5.带缓存的数据库查找关联架构"></center></p><p>虽然在冷启动的时候仍会给数据库造成一定压力，但后续取决于缓存命中率，数据库的压力将得到一定程度的缓解。然而使用缓存带来的问题是维表的更新并不能及时反应到关联操作上，当然这也和缓存剔除的策略有关，需要根据维度表更新频率和业务对过时维表数据的容忍程度来设计。</p><p><center><img src="/img/flink-datastream-join/img6.cached-db-lookup.png" alt="图6.带缓存的数据库查找关联关键指标" title="图6.带缓存的数据库查找关联关键指标"></center></p><p>总而言之，带缓存的数据库查找关联适合于流量比较低，且对维表数据实时性要求不太高或维表更新比较少的业务场景。</p><h1 id="预加载维表关联"><a href="#预加载维表关联" class="headerlink" title="预加载维表关联"></a>预加载维表关联</h1><p>相比起实时数据库查找在运行期间为每条数据访问一次数据库，预加载维表关联是在作业启动时就将维表读到内存中，而在后续运行期间，每条数据都会和内存中的维表进行关联，而不会直接触发对数据的访问。与带缓存的实时数据库查找关联相比，区别是后者如果不命中缓存还可以 fallback 到数据库访问，而前者如果不名中则会关联不到数据。</p><h2 id="启动预加载维表"><a href="#启动预加载维表" class="headerlink" title="启动预加载维表"></a>启动预加载维表</h2><p>启动预加载维表是最为简单的一种方式，即在作业初始化的时候，比如用户函数的 <code>open()</code> 方法，直接从数据库将维表拷贝到内存中。维表并不需要用 State 来保存，因为无论是手动重启或者是 Flink 的错误重试机制导致的重启，<code>open()</code> 方法都会被执行，从而得到最新的维表数据。</p><p><center><img src="/img/flink-datastream-join/img7.startup-preloading.png" alt="图7.启动预加载维表架构" title="图7.启动预加载维表架构"></center></p><p>启动预加载维表对数据库的压力只持续很短时间，但因为是拷贝整个维表所以压力是很大的，而换来的优势是在运行期间不需要再访问数据库，可以提高效率，有点类似离线计算。相对地，问题在于运行期间维表数据不能更新，且对 TaskManager 内存的要求比较高。</p><p><center><img src="/img/flink-datastream-join/img8.startup-preloading.png" alt="图8.启动预加载维表关键指标" title="图8.启动预加载维表关键指标"></center></p><p>启动预加载维表适合于维表比较小、变更实时性要求不高的场景，比如根据 ip 库解析国家地区，如果 ip 库有新版本，重启作业即可。</p><h2 id="启动预加载分区维表"><a href="#启动预加载分区维表" class="headerlink" title="启动预加载分区维表"></a>启动预加载分区维表</h2><p>对于维表比较大的情况，可以启动预加载维表基础之上增加分区功能。简单来说就是将数据流按字段进行分区，然后每个 Subtask 只需要加在对应分区范围的维表数据。值得注意的是，这里的分区方式并不是用 keyby 这种通用的 hash 分区，而是需要根据业务数据定制化分区策略，然后调用 <code>DataStream#partitionCustom</code>。比如按照 <code>userId</code> 等区间划分，0-999 划分到 subtask 1，1000-1999 划分到 subtask 2，以此类推。而在 <code>open()</code> 方法中，我们再根据 subtask 的 id 和总并行度来计算应该加载的维表数据范围。</p><p><center><img src="/img/flink-datastream-join/img9.startup-partition-preloading.png" alt="图9.启动预加载分区维表架构" title="图9.启动预加载分区维表架构"></center></p><p>通过这种分区方式，维表的大小上限理论上可以线性拓展，解决了维表大小受限于单个 TaskManager 内存的问题（现在是取决于所有 TaskManager 的内存总量），但同时给带来设计和维护分区策略的复杂性。</p><p><center><img src="/img/flink-datastream-join/img10.startup-partition-preloading.png" alt="图10.启动预加载分区维表关键指标" title="图10.启动预加载分区维表关键指标"></center></p><p>总而言之，启动预加载分区维表适合维表比较大而变更实时性要求不高的场景，比如用户点击数据关联用户所在地。</p><h2 id="启动预加载维表并定时刷新"><a href="#启动预加载维表并定时刷新" class="headerlink" title="启动预加载维表并定时刷新"></a>启动预加载维表并定时刷新</h2><p>除了维表大小的限制，启动预加载维表的另一个主要问题在于维度数据的更新，我们可以通过引入定时刷新机制的办法来缓解这个问题。定时刷新可以通过 Flink ProcessFucntion 提供的 Timer 或者直接在 <code>open()</code> 初始化一个线程（池）来做这件事。不过 Timer 要求 KeyedStream，而上述的 <code>DataStream#partitionCustom</code> 并不会返回一个 KeyedStream，因此两者并不兼容。而如果使用额外线程定时刷新的办法则不受这个限制。</p><p><center><img src="/img/flink-datastream-join/img11.startup-preloading-refresh.png" alt="图11.启动预加载维表并定时刷新架构" title="图11.启动预加载维表并定时刷新架构"></center></p><p>比起基础的启动预加载维表 ，这种方式在于引入比较小复杂性的情况下大大缓解了的维度表更新问题，但也给维表数据库带来更多压力，因为每次 reload 的时候都是一次请求高峰。</p><p><center><img src="/img/flink-datastream-join/img12.startup-preloading-refresh.png" alt="图12.启动预加载维表并定时刷新关键指标" title="图12.启动预加载维表并定时刷新关键指标"></center></p><p>启动预加载维表和定时刷新的组合适合维表变更实时性要求不是特别高的场景。取决于定时刷新的频率和数据库的性能，这种方式可以满足大部分关联维表的业务。</p><h2 id="启动预加载维表-实时数据库查找"><a href="#启动预加载维表-实时数据库查找" class="headerlink" title="启动预加载维表 + 实时数据库查找"></a>启动预加载维表 + 实时数据库查找</h2><p>启动预加载维表还可以和实时数据库查找混合使用，即将预加载的维表作为缓存给实时关联时使用，若未名中则 fallback 到数据库查找。</p><p><center><img src="/img/flink-datastream-join/img13.startup-preloading-realtime-lookup.png" alt="图13.启动预加载维表结合实时数据库查找架构" title="图13.启动预加载维表结合实时数据库查找架构"></center></p><p>这种方式实际是带缓存的数据库查找关联的衍生，不同之处在于相比冷启动时未命中缓存导致的多次实时数据库访问，该方式直接批量拉取整个维表效率更高，但也有可能拉取到不会访问到的多余数据。下面雷达图中显示的是用异步数据库查找，如果是同步数据库查找吞吐量上会低一些。</p><p><center><img src="/img/flink-datastream-join/img14.startup-preloading-realtime-lookup.png" alt="图14.启动预加载维表结合实时数据库查找关键指标" title="图14.启动预加载维表结合实时数据库查找关键指标"></center></p><p>这种方式和带缓存的实时数据库查找关联基本相同，适合流量比较低，且对维表数据实时性要求不太高或维表更新比较少的业务场景。</p><h1 id="维表变更日志关联"><a href="#维表变更日志关联" class="headerlink" title="维表变更日志关联"></a>维表变更日志关联</h1><p>不同于上述两者将维表作为静态表关联的方式，维表变更日志关联将维表以 changelog 数据流的方式表示，从而将维表关联转变为两个数据流的 join。这里的 changelog 数据流类似于 MySQL 的 binlog，通常需要维表数据库端以 push 的方式将日志写到 Kafka 等消息队列中。Changelog 数据流称为 build 数据流，另外待关联的主要数据流成为 probe 数据流。</p><p>维表变更日志关联的好处在于可以获取某个 key 数据变化的时间，从而使得我们能在关联中使用 Event Time（当然也可以使用 Processing Time）。</p><h2 id="Processing-Time-维表变更日志关联"><a href="#Processing-Time-维表变更日志关联" class="headerlink" title="Processing Time 维表变更日志关联"></a>Processing Time 维表变更日志关联</h2><p>如果基于 Processing Time 做关联，我们可以利用 keyby 将两个数据流中关联字段值相同的数据划分到 KeyedCoProcessFunction 的同一个分区，然后用 ValueState 或者 MapState 将维表数据保存下来。在普通数据流的一条记录进到函数时，到 State 中查找有无符合条件的 join 对象，若有则关联输出结果，若无则根据 join 的类型决定是直接丢弃还是与空值关联。这里要注意的是，State 的大小要尽量控制好。首先是只保存每个 key 最新的维度数据值，其次是要给 State 设置好 TTL，让 Flink 可以自动清理。</p><p><center><img src="/img/flink-datastream-join/img15.processing-time-join.png" alt="图15.Processing Time 维表变更日志关联架构" title="图15.Processing Time 维表变更日志关联架构"></center></p><p>基于 Processing Time 的维表变更日志关联优点是不需要直接请求数据库，不会对数据库造成压力；缺点是比较复杂，相当于使用 changelog 在 Flink 应用端重新构建一个维表，会占用一定的 CPU 和比较多的内存和磁盘资源。值得注意的是，我们可以利用 Flink 提供的 RocksDB StateBackend，将大部分的维表数据存在磁盘而不是内存中，所以并不会占用很高的内存。不过基于 Processing Time 的这种关联对两个数据流的延迟要求比较高，否则如果其中一个数据流出现 lag 时，关联得到的结果可能并不是我们想要的，比如可能会关联到未来时间点的维表数据。</p><p><center><img src="/img/flink-datastream-join/img16.processing-time-join.png" alt="图16.Processing Time 维表变更日志关联关键指标" title="图16.Processing Time 维表变更日志关联关键指标"></center></p><p>基于 Processing Time 的维表变更日志关联比较适用于不便直接访问数据的场景（比如维表数据库是业务线上数据库，出于安全和负载的原因不能直接访问），或者对维表的变更实时性要求比较高的场景（但因为数据准确性的关系，一般用下文的 Event Time 关联会更好）。</p><h2 id="Event-Time-维表变更日志关联"><a href="#Event-Time-维表变更日志关联" class="headerlink" title="Event Time 维表变更日志关联"></a>Event Time 维表变更日志关联</h2><p>基于 Event Time 的维表关联实际上和基于 Processing Time 的十分相似，不同之处在于我们将维表 changelog 的多个时间版本都记录下来，然后每当一条记录进来，我们会找到对应时间版本的维表数据来和它关联，而不是总用最新版本，因此延迟数据的关联准确性大大提高。不过因为目前 State 并没有提供 Event Time 的 TTL，因此我们需要自己设计和实现 State 的清理策略，比如直接设置一个 Event Time Timer（但要注意 Timer 不能太多导致性能问题），再比如对于单个 key 只保存最近的 10 个版本，当有更新版本的维表数据到达时，要清理掉最老版本的数据。</p><p><center><img src="/img/flink-datastream-join/img17.event-time-join.png" alt="图17.Event Time 维表变更日志关联架构" title="图17.Event Time 维表变更日志关联架构"></center></p><p>基于 Event Time 的维表变更日志关联相对基于 Processing Time 的方式来说是一个改进，虽然多个维表版本导致空间资源要求更大，但确保准确性对于大多数场景来说都是十分重要的。相比 Processing Time 对两个数据的延迟都有要求，Event Time 要求 build 数据流的延迟低，否则可能一条数据到达时关联不到对应维表数据或者关联了一个过时版本的维表数据，</p><p><center><img src="/img/flink-datastream-join/img18.event-time-join.png" alt="图18.Event Time 维表变更日志关联关键指标" title="图18.Event Time 维表变更日志关联关键指标"></center></p><p>基于 Event Time 的维表变更日志关联比较适合于维表变更比较多且对变更实时性要求较高的场景 同时也适合于不便直接访问数据库的场景。</p><h2 id="Temporal-Table-Join"><a href="#Temporal-Table-Join" class="headerlink" title="Temporal Table Join"></a>Temporal Table Join</h2><p>Temporal Table Join 是 Flink SQL/Table API 的原生支持，它对两个数据流的输入都进行了缓存，因此比起上述的基于 Event Time 的维表变更日志关联，它可以容忍任意数据流的延迟，数据准确性更好。Temporal Table Join 在 SQL/Table API 使用时是十分简单的，但如果想在 DataStream API 中使用，则需要自己实现对应的逻辑。</p><p>总体思路是使用一个 CoProcessFunction，将 build 数据流以时间版本为 key 保存在 MapState 中（与基于 Event Time 的维表变更日志关联相同），再将 probe 数据流和输出结果也用 State 缓存起来（同样以 Event Time 为 key），一直等到 Watermark 提升到它们对应的 Event Time，才把结果输出和将两个数据流的输入清理掉。</p><p>这个 Watermark 触发很自然地是用 Event Time Timer 来实现，但要注意不要为每条数据都设置一遍 Timer，因为一旦 Watermark 提升会触发很多个 Timer 导致性能急剧下降。比较好的实践是为每个 key  只注册一个 Timer。实现上可以记录当前未处理的最早一个 Event Time，并用来注册 Timer。当前 Watermark。每当 Watermark 触发 Timer 时，我们检查处理掉未处理的最早 Event Time 到当前 Event Time 的所有数据，并将未处理的最早 Event Time 更新为当前时间。</p><p><center><img src="/img/flink-datastream-join/img19.temporal-table-join.png" alt="图19.Temporal Table Join 架构" title="图19.Temporal Table Join 架构"></center></p><p>Temporal Table Join 的好处在于对于两边数据流的延迟的容忍度较大，但作为代价会引入一定的输出结果的延迟，这也是基于 Watermark 机制的计算的常见问题，或者说，妥协。另外因为吞吐量较大的 probe 数据流也需要缓存，Flink 应用对空间资源的需求会大很多。最好，要注意的是如果维表变更太慢，导致 Watermark 提升太慢，会导致 probe 数据流被大量缓存，所以最好要确保 build 数据流尽量实时，同时给 Source 设置一个比较短的 idle timeout。</p><p><center><img src="/img/flink-datastream-join/img20.temporal-table-join.png" alt="图20.Temporal Table Join 关键指标" title="图20.Temporal Table Join 关键指标"></center></p><p>Temporal Table Join 这种方式最为复杂，但数据准确性最好，适合一些对数据准确性要求高且可以容忍一定延迟（一般分钟级别）的关键业务。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>用 Flink DataStream API 实现关联维表的方式十分丰富，可以直接访问数据库查找（实时数据库查找关联），可以启动时就将全量维表读到内存（预加载维表关联），也可以通过维表的 changelog 在 Flink 应用端实时构建一个新的维表（维表变更日志关联）。我们可以从实现简单性、吞吐量、维表数据的实时性、数据库的负载、内存资源占用、可拓展性和结果确定性这 7 个维度来衡量一个具体实现方式，并根据业务需求来选择最合适的实现。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.ververica.com/about/events-talks" target="_blank" rel="external">WEBINAR: 99 Ways to Enrich Streaming Data with Apache Flink</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/asyncio.html" target="_blank" rel="external">Asynchronous I/O for External Data Access</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇博客提到 Flink SQL 如何 Join 两个数据流，有读者反馈说如果不打算用 SQL 或者想自己实现底层操作，那么如何基于 DataStream API 来关联维表呢？实际上由于 Flink DataStream API 的灵活性，实现这个需求的方式是非常多样的，但是大部分用户很难在设计架构时就考虑得很全面，可能会走不少弯路。针对于此，笔者根据工作经验以及社区资源整理了用 DataStream 实现 Join 维表的常见方式，并给每种的方式优劣和适用场景给出一点可作为参考的个人观点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 如何实现数据流的 Join</title>
    <link href="https://link3280.github.io/2019/12/15/Flink-SQL-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E6%B5%81%E7%9A%84-Join/"/>
    <id>https://link3280.github.io/2019/12/15/Flink-SQL-如何实现数据流的-Join/</id>
    <published>2019-12-15T09:04:57.000Z</published>
    <updated>2019-12-17T13:34:58.666Z</updated>
    
    <content type="html"><![CDATA[<p>无论在 OLAP 还是 OLTP 领域，Join 都是业务常会涉及到且优化规则比较复杂的 SQL 语句。对于离线计算而言，经过数据库领域多年的积累 Join 的语义以及实现已经十分成熟，然而对于近年来刚兴起的 Streaming SQL 来说 Join 却处于刚起步的状态。其中最为关键的问题在于 Join 的实现依赖于缓存整个数据集，而 Streaming SQL Join 的对象却是无限的数据流，内存压力和计算效率在长期运行来说都是不可避免的问题。下文将结合 SQL 的发展解析 Flink SQL 是如何解决这些问题并实现两个数据流的 Join。</p><a id="more"></a><h1 id="离线-Batch-SQL-Join-的实现"><a href="#离线-Batch-SQL-Join-的实现" class="headerlink" title="离线 Batch SQL Join 的实现"></a>离线 Batch SQL Join 的实现</h1><p>传统的离线 Batch SQL （面向有界数据集的 SQL）有三种基础的实现方式，分别是 Nested-loop Join、Sort-Merge Join 和 Hash Join。</p><p>Nested-loop Join 最为简单直接，将两个数据集加载到内存，并用内嵌遍历的方式来逐个比较两个数据集内的元素是否符合 Join 条件。Nested-loop Join 虽然时间效率以及空间效率都是最低的，但胜在比较灵活适用范围广，因此其变体 BNL 常被传统数据库用作为 Join 的默认基础选项。</p><p>Sort-Merge Join 顾名思义，分为两个 Sort 和 Merge 阶段。首先将两个数据集进行分别排序，然后对两个有序数据集分别进行遍历和匹配，类似于归并排序的合并。值得注意的是，Sort-Merge 只适用于 Equi-Join（Join 条件均使用等于作为比较算子）。Sort-Merge Join 要求对两个数据集进行排序，成本很高，通常作为输入本就是有序数据集的情况下的优化方案。</p><p>Hash Join 同样分为两个阶段，首先将一个数据集转换为 Hash Table，然后遍历另外一个数据集元素并与 Hash Table 内的元素进行匹配。第一阶段和第一个数据集分别称为 build 阶段和 build table，第二个阶段和第二个数据集分别称为 probe 阶段和 probe table。Hash Join 效率较高但对空间要求较大，通常是作为 Join 其中一个表为适合放入内存的小表的情况下的优化方案。和 Sort-Merge Join 类似，Hash Join 也只适用于 Equi-Join。</p><h1 id="实时-Streaming-SQL-Join"><a href="#实时-Streaming-SQL-Join" class="headerlink" title="实时 Streaming SQL Join"></a>实时 Streaming SQL Join</h1><p>相对于离线的 Join，实时 Streaming SQL（面向无界数据集的 SQL）无法缓存所有数据，因此 Sort-Merge Join 要求的对数据集进行排序基本是无法做到的，而 Nested-loop Join 和 Hash Join 经过一定的改良则可以满足实时 SQL 的要求。</p><p>我们通过例子来看基本的 Nested Join 在实时 Streaming SQL 的基础实现（案例及图来自 Piotr Nowojski 在 Flink Forward San Francisco 的分享[2]）。</p><p><center><p><img src="/img/streaming-join/img1.join-in-continuous-query-1.png" alt="图1. Join-in-continuous-query-1" title="图1. Join-in-continuous-query-1"></p></center></p><p></p><p>Table A 有 <code>1</code>、<code>42</code> 两个元素，Table B 有 <code>42</code> 一个元素，所以此时的 Join 结果会输出 42。</p><p><center><p><img src="/img/streaming-join/img2.join-in-continuous-query-2.png" alt="图2. Join-in-continuous-query-2" title="图2. Join-in-continuous-query-2"></p></center></p><p></p><p>接着 Table B 依次接受到三个新的元素，分别是 <code>7</code>、<code>3</code>、<code>1</code>。因为 <code>1</code> 匹配到 Table A 的元素，因此结果表再输出一个元素 <code>1</code>。</p><p><center><p><img src="/img/streaming-join/img3.join-in-continuous-query-3.png" alt="图3. Join-in-continuous-query-3" title="图3. Join-in-continuous-query-3"></p></center></p><p></p><p>随后 Table A 出现新的输入 <code>2</code>、<code>3</code>、<code>6</code>，<code>3</code> 匹配到 Table B 的元素，因此再输出 <code>3</code> 到结果表。</p><p>可以看到在 Nested-Loop Join 中我们需要保存两个输入表的内容，而随着时间的增长 Table A 和 Table B 需要保存的历史数据无止境地增长，导致很不合理的内存磁盘资源占用，而且单个元素的匹配效率也会越来越低。类似的问题也存在于 Hash Join 中。</p><p>那么有没有可能设置一个缓存剔除策略，将不必要的历史数据及时清理呢？答案是肯定的，关键在于缓存剔除策略如何实现，这也是 Flink SQL 提供的三种 Join 的主要区别。</p><h1 id="Flink-SQL-的-Join"><a href="#Flink-SQL-的-Join" class="headerlink" title="Flink SQL 的 Join"></a>Flink SQL 的 Join</h1><h2 id="Regular-Join"><a href="#Regular-Join" class="headerlink" title="Regular Join"></a>Regular Join</h2><p>Regular Join 是最为基础的没有缓存剔除策略的 Join。Regular Join 中两个表的输入和更新都会对全局可见，影响之后所有的 Join 结果。举例，在一个如下的 Join 查询里，Orders 表的新纪录会和 Product 表所有历史纪录以及未来的纪录进行匹配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">SELECT * FROM Orders</div><div class="line">INNER JOIN Product</div><div class="line">ON Orders.productId = Product.id</div></pre></td></tr></table></figure><p>因为历史数据不会被清理，所以 Regular Join 允许对输入表进行任意种类的更新操作（insert、update、delete）。然而因为资源问题 Regular Join 通常是不可持续的，一般只用做有界数据流的 Join。</p><h2 id="Time-Windowed-Join"><a href="#Time-Windowed-Join" class="headerlink" title="Time-Windowed Join"></a>Time-Windowed Join</h2><p>Time-Windowed Join 利用窗口的给两个输入表设定一个 Join 的时间界限，超出时间范围的数据则对 JOIN 不可见并可以被清理掉。值得注意的是，这里涉及到的一个问题是时间的语义，时间可以是指计算发生的系统时间（即 Processing Time），也可以是指从数据本身的时间字段提取的 Event Time。如果是 Processing Time，Flink 根据系统时间自动划分 Join 的时间窗口并定时清理数据；如果是 Event Time，Flink 分配 Event Time 窗口并依据 Watermark 来清理数据。</p><p>以更常用的 Event Time Windowed Join 为例，一个将 Orders 订单表和 Shipments 运输单表依据订单时间和运输时间 Join 的查询如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">SELECT *</div><div class="line">FROM </div><div class="line">Orders o, </div><div class="line">Shipments s</div><div class="line">WHERE </div><div class="line">o.id = s.orderId AND</div><div class="line">s.shiptime BETWEEN o.ordertime AND o.ordertime + INTERVAL &apos;4&apos; HOUR</div></pre></td></tr></table></figure><p>这个查询会为 Orders 表设置了 <code>o.ordertime &gt; s.shiptime- INTERVAL &#39;4&#39;HOUR</code> 的时间下界（图4），</p><p><center><p><img src="/img/streaming-join/img4.time-window-orders-lower-bound.png" alt="图4. Time-Windowed Join 的时间下界 - Orders 表" title="图4. Time-Windowed Join 的时间下界 - Orders 表"></p></center></p><p></p><p>并为 Shipmenets 表设置了 <code>s.shiptime &gt;= o.ordertime</code> 的时间下界（图5）。</p><p><center><p><img src="/img/streaming-join/img5.time-window-shipment-lower-bound.png" alt="图5. Time-Windowed Join 的时间下界 - Shipment 表" title="图5. Time-Windowed Join 的时间下界 - Shipment 表"></p></center></p><p></p><p>因此两个输入表都只需要缓存在时间下界以上的数据，将空间占用维持在合理的范围。</p><p>不过虽然底层实现上没有问题，但如何通过 SQL 语法定义时间仍是难点。尽管在实时计算领域 Event Time、Processing Time、Watermark 这些概念已经成为业界共识，但在 SQL 领域对时间数据类型的支持仍比较弱[4]。因此，定义 Watermark 和时间语义都需要通过编程 API 的方式完成，比如从 DataStream 转换至 Table 时定义，而不能单纯靠 SQL 完成。这方面的支持 Flink 社区计划通过拓展 SQL 方言来完成，感兴趣的读者可以通过 FLIP-66[7] 来追踪进度。</p><h2 id="Temporal-Table-Join"><a href="#Temporal-Table-Join" class="headerlink" title="Temporal Table Join"></a>Temporal Table Join</h2><p>虽然 Timed-Windowed Join 解决了资源问题，但也限制了使用场景: Join 两个输入流都必须有时间下界，超过之后则不可访问。这对于很多 Join 维表的业务来说是不适用的，因为很多情况下维表并没有时间界限。针对这个问题，Flink 提供了 Temporal Table Join 来满足用户需求。</p><p>Temporal Table Join 类似于 Hash Join，将输入分为 Build Table 和 Probe Table。前者一般是纬度表的 changelog，后者一般是业务数据流，典型情况下后者的数据量应该远大于前者。在 Temporal Table Join 中，Build Table 是一个基于 append-only 数据流的带时间版本的视图，所以又称为 Temporal Table。Temporal Table 要求定义一个主键和用于版本化的字段（通常就是 Event Time 时间字段），以反映记录内容在不同时间的内容。</p><p>比如典型的一个例子是对商业订单金额进行汇率转换。假设有一个 Oders 流记录订单金额，需要和 RatesHistory 汇率流进行 Join。RatesHistory 代表不同货币转为日元的汇率，每当汇率有变化时就会有一条更新记录。两个表在某一时间节点内容如下:</p><p><center><p><img src="/img/streaming-join/img6.temporal-table-join-example.png" alt="图6. Temporal Table Join Example" title="图6. Temporal Table Join Example]"></p></center></p><p></p><p>我们将 RatesHistory 注册为一个名为 Rates 的 Temporal Table，设定主键为 currency，版本字段为 time。</p><p><center><p><img src="/img/streaming-join/img7.temporal-table-registration.png" alt="图7. Temporal Table Registration" title="图7. Temporal Table Registration]"></p></center></p><p></p><p>此后给 Rates 指定时间版本，Rates 则会基于 RatesHistory 来计算符合时间版本的汇率转换内容。</p><p><center><p><img src="/img/streaming-join/img8.temporal-table-content.png" alt="图8. Temporal Table Content" title="图8. Temporal Table Content]"></p></center></p><p></p><p>在 Rates 的帮助下，我们可以将业务逻辑用以下的查询来表达:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">SELECT </div><div class="line">o.amount * r.rate</div><div class="line">FROM</div><div class="line">Orders o,</div><div class="line">LATERAL Table(Rates(o.time)) r</div><div class="line">WHERE</div><div class="line">o.currency = r.currency</div></pre></td></tr></table></figure><p>值得注意的是，不同于在 Regular Join 和 Time-Windowed Join 中两个表是平等的，任意一个表的新记录都可以与另一表的历史记录进行匹配，在 Temporal Table Join 中，Temoparal Table 的更新对另一表在该时间节点以前的记录是不可见的。这意味着我们只需要保存 Build Side 的记录直到 Watermark 超过记录的版本字段。因为 Probe Side 的输入理论上不会再有早于 Watermark 的记录，这些版本的数据可以安全地被清理掉。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实时领域 Streaming SQL 中的 Join 与离线 Batch SQL 中的 Join 最大不同点在于无法缓存完整数据集，而是要给缓存设定基于时间的清理条件以限制 Join 涉及的数据范围。根据清理策略的不同，Flink SQL 分别提供了 Regular Join、Time-Windowed Join 和 Temporal Table Join 来应对不同业务场景。</p><p>另外，尽管在实时计算领域 Join 可以灵活地用底层编程 API 来实现，但在 Streaming SQL 中 Join 的发展仍处于比较初级的阶段，其中关键点在于如何将时间属性合适地融入 SQL 中，这点 ISO SQL 委员会制定的 SQL 标准并没有给出完整的答案。或者从另外一个角度来讲，作为 Streaming SQL 最早的开拓者之一，Flink 社区很适合探索出一套合理的 SQL 语法反过来贡献给 ISO。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://flink.apache.org/2019/05/14/temporal-tables.html" target="_blank" rel="external">Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL</a></li><li><a href="https://www.slideshare.net/FlinkForward/flink-forward-san-francisco-2019-how-to-join-two-data-streams-piotr-nowojski" target="_blank" rel="external">How to Join Two Data Streams? - Piotr Nowojski</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/streaming/joins.html#joins-in-continuous-queries" target="_blank" rel="external">Joins in Continuous Queries</a></li><li><a href="https://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf" target="_blank" rel="external">Temporal features in SQL:2011</a></li><li><a href="https://mysqlserverteam.com/hash-join-in-mysql-8/" target="_blank" rel="external">Hash join in MySQL 8</a></li><li><a href="https://en.wikipedia.org/wiki/SQL:2011" target="_blank" rel="external">SQL:2011</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-66%3A+Support+Time+Attribute+in+SQL+DDL" target="_blank" rel="external">FLIP-66: Support Time Attribute in SQL DDL</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;无论在 OLAP 还是 OLTP 领域，Join 都是业务常会涉及到且优化规则比较复杂的 SQL 语句。对于离线计算而言，经过数据库领域多年的积累 Join 的语义以及实现已经十分成熟，然而对于近年来刚兴起的 Streaming SQL 来说 Join 却处于刚起步的状态。其中最为关键的问题在于 Join 的实现依赖于缓存整个数据集，而 Streaming SQL Join 的对象却是无限的数据流，内存压力和计算效率在长期运行来说都是不可避免的问题。下文将结合 SQL 的发展解析 Flink SQL 是如何解决这些问题并实现两个数据流的 Join。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
      <category term="SQL" scheme="https://link3280.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>如何分析及处理 Flink 反压</title>
    <link href="https://link3280.github.io/2019/11/03/Flink-%E5%8F%8D%E5%8E%8B%E5%88%86%E6%9E%90%E5%8F%8A%E5%A4%84%E7%90%86/"/>
    <id>https://link3280.github.io/2019/11/03/Flink-反压分析及处理/</id>
    <published>2019-11-03T11:49:21.000Z</published>
    <updated>2019-11-03T12:06:04.979Z</updated>
    
    <content type="html"><![CDATA[<p>反压（backpressure）是实时计算应用开发中，特别是流式计算中，十分常见的问题。反压意味着数据管道中某个节点成为瓶颈，处理速率跟不上上游发送数据的速率，而需要对上游进行限速。由于实时计算应用通常使用消息队列来进行生产端和消费端的解耦，消费端数据源是 pull-based 的，所以反压通常是从某个节点传导至数据源并降低数据源（比如 Kafka consumer）的摄入速率。</p><a id="more"></a><p>关于 Flink 的反压机制，网上已经有不少博客介绍，中文博客推荐这两篇[1][2]。简单来说，Flink 拓扑中每个节点（Task）间的数据都以阻塞队列的方式传输，下游来不及消费导致队列被占满后，上游的生产也会被阻塞，最终导致数据源的摄入被阻塞。而本文将着重结合官方的博客[4]分享笔者在实践中分析和处理 Flink 反压的经验。</p><h2 id="反压的影响"><a href="#反压的影响" class="headerlink" title="反压的影响"></a>反压的影响</h2><p>反压并不会直接影响作业的可用性，它表明作业处于亚健康的状态，有潜在的性能瓶颈并可能导致更大的数据处理延迟。通常来说，对于一些对延迟要求不太高或者数据量比较小的应用来说，反压的影响可能并不明显，然而对于规模比较大的 Flink 作业来说反压可能会导致严重的问题。</p><p>这是因为 Flink 的 checkpoint 机制，反压还会影响到两项指标: checkpoint 时长和 state 大小。前者是因为 checkpoint barrier 是不会越过普通数据的，数据处理被阻塞也会导致 checkpoint barrier 流经整个数据管道的时长变长，因而 checkpoint 总体时间（End to End Duration）变长。后者是因为为保证 EOS（Exactly-Once-Semantics，准确一次），对于有两个以上输入管道的 Operator，checkpoint barrier 需要对齐（Alignment），接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到state 里面，导致 checkpoint 变大。这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint 超时失败，而 state 大小同样可能拖慢 checkpoint 甚至导致 OOM （使用 Heap-based StateBackend）或者物理内存使用超出容器资源（使用 RocksDBStateBackend）的稳定性问题。因此，我们在生产中要尽量避免出现反压的情况（顺带一提，为了缓解反压给 checkpoint 造成的压力，社区提出了 FLIP-76: Unaligned Checkpoints[4] 来解耦反压和 checkpoint）。</p><h2 id="定位反压节点"><a href="#定位反压节点" class="headerlink" title="定位反压节点"></a>定位反压节点</h2><p>要解决反压首先要做的是定位到造成反压的节点，这主要有两种办法: 1.通过 Flink Web UI 自带的反压监控面板；2.通过 Flink Task Metrics。前者比较容易上手，适合简单分析，后者则提供了更加丰富的信息，适合用于监控系统。因为反压会向上游传导，这两种方式都要求我们从 Source 节点到 Sink 的逐一排查，直到找到造成反压的根源原因[4]。下面分别介绍这两种办法。</p><h3 id="反压监控面板"><a href="#反压监控面板" class="headerlink" title="反压监控面板"></a>反压监控面板</h3><p>Flink Web UI 的反压监控提供了 SubTask 级别的反压监控，原理是通过周期性对 Task 线程的栈信息采样，得到线程被阻塞在请求 Buffer（意味着被下游队列阻塞）的频率来判断该节点是否处于反压状态。默认配置下，这个频率在 0.1 以下则为 <code>OK</code>，0.1 至 0.5 为 <code>LOW</code>，而超过 0.5 则为 <code>HIGH</code>。</p><center><p><img src="/img/flink-backpressure-handling/back-pressure-sampling-high.png" alt="图1. Flink 1.8 的 Web UI 反压面板" title="图1. Flink 1.8 的 Web UI 反压面板(来自官方博客)"></p></center><p>如果处于反压状态，那么有两种可能性：</p><ol><li>该节点的发送速率跟不上它的产生数据速率。这一般会发生在一条输入多条输出的 Operator（比如 flatmap）。</li><li>下游的节点接受速率较慢，通过反压机制限制了该节点的发送速率。</li></ol><p>如果是第一种状况，那么该节点则为反压的根源节点，它是从 Source Task 到 Sink Task 的第一个出现反压的节点。如果是第二种情况，则需要继续排查下游节点。值得注意的是，反压的根源节点并不一定会在反压面板体现出高反压，因为反压面板监控的是发送端，如果某个节点是性能瓶颈并不会导致它本身出现高反压，而是导致它的上游出现高反压。总体来看，如果我们找到第一个出现反压的节点，那么反压根源要么是就这个节点，要么是它紧接着的下游节点。</p><p>那么如果区分这两种状态呢？很遗憾只通过反压面板是无法直接判断的，我们还需要结合 Metrics 或者其他监控手段来定位。此外如果作业的节点数很多或者并行度很大，由于要采集所有 Task 的栈信息，反压面板的压力也会很大甚至不可用。</p><h3 id="Task-Metrics"><a href="#Task-Metrics" class="headerlink" title="Task Metrics"></a>Task Metrics</h3><p>Flink 提供的 Task Metrics 是更好的反压监控手段，但也要求更加丰富的背景知识。首先我们简单回顾下 Flink 1.5 以后的网路栈，熟悉的读者可以直接跳过。</p><p>TaskManager 传输数据时，不同的 TaskManager 上的两个 Subtask 间通常根据 key 的数量有多个 Channel，这些 Channel 会复用同一个 TaskManager 级别的 TCP 链接，并且共享接收端 Subtask 级别的 Buffer Pool。在接收端，每个 Channl 在初始阶段会被分配固定数量的 Exclusive Buffer，这些 Buffer 会被用于存储接受到的数据，交给 Operator 使用后再次被释放。Channel 接收端空闲的 Buffer 数量称为 Credit，Credit 会被定时同步给发送端被后者用于决定发送多少个 Buffer 的数据。在流量较大时，Channel 的 Exclusive Buffer 可能会被写满，此时 Flink 会向 Buffer Pool 申请剩余的 Floating Buffer。这些 Floating Buffer 属于备用 Buffer，哪个 Channel 需要就去哪里。而在 Channel 发送端，一个 Subtask 所有的 Channel 会共享同一个 Buffer Pool，这边就没有区分 Exclusive Buffer 和 Floating Buffer。</p><center><p><img src="/img/flink-backpressure-handling/credit-based-network.png" alt="图2. Flink Credit-Based 网络" title="图2. Flink Credit-Based 网络"></p></center><p>我们在监控反压时会用到的 Metrics 主要和 Channel 接受端的 Buffer 使用率有关，最为有用的是以下几个 Metrics:</p><table><thead><tr><th>Metris</th><th>描述</th></tr></thead><tbody><tr><td>outPoolUsage</td><td>发送端 Buffer 的使用率</td></tr><tr><td>inPoolUsage</td><td>接收端 Buffer 的使用率</td></tr><tr><td>floatingBuffersUsage（1.9 以上）</td><td>接收端 Floating Buffer 的使用率</td></tr><tr><td>exclusiveBuffersUsage （1.9 以上）</td><td>接收端 Exclusive Buffer 的使用率</td></tr></tbody></table><p>其中 inPoolUsage 等于 floatingBuffersUsage 与 exclusiveBuffersUsage 的总和。</p><p>分析反压的大致思路是：如果一个 Subtask 的发送端 Buffer 占用率很高，则表明它被下游反压限速了；如果一个 Subtask 的接受端 Buffer 占用很高，则表明它将反压传导至上游。反压情况可以根据以下表格进行对号入座(图片来自官网):</p><center><p><img src="/img/flink-backpressure-handling/1.8-backpressure-table.png" alt="图3. 反压分析表" title="图3. 反压分析表"></p></center><p>outPoolUsage 和 inPoolUsage 同为低或同为高分别表明当前 Subtask 正常或处于被下游反压，这应该没有太多疑问。而比较有趣的是当 outPoolUsage 和 inPoolUsage 表现不同时，这可能是出于反压传导的中间状态或者表明该 Subtask 就是反压的根源。如果一个 Subtask 的 outPoolUsage 是高，通常是被下游 Task 所影响，所以可以排查它本身是反压根源的可能性。如果一个 Subtask 的 outPoolUsage 是低，但其 inPoolUsage 是高，则表明它有可能是反压的根源。因为通常反压会传导至其上游，导致上游某些 Subtask 的 outPoolUsage 为高，我们可以根据这点来进一步判断。值得注意的是，反压有时是短暂的且影响不大，比如来自某个 Channel 的短暂网络延迟或者 TaskManager 的正常 GC，这种情况下我们可以不用处理。</p><p>对于 Flink 1.9 及以上版本，除了上述的表格，我们还可以根据 floatingBuffersUsage/exclusiveBuffersUsage 以及其上游 Task 的 outPoolUsage 来进行进一步的分析一个 Subtask 和其上游 Subtask 的数据传输。</p><center><p><img src="/img/flink-backpressure-handling/1.9-backpressure-table.png" alt="图4. Flink 1.9 反压分析表" title="图4. Flink 1.9 反压分析表"></p></center><p>通常来说，floatingBuffersUsage 为高则表明反压正在传导至上游，而 exclusiveBuffersUsage 则表明了反压是否存在倾斜（floatingBuffersUsage 高、exclusiveBuffersUsage 低为有倾斜，因为少数 channel 占用了大部分的 Floating Buffer）。</p><p>至此，我们已经有比较丰富的手段定位反压的根源是出现在哪个节点，但是具体的原因还没有办法找到。另外基于网络的反压 metrics 并不能定位到具体的 Operator，只能定位到 Task。特别是那种 embarrassingly parallel（易并行）的作业（所有的 Operator 会被放入一个 Task，因此只有一个节点），反压 metrics 则排不上用场。</p><h2 id="分析具体原因及处理"><a href="#分析具体原因及处理" class="headerlink" title="分析具体原因及处理"></a>分析具体原因及处理</h2><p>定位到反压节点后，分析造成原因的办法和我们分析一个普通程序的性能瓶颈的办法是十分类似的，可能还要更简单一点，因为我们要观察的主要是 Task Thread。</p><p>在实践中，很多情况下的反压是由于数据倾斜造成的，这点我们可以通过 Web UI 各个 SubTask 的 Records Sent 和 Record Received 来确认，另外 Checkpoint detail 里不同 SubTask 的 State size 也是一个分析数据倾斜的有用指标。</p><p>此外，最常见的问题可能是用户代码的执行效率问题（频繁被阻塞或者性能问题）。最有用的办法就是对 TaskManager 进行 CPU profile，从中我们可以分析到 Task Thread 是否跑满一个 CPU 核：如果是的话要分析 CPU 主要花费在哪些函数里面，比如我们生产环境中就偶尔遇到卡在 Regex 的用户函数（ReDoS）；如果不是的话要看 Task Thread 阻塞在哪里，可能是用户函数本身有些同步的调用，可能是 checkpoint 或者 GC 等系统活动导致的暂时系统暂停。当然，性能分析的结果也可能是正常的，只是作业申请的资源不足而导致了反压，这就通常要求拓展并行度。值得一提的，在未来的版本 Flink 将会直接在 WebUI 提供 JVM 的 CPU 火焰图[5]，这将大大简化性能瓶颈的分析。</p><p>另外 TaskManager 的内存以及 GC 问题也可能会导致反压，包括 TaskManager JVM 各区内存不合理导致的频繁 Full GC 甚至失联。推荐可以通过给 TaskManager 启用 G1 垃圾回收器来优化 GC，并加上 <code>-XX:+PrintGCDetails</code> 来打印 GC 日志的方式来观察 GC 的问题。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>反压是 Flink 应用运维中常见的问题，它不仅意味着性能瓶颈还可能导致作业的不稳定性。定位反压可以从 Web UI 的反压监控面板和 Task Metric 两者入手，前者方便简单分析，后者适合深入挖掘。定位到反压节点后我们可以通过数据分布、CPU Profile 和 GC 指标日志等手段来进一步分析反压背后的具体原因并进行针对性的优化。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1.<a href="http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/" target="_blank" rel="external">Flink 原理与实现：如何处理反压问题</a><br>2.<a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1571628927&amp;ver=1925&amp;signature=cHpaczGLH6QninlmHmM0mDKbb2-fuTMw83YjIFQFa7iN3omCrdlL51zCKo7N0n1uwM7*9JL-DmsQXhR*1Uh0YiUpVLHEzklFN9KUK33PVeF2fnoXcr0cDPPZ2s8HmK-D&amp;new=1" target="_blank" rel="external">一文彻底搞懂 Flink 网络流控与反压机制</a><br>3.<a href="http://www.whitewood.me/2018/05/13/Flink-%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%BC%82%E6%AD%A5%E5%BF%AB%E7%85%A7-ABS-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" target="_blank" rel="external">Flink 轻量级异步快照 ABS 实现原理</a><br>4.<a href="https://flink.apache.org/2019/07/23/flink-network-stack-2.html" target="_blank" rel="external">Flink Network Stack Vol. 2: Monitoring, Metrics, and that Backpressure Thing</a><br>5.<a href="https://issues.apache.org/jira/browse/FLINK-13550" target="_blank" rel="external">Support for CPU FlameGraphs in new web UI</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;反压（backpressure）是实时计算应用开发中，特别是流式计算中，十分常见的问题。反压意味着数据管道中某个节点成为瓶颈，处理速率跟不上上游发送数据的速率，而需要对上游进行限速。由于实时计算应用通常使用消息队列来进行生产端和消费端的解耦，消费端数据源是 pull-based 的，所以反压通常是从某个节点传导至数据源并降低数据源（比如 Kafka consumer）的摄入速率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.10 细粒度资源管理解析</title>
    <link href="https://link3280.github.io/2019/10/17/Flink-1-10-%E7%BB%86%E7%B2%92%E5%BA%A6%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <id>https://link3280.github.io/2019/10/17/Flink-1-10-细粒度资源管理解析/</id>
    <published>2019-10-17T12:44:50.000Z</published>
    <updated>2019-10-17T13:05:59.466Z</updated>
    
    <content type="html"><![CDATA[<p>相信不少读者在开发 Flink 应用时或多或少会遇到在内存调优方面的问题，比如在我们生产环境中遇到最多的 TaskManager 在容器化环境下占用超出容器限制的内存而被 YARN/Mesos kill 掉[1]，再比如使用 heap-based StateBackend 情况下 State 过大导致 GC 频繁影响吞吐。这些问题对于不熟悉 Flink 内存管理的用户来说十分难以排查，而且 Flink 晦涩难懂的内存配置参数更是让用户望而却步，结果是往往将内存调大至一个比较浪费的阈值以尽量避免内存问题。</p><a id="more"></a><p>对于作业规模不大的普通用户而言，这些通常在可以接受的范围之内，但对于上千并行度的大作业来说，浪费资源的总量会非常可观，而且进程的不稳定性导致的作业恢复时间也会比普通作业长得多，因此阿里巴巴的 Blink 团队针对内存管理机制做了大量的优化，并于近期开始合并到 Flink。本文的内容主要基于阿里团队工程师宋辛童在 Flink Forward Beijing 的分享[1]，以及后续相关的几个 FLIP 提案。</p><h1 id="Flink-目前（1-9）的内存管理"><a href="#Flink-目前（1-9）的内存管理" class="headerlink" title="Flink 目前（1.9）的内存管理"></a>Flink 目前（1.9）的内存管理</h1><p>TaskManager 作为 Master/Slave 架构中的 Slave 提供了作业执行需要的环境和资源，最为重要而且复杂，因此 Flink 的内存管理也主要指 TaskManager 的内存管理。</p><p>TaskManager 的资源（主要是内存）分为三个层级，分别是最粗粒度的进程级（TaskManager 进程本身），线程级（TaskManager 的 slot）和 SubTask 级（多个 SubTask 共用一个 slot）。</p><center><p><img src="/img/flink-new-mem-management/taskmanager-memory-hierachy.png" alt="图1.TaskManager 资源层级" title="图1.TaskManager 资源层级"></p></center><p>在进程级，TaskManager 将内存划分为以下几块:</p><ul><li>Heap Memory: 由 JVM 直接管理的 heap 内存，留给用户代码以及没有显式内存管理的 Flink 系统活动使用（比如 StateBackend、ResourceManager 的元数据管理等）。</li><li>Network Memory: 用于网络传输（比如 shuffle、broadcast）的内存 Buffer 池，属于 Direct Memory 并由 Flink 管理。</li><li>Cutoff Memory: 在容器化环境下进程使用的物理内存有上限，需要预留一部分内存给 JVM 本身，比如线程栈内存、class 等元数据内存、GC 内存等。</li><li>Managed Memory: 由 Flink Memory Manager 直接管理的内存，是数据在 Operator 内部的物理表示。Managed Memory 可以被配置为 on-heap 或者 off-heap (direct memory)的，off-heap 的 Managed Memory 将有效减小 JVM heap 的大小并减轻 GC 负担。目前 Managed Memory 只用于 Batch 类型的作业，需要缓存数据的操作比如 hash join、sort 等都依赖于它。</li></ul><p>根据 Managed Memory 是 on-heap 或 off-heap 的不同，TaskManager 的进程内存与 JVM 内存分区关系分别如下:</p><center><p><img src="/img/flink-new-mem-management/taskmanager-memory-partitions.png" alt="图2.TaskManager 内存分区" title="图2.TaskManager 内存分区"></p></center><p>在线程级别，TaskManager 会将其资源均分为若干个 slot (在 YARN/Mesos/K8s 环境通常是每个 TaskManager 只包含 1 个 slot)，没有 slot sharing 的情况下每个 slot 可以运行一个 SubTask 线程。除了 Managed Memory，属于同一 TaskManager 的 slot 之间基本是没有资源隔离的，包括 Heap Memory、Network Buffer、Cutoff Memory 都是共享的。所以目前 slot 主要的用处是限制一个 TaskManager 的 SubTask 数。</p><p>从作为资源提供者的 TaskManager 角度看， slot 是资源的最小单位，但从使用者 SubTask 的角度看，slot 的资源还可以被细分，因为 Flink 的 slot sharing 机制。默认情况下， Flink 允许多个 SubTask 共用一个 slot 的资源，前提是这些 SubTask 属于同一个 Job 的不同 Task。以官网的例子来说，一个拓扑为 <code>Source(6)-map(6)-keyby/window/apply(6)-sink(1)</code> 的作业，可以运行在 2 个 slot 数为 3 的 TaskManager 上（见图3）。</p><center><p><img src="/img/flink-new-mem-management/slot-sharing.png" alt="图3.TaskManager Slot Sharing" title="图3.TaskManager Slot Sharing"></p></center><p>这样的好处是，原本一共需要 19 个 slot 的作业，现在只需要作业中与 Task 最大并行度相等的 slot， 即 6 个 slot 即可运行起来。此外因为不同 Task 通常有不同的资源需求，比如 source 主要使用网络 IO，而 map 可能主要需要 cpu，将不同 Task 的 subtask 放到同一 slot 中有利于资源的充分利用。</p><p>可以看到，目前 Flink 的内存管理是比较粗粒度的，资源隔离并不是很完整，而且在不同部署模式下（Standalone/YARN/Mesos/K8s）或不同计算模式下（Streaming/Batch）的内存分配也不太一致，为深度平台化及大规模应用增添了难度。</p><h1 id="Flink-1-10-细粒度的资源管理"><a href="#Flink-1-10-细粒度的资源管理" class="headerlink" title="Flink 1.10 细粒度的资源管理"></a>Flink 1.10 细粒度的资源管理</h1><p>为了改进 Flink 内存管理机制，阿里巴巴的工程师结合 Blink 的优化经验分别就进程、线程、SubTask（Operator）三个层面分别提出了 3 个 FLIP，均以 1.10 为目标 release 版本。下面将逐一介绍每个提案的内容。</p><h2 id="FLIP-49-统一-TaskExecutor-的内存配置"><a href="#FLIP-49-统一-TaskExecutor-的内存配置" class="headerlink" title="FLIP-49: 统一 TaskExecutor 的内存配置"></a>FLIP-49: 统一 TaskExecutor 的内存配置</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>TaskExecutor 在不同部署模式下具体负责作业执行的进程，可以简单视为 TaskManager。目前 TaskManager 的内存配置存在不一致以及不够直观的问题，具体有以下几点:</p><ul><li>流批作业内容配置不一致。Managed Memory 只覆盖 DataSet API，而 DataStream API 的则主要使用 JVM 的 heap 内存，相比前者需要更多的调优参数且内存消耗更难把控。</li><li>RocksDB 占用的 native 内存并不在内存管理里，导致使用 RocksDB 时内存需要很多手动调优。</li><li>不同部署模式下，Flink 内存计算算法不同，并且令人难以理解。</li></ul><p>针对这些问题，FLIP-49[4] 提议通过将 Managed Memory 的用途拓展至 DataStream 以解决这个问题。DataStream 中主要占用内存的是 StateBackend，它可以从管理 Managed Memory 的 MemoryManager 预留部分内存或分配内存。通过这种方式同一个 Flink 配置可以运行 Batch 作业和 Streaming 作业，有利于流批统一。</p><h3 id="改进思路"><a href="#改进思路" class="headerlink" title="改进思路"></a>改进思路</h3><p>总结一下，目前 DataStream 和 DataSet 的内存使用可以分为如下几类:</p><table><thead><tr><th>场景</th><th>内存类型</th><th>内存分配方式</th><th>内存限制</th></tr></thead><tbody><tr><td>Streaming(Heap-based StateBackend)</td><td>heap</td><td>隐式分配</td><td>小于 JVM heap size</td></tr><tr><td>Streaming(RocksDB StateBackend)</td><td>off-heap</td><td>隐式分配</td><td>只受限于机器内存</td></tr><tr><td>Batch</td><td>heap 或 off-heap</td><td>显式通过 MemoryManager 分配</td><td>不大于显式分配的内存数</td></tr></tbody></table><p>可以看到目前 DataStream 作业的内存分配没有经过 MemoryManager 而是直接向 JVM 申请，容易造成 heap OOM 或者物理内存占用过大[3]，因此直接的修复办法是让 MemoryManager 了解到 StateBackend 的内存占用。这会有两种方式，一是直接通过 MemoryManager 申请内存，二是仍使用隐式分配的办法，但需要通知 MemoryManager 预留这部分内存。此外 MemoryManager 申请 off-heap 的方式也会有所变化，从 <code>ByteBuffer#allocateDirect()</code> 变为 <code>Unsafe#allocateMemory()</code>，这样的好处是显式管理的 off-heap 内存可以从 JVM 的 <code>-XX:MaxDirectMemorySize</code> 参数限制中分离出来。</p><p>另外 MemoryManager 将不只可以被配置为 heap/off-heap，而是分别拥有对应的内存池。这样的好处是在同一个集群可以运行要求不同类型内存的作业，比如一个 FsStateBackend 的 DataStream 作业和一个 RocksDBStateBackend 的 DataStream 作业。heap/off-heap 的比例可以通过参数配置，1/0 则代表了完全的 on-heap 或者 off-heap。</p><p>改进之后 TaskManager 的各内存分区如下:</p><center><p><img src="/img/flink-new-mem-management/taskmanager-memory-partitions.png" alt="图4.TaskManager 新内存结构" title="TaskManager 新内存结构"></p></center><table><thead><tr><th>分区</th><th>内存类型</th><th>描述</th><th>配置项</th><th>默认值</th></tr></thead><tbody><tr><td>Framework Heap Memory</td><td>heap</td><td>Flink 框架消耗的 heap 内存</td><td>taskmanager.memory.<br>framework.heap</td><td>128mb</td></tr><tr><td>Task Heap Memory</td><td>heap</td><td>用户代码使用的 heap 内存</td><td>taskmanager.memory.<br>task.heap</td><td>无</td></tr><tr><td>Task Off-Heap Memory</td><td>off-heap</td><td>用户代码使用的 off-heap 内存</td><td>taskmanager.memory.<br>task.offheap</td><td>0b</td></tr><tr><td>Shuffle Memory</td><td>off-heap</td><td>网络传输/suffle 使用的内存</td><td>taskmanager.memory.<br>shuffle.[min/max/fraction]</td><td>min=64mb, max=1gb, fraction=0.1</td></tr><tr><td>Managed Heap Memory</td><td>heap</td><td>Managed Memory 使用的 heap 内存</td><td>taskmanager.memory.<br>managed.[size/fraction]</td><td>fraction=0.5</td></tr><tr><td>Managed Off-heap Memory</td><td>off-heap</td><td>Managed Memory 使用的 off-heap 内存</td><td>taskmanager.memory.<br>managed.offheap-fraction</td><td>0.0</td></tr><tr><td>JVM Metaspace</td><td>off-heap</td><td>JVM metaspace 使用的 off-heap 内存</td><td>taskmanager.memory.jvm-metaspace</td><td>192mb</td></tr><tr><td>JVM Overhead</td><td>off-heap</td><td>JVM 本身使用的内存</td><td>taskmanager.memory.jvm-overhead.[min/max/fraction]</td><td>min=128mb, max=1gb, fraction=0.1)</td></tr><tr><td>Total Flink Memory</td><td>heap &amp; off-heap</td><td>Flink 框架使用的总内存，是以上除 JVM Metaspace 和 JVM Overhead 以外所有分区的总和</td><td>taskmanager.memory.total-flink.size</td><td>无</td></tr><tr><td>Total Process Memory</td><td>heap &amp; off-heap</td><td>进程使用的总内存，是所有分区的总和，包括 JVM Metaspace 和 JVM Overhead</td><td>taskmanager.memory.total-process.size</td><td>无</td></tr></tbody></table><p>值得注意的是有 3 个分区是没有默认值的，包括 Framework Heap Memory、Total Flink Memory 和 Total Process Memory，它们是决定总内存的最关键参数，三者分别满足不同部署模式的需要。比如在 Standalone 默认下，用户可以配置 Framework Heap Memory 来限制用户代码使用的 heap 内存；而在 YARN 部署模式下，用户可以通过配置 YARN container 的资源来间接设置 Total Process Memory。</p><h2 id="FLIP-56-动态-slot-分配"><a href="#FLIP-56-动态-slot-分配" class="headerlink" title="FLIP-56: 动态 slot 分配"></a>FLIP-56: 动态 slot 分配</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p>目前 Flink 的资源是预先静态分配的，也就是说 TaskManager 进程启动后 slot 的数目和每个 slot 的资源数都是固定的而且不能改变，这些 slot 的生命周期和 TaskManager 是相同的。Flink Job 后续只能向 TaskManager 申请和释放这些 slot，而没有对 slot 资源数的话语权。</p><center><p><img src="/img/flink-new-mem-management/static-slot.png" alt="图5. 静态 slot 分配" title="图5. 静态 slot 分配"></p></center><p>这种粗粒度的资源分配假定每个 SubTask 的资源需求都是大致相等的，优点是较为简单易用，缺点在于如果出现 SubTask 的资源需求有倾斜的情况，用户则需要按其中某个 SubTask 最大资源来配置总体资源，导致资源浪费且不利于多个作业复用相同 Flink 集群。</p><h3 id="改进思路-1"><a href="#改进思路-1" class="headerlink" title="改进思路"></a>改进思路</h3><p>FLIP-56[5] 提议通过将 TaskManager 的资源改为动态申请来解决这个问题。TaskManager 启动的时候只需要确定资源池大小，然后在有具体的 Flink Job 申请资源时再按需动态分配 slot。Flink Job 申请 slot 时需要附上资源需求，TaskManager 会根据该需求来确定 slot 资源。</p><center><p><img src="/img/flink-new-mem-management/dynamic-slot.png" alt="图6. 动态 slot 分配" title="图6. 动态 slot 分配"></p></center><p>值得注意的是，slot 资源需求可以是 <code>unknown</code>。提案引入了一个新的默认 slot 资源要求配置项，它表示一个 slot 占总资源的比例。如果 slot 资源未知，TaskManager 将按照该比例切分出 slot 资源。为了保持和现有静态 slot 模型的兼容性，如果该配置项没有被配置，TaskManager 会根据 slot 数目均等分资源生成 slot。</p><p>目前而言，该 FLIP 主要涉及到 Managed Memory 资源，TaskManager 的其他资源比如 JVM heap 还是多个 slot 共享的。</p><h2 id="FLIP-53-细粒度的算子资源管理"><a href="#FLIP-53-细粒度的算子资源管理" class="headerlink" title="FLIP-53: 细粒度的算子资源管理"></a>FLIP-53: 细粒度的算子资源管理</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p>FLIP-56 使得 slot 的资源可以根据实际需求确定，而 FLIP-53 则探讨了 Operator (算子)层面如何表达资源需求，以及如何根据不同 Operator 的设置来计算出总的 slot 资源。</p><p>目前 DataSet API 以及有可以指定 Operator 资源占比的方法（TaskConfig 和 ChainedDriver），因此这个 FLIP 只涉及到 DataStream API 和 Table/SQL API (先在 Blink Planner 实现)。不过提案并没有包括用户函数 API 上的变化（类似新增 <code>dataStream.setResourceSpec()</code> 函数），而是主要讨论 DataStream 到 StreamGraph 的翻译过程如何计算 slot 资源。改进完成后，这三个 API 的资源计算逻辑在底层会是统一的。</p><h3 id="改进思路-2"><a href="#改进思路-2" class="headerlink" title="改进思路"></a>改进思路</h3><p>要理解 Flink 内部如何划分资源，首先要对 Flink 如何编译用户代码并部署到分布式环境的过程有一定的了解。</p><center><p><img src="/img/flink-new-mem-management/flink-graph.jpg" alt="图7. Flink 作业编译部署流程" title="图7. Flink 作业编译部署流程"></p></center><p>以 DataStream API 为例，用户为 DataStream 新增 Operator 时，Flink 在底层会将以一个对应的 Transform 来封装。比如 <code>dataStream.map(new MyMapFunc())</code> 会新增一个 <code>OneInputTransformation</code> 实例，里面包括了序列化的 <code>MyMapFunc</code> 实例，以及 Operator 的配置（包括名称、uid、并行度和资源等），并且记录了它在拓扑中的前一个 Transformation 作为它的数据输入。</p><p>当 <code>env.execute()</code> 被调用时，在 client 端 StreamGraphGenerator 首先会遍历 Transformation 列表构造出 StreamGraph 对象（每个 Operator 对应一个 StreamNode），然后 StreamingJobGraphGenerator 再将 StreamGraph 翻译成 DataStream/DataSet/Table/SQL 通用的 JobGraph（此时会应用 chaining policy 将可以合并的 Operator 合并为 OperatorChain，每个 OperatorChain 或不能合并的 Operator 对应一个 JobVertex），并将其传给 JobManager。</p><p>JobManager 收到 JobGraph 后首先会将其翻译成表示运行状态的 ExecutionGraph，ExecutionGraph 的每个节点称为 ExecutionJobVertex，对应一个 JobVertex。ExecutionJobVertex 有一个或多个并行度且可能被调度和执行多次，其中一个并行度的一次执行称为 Execution，JobManager 的 Scheduler 会为每个 Execution 分配 slot。</p><p>细粒度的算子资源管理将以下面的方式作用于目前的流程:</p><ol><li>用户使用 API 构建的 Operator（以 Transformation 表示）会附带 <code>ResourceSpecs</code>，描述该 Operator 需要的资源，默认为 <code>unknown</code>。</li><li>当生成 JobGraph 的时候，StreamingJobGraphGenerator 根据 <code>ResourceSpecs</code> 计算出每个 Operator 占的资源比例（主要是 Managed Memory 的比例）。</li><li>进行调度的时候，Operator 的资源将被加总成为 Task 的 <code>ResourceProfiles</code> （包括 Managed Memory 和根据 Task 总资源算出的 Network Memory）。这些 Task 会被划分为 SubTask 实例被部署到 TaskManager 上。</li><li>当 TaskManager 启动 SubTask    的时候，会根据各 Operator 的资源占比划分 Slot Managed Memory。划分的方式可以是用户指定每个 Operator 的资源占比，或者默认均等分。</li></ol><p>值得注意的是，Scheduler 的调度有分 EAGER 模式和 LAZY_FROM_SOURCE 两种模式，分别用于 Stream 作业和 Batch 作业，它们会影响到 slot 的资源计算。Stream 类型的作业要求所有的 Operator 同时运行，因此资源的需求是急切的（EAGER）；而 Batch 类型的作业可以划分为多个阶段，不同阶段的 Operator 不需要同时运行，可以等输入数据准备好了再分配资源（LAZY_FROM_SOURCE）。这样的差异导致如果要充分利用 slot，Batch 作业需要区分不同阶段的 Task，同一时间只考虑一个阶段的 Task 资源。</p><p>解决的方案是将 slot sharing 的机制拓展至 Batch 作业。默认情况下 Stream 作业的所有 Operator 都属于 default sharing group，所以全部 Operator 都能共用都一个 slot。对于 Batch 作业而言，我们将整个 JobGraph 根据 suffle 划分为一至多个 Region，每个 Region 属于独立的 sharing group，因而不会被放到同一个 slot 里面。</p><center><p><img src="/img/flink-new-mem-management/slot-sharing-group.png" alt="图8. 不同作业类型的 Slot Sharing Group" title="图8. 不同作业类型的 Slot Sharing Group"></p></center><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>随着 Flink 的越来越大规模地被应用于各种业务，目前资源管理机制的灵活性、易用性不足的问题越发凸显，新的细粒度资源管理机制将大大缓解这个问题。此外，新资源管理机制将统一流批两者在 runtime 层资源管理，这也为将最终的流批统一打下基础。对于普通用户而言，这里的大多数变动是透明的，主要的影响应该是出现新的内存相关的配置项需要了解一下。</p><p>1.<a href="https://issues.apache.org/jira/browse/FLINK-13477" target="_blank" rel="external">[FLINK-13477] Containerized TaskManager killed because of lack of memory overhead</a><br>2.<a href="https://www.bilibili.com/video/av68914405/?p=3" target="_blank" rel="external">机遇与挑战：Apache Flink 资源管理机制解读与展望</a><br>3.<a href="https://issues.apache.org/jira/browse/FLINK-7289" target="_blank" rel="external">[FLINK-7289] Memory allocation of RocksDB can be problematic in container environments</a><br>4.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-49%3A+Unified+Memory+Configuration+for+TaskExecutors?src=contextnavpagetreemode" target="_blank" rel="external">FLIP-49: Unified Memory Configuration for TaskExecutors</a><br>5.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-56%3A+Dynamic+Slot+Allocation" target="_blank" rel="external">FLIP-56: Dynamic Slot Allocation</a><br>6.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-53%3A+Fine+Grained+Operator+Resource+Management" target="_blank" rel="external">FLIP-53: Fine Grained Operator Resource Management</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;相信不少读者在开发 Flink 应用时或多或少会遇到在内存调优方面的问题，比如在我们生产环境中遇到最多的 TaskManager 在容器化环境下占用超出容器限制的内存而被 YARN/Mesos kill 掉[1]，再比如使用 heap-based StateBackend 情况下 State 过大导致 GC 频繁影响吞吐。这些问题对于不熟悉 Flink 内存管理的用户来说十分难以排查，而且 Flink 晦涩难懂的内存配置参数更是让用户望而却步，结果是往往将内存调大至一个比较浪费的阈值以尽量避免内存问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.9 Release 解读</title>
    <link href="https://link3280.github.io/2019/09/21/Flink-1-9-Release-%E8%A7%A3%E8%AF%BB/"/>
    <id>https://link3280.github.io/2019/09/21/Flink-1-9-Release-解读/</id>
    <published>2019-09-21T03:00:41.000Z</published>
    <updated>2019-09-21T03:02:57.160Z</updated>
    
    <content type="html"><![CDATA[<p>距离上个发行版近 4 个月后，不久前 Apache Flink 发行了 1.9 系列的首个版本。Flink 1.9 是个有重要意义的版本，它初步合并了 Blink 的大部分新特性（虽然是预览特性），其中包括 Blink planner、Hive 集成、Python Table API 和新版 Web UI。此外，1.9 版本正式引入了 Savepoint Processor 来提供离线访问和修改 State 的能力，这也是社区呼声比较高的一个特性。下文将选取一些笔者认为比较重要的特性、improvement 和 bugfix 进行解读（主要集中在实时场景），详细的变动进参考 [1]。</p><a id="more"></a><h1 id="Blink-Planner"><a href="#Blink-Planner" class="headerlink" title="Blink Planner"></a>Blink Planner</h1><p>在阿里巴巴内部，Table API 和 SQL API 是开发 Flink 应用使用得最多的 API，因此阿里巴巴也花费了大量的精力在这两个 API 的优化上，其中最重要的一个便是 Blink Planner。Planner 是 SQL/Table API 和 runtime 的桥梁，它负责将 SQL/Table API 翻译为物理执行计划，也就是 runtime 的 operator。</p><p><center><img src="/img/flink-1.9-release/planner-architecture.jpeg" alt="图一. Planner 架构" title="图一. Planner 架构"></center></p><p>比起 Flink 原生的 Planner，Blink Planner 主要有以下的优势: </p><ol><li>流批统一。无论是 Stream 作业还是 Batch 作业都会直接被翻译为 StreamGraph，也就是常说的将批处理作为流处理的特例。</li><li>解耦 SQL/Table API 和 DataStream/DataSet API。这与流批统一是紧密联系的，因为 DataStream/DataSet 两者是独立的，基于它们很难建立简洁的流批统一架构。</li><li>建立统一的 Table/SQL 入口，简化当前多个 *Environment 的复杂 API 设计。</li><li>更多的 SQL 优化规则，比如 Join 的谓词下推和多余的 aggregate 移除。</li></ol><p>Blink Planner 作为预览功能在 1.9 版本发布，用户可以通过引入依赖:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;1.9.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div><div class="line"></div><div class="line">&lt;dependency&gt;</div><div class="line">&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</div><div class="line">&lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;</div><div class="line">&lt;version&gt;1.9.0&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure><p>并在 main 函数中设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">EnvironmentSettings.useBlinkPlanner();</div></pre></td></tr></table></figure><p>来启用 Blink Planner。但值得注意的是目前 Blink Planner 仍有些未解决的 issue，比如不能在同一个 TableEnvironment 执行多条 SQL 语句，所以并不推荐在生产中使用。</p><h1 id="SQL-DDL"><a href="#SQL-DDL" class="headerlink" title="SQL DDL"></a>SQL DDL</h1><p>在以往版本，Flink SQL 只提供了 DSL 和 DML，而缺少了 DDL，这意味着我们不能在 SQL Client 或者其他 Flink SQL 程序中持久化创建的表。这个问题在 1.9 版本得到改善。1.9 支持了可用于 Batch 场景的标准 SQL DDL [2]，但因为 Stream 场景的 DDL 要求定义时间属性（Time Characteristic）、 Watermark 算法和 append mode 等额外的参数，需要进一步考虑实现细节，则预计在 1.10 版本再支持。尽管两种 DDL 要求的信息略有不同，但最终的目标是提供统一的语法，换句话说用户不需要区分定义的 Table 是基于无边界还有有边界的数据集，Flink 会自动根据上下文来判断。</p><h1 id="Hive-集成"><a href="#Hive-集成" class="headerlink" title="Hive 集成"></a>Hive 集成</h1><p>目前 Flink 主要作为实时计算引擎，在与离线数据仓库组件 Hive 的集成方面做得并不足够，但随着流批统一的大趋势，Flink 在批处理方面的潜力也会逐渐被挖掘，比如在阿里巴巴 Flink 已经被引用于大部分的批处理场景。在与 Hive 的集成方面，我们可以看到另一个主流分布式计算引擎 Apache Spark 做得很好，而 Flink 的 Hive 集成也会主要参考 SparkSQL 的特性，其中比较重要的比如打通 Flink Table/SQL API 和 Hive Metasstaore、支持 Flink 作为 Hive 的计算引擎。</p><p>整体工作会分为三个步骤来完成[3]：</p><ol><li><p>Flink SQL 基础集成<br>这包括在 Flink SQL API 集成 Hive 的基本功能，比如通过 Hive Connector 读写 Hive 表、支持 Hive 的数据类型（Timestamp/String 等）、可以在 Flink SQL 中使用 Hive 的内置函数和在 Flink SQL 中支持 Hive 的 DDL/DML 操作。这部分工作在 1.9 版本基本得到实现，主要的功能通过 HiveCatalog 封装的形式暴露给用户。顺带一提，为了支持 HiveCatalog，原本 Table API 的 Catalag 接口也进行了大量的重构。</p></li><li><p>Hive 兼容性<br>这部分可以概括为充分利用 Hive 的高级特性，其中包括完整的 Hive 数据类型支持、Thrift Server（类似于 Spark SQL Thrift Server）、在 Beeline 中支持 Flink 作为计算引擎、JDBC/ODBC 驱动支持、支持多种 Hive SerDe 等等。这部分的内容较多，将在后续版本逐步实现。</p></li><li><p>Flink SQL 优化<br>SQL 优化是 SQL 计算引擎老生常谈的课题，在 Hive on Flink SQL 上，可预见的优化工作当然也不少。这里的内容可以细分为三个方面：SQL 优化规则、Query metric（包括资源消耗、执行时间等）以及 Flink runtime 的 Task 调度和容错。</p></li></ol><p>总而言之，Flink 和 Hive 集成将极大地提升 Flink SQL 在 Batch 场景的应用能力，同时随着流批统一，尤其是 SQL 的流批统一，Streaming SQL 也将从中受益。</p><h1 id="终止-暂停作业"><a href="#终止-暂停作业" class="headerlink" title="终止/暂停作业"></a>终止/暂停作业</h1><p>目前 Flink 提供了 <code>cancel-with-savepoint</code> 的选项以方便用户在停止作业时持久化作业状态，在底层它会分为两步: 1. 触发 Savepoint 快照；2. Savepoint 完成后 cancel 作业。这会主要带来三个数据一致性上的问题。</p><p>首先，Flink 依靠两步提交（Tow-Phase Commit）来确保 Exactly-Once，简单来说 Operator 收到 checkpoint barrier 时进行 State 快照，等全部 Operator 都完成快照后再统一由 JobManager 通知 commit。这里的 commit 是一个 best-efford 的操作，不保证每次成功，而是依靠失败状况下的重试保证最终成功（eventually succeed）。如果一次 checkpoint 顺利完成，但某个 Operator commit 时失败了，Flink 作业会重启并从这次成功的 checkpoint 恢复，恢复完毕后再次触发 JobManager 的 checkpoint 完成通知。问题在于在 <code>cancel-with-savepoint</code> 场景，savepoint 完成之后 Task 立刻被取消，这很可能发生在 Operator 进行 commit 之前，导致 commit 被跳过。</p><p>其次，checkpoint/savepoint 过程并不阻止 source 摄入数据，在 <code>cancel-with-savepoint</code> 取消作业时作业通常会多处理一部分数据，如果使用 At-Least-Once 的 sink 则会造成数据的重复。</p><p>最后，在 event-time 窗口统计的业务场景下，窗口数据的输出依赖于 watermark 的提升，如果用户希望在停止作业时输出目前的结果，比如作业准备下线需要保存最新的计算结果，则无法简单地做到这点，也就是说停止作业时总是得不到最新数据。</p><p>FLIP-34[4] 通过重构作业停止的流程设计解决了以上的问题。第一个问题根源在于 cancel 命令应该在 Task commit 之后再执行；第二个问题根源在于 Savepoint 设计上不能阻止消费，因为 Savepoint 可能被简单用于保存某个时间点的状态，之后并不一定会 cancel 作业；第三个问题类似于第二个问题，没有区分是要临时停止还是完全下线。</p><p>针对第二、第三点，FLIP-34 引入 <code>TERMINATE</code> 和 <code>SUSPEND</code> 两种停止作业的方式，前者表示完全下线希望全部提交中间状态，后者表示临时下线，比如维护升级等。两者都会触发 Source 发出 EOS (End of Stream)的信号，令 Task 变为 <code>FINISHED</code> 状态（目前停止作业后 Task 是 <code>CANCELED</code> 状态）。此外 <code>TERMINATE</code> 停止还会令 Source 额外发出一个 LONG 最大值的 Watermak，这会强制触发所有基于 event time 的操作，比如 event time 的窗口统计。</p><p><center><img src="/img/flink-1.9-release/new-stop-properties.png" alt="图二. TERMINATE/SUSPEND 总览" title="图二. TERMINATE/SUSPEND 总览"></center></p><p>针对第一点，Savepoint 由默认的异步执行改为同步执行，因此 Task 的 commit 会阻塞其他操作，包括 cancel 命令，保证 Task 的 commit 总是能保证被执行。</p><p>重构之后停止作业的流程如下:</p><ol><li>JobManager 从 Source 端开始触发同步的 Savepoint（包括 <code>TERMINATE</code> 或 <code>SUSPEND</code>）。</li><li>如果是 <code>TERMINATE</code> 停止，Source 会额外发出 MAX_WATERMARK。</li><li>TaskManager 收到 Savepoint barrier 之后执行同步的 Savepoint 快照，这会阻塞数据处理以及其他控制命令，直到快照结束。</li><li>TaskManager 向 JobManager 确认 Savepoint 成功。</li><li>JobManager     确认 Savepoint 完成并通知 TaskManager 进行第二阶段的 commit。</li><li>TaskManager 进行 commit，并移除阻塞状态。</li><li>Source 发出 EOS 信号，接受到信号的 TaskManager 依次关闭 Task。</li><li>JobManager 会在等待所有 Task 和 Job 变为 FINISHED 后关闭。</li></ol><p>在 CLI 端，Flink 新增了 <code>stop</code> 命令，其用法示例如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/flink stop [-p targetDirectory] [-d] &lt;jobID&gt;</div></pre></td></tr></table></figure><p>其中 <code>-p</code> 指定 Savepoint 的存储路径，<code>-d</code> 表示 <code>TERMINATE</code> 类型 stop。</p><h1 id="重构-WebUI"><a href="#重构-WebUI" class="headerlink" title="重构 WebUI"></a>重构 WebUI</h1><p>1.9 版本合并了 Blink 的新 WebUI。新的 WebUI 从风格来说更加明亮简洁，交互也更加友好。此外之前版本的 WebUI 对日志的支持比较糟糕，是直接拉取机器上的日志文件展示在页面上，这在日志比较大的时候很卡并容易导致标签页崩溃。</p><p>新版的 WebUI 提供分页并有一个类似 IDE 的全局缩略图，可以很容易定位到某个位置（似乎就是通过 VS Code 的库，所以也附送了语法高亮的特性）。</p><p><center><img src="/img/flink-1.9-release/blink-webui-logs.png" alt="图三. 新 WebUI 日志页面" title="图三. 新 WebUI 日志页面"></center></p><p>此外在 JobGraph 上也提供了更多的监控信息，比如 InQueue 和 OutQueue 被暴露到 Operator 上，方面用户排查作业瓶颈。</p><p><center><img src="/img/flink-1.9-release/blink-webui-operators.png" alt="图四. 新 WebUI Operators" title="图四. 新 WebUI Operators"></center></p><h1 id="Failover-策略"><a href="#Failover-策略" class="headerlink" title="Failover 策略"></a>Failover 策略</h1><p>在 1.9 以前 Flink 遇到 Task 错误的默认行为是重启整个 Job，在作业比较大的情况下可能会带来很高的 downtime 成本。针对这个问题 FLIP1[5] 提出了细粒度的容错机制，提供 RestartRegion 的 Failover 策略，使得只有与错误 Task 有数据联系的 Task 会被重启。RestartRegion 其实并不是在 1.9 版本才用，但在之前存在一个严重的 bug 导致使用 RestartRegion 并不会恢复作业状态，因此应用范围很有限。1.9 版本修复了这个问题，并且将 RestartRegion 设为默认的策略。</p><h1 id="Runtime-稳定性"><a href="#Runtime-稳定性" class="headerlink" title="Runtime 稳定性"></a>Runtime 稳定性</h1><p>1.9 版本修复了几个 Flink runtime 比较严重的 bug，将在这里统一整理。</p><h2 id="Per-job-集群在-job-失败后没有自动退出"><a href="#Per-job-集群在-job-失败后没有自动退出" class="headerlink" title="Per-job 集群在 job 失败后没有自动退出"></a>Per-job 集群在 job 失败后没有自动退出</h2><p>相信不少 Flink 用户都遇到过的一个问题是以 detached 模式，即 per-job cluster，运行作业时，作业失败后有一定几率出现 Yarn application 仍没有退出，变成类似一个没有作业的 session cluster。根据 FLINK-12219[6] 这个问题在于 JobManager 在退出先需要将作业的归档信息持久化（给 HistoryServer 用），但这个过程没有异常处理，如果出现出错将导致 JobManager 不执行关闭的命令。</p><h2 id="单-Task-包含多个-Stateful-Operator-时-RocksDB-StateBackend-会丢失数据"><a href="#单-Task-包含多个-Stateful-Operator-时-RocksDB-StateBackend-会丢失数据" class="headerlink" title="单 Task 包含多个 Stateful Operator 时 RocksDB StateBackend 会丢失数据"></a>单 Task 包含多个 Stateful Operator 时 RocksDB StateBackend 会丢失数据</h2><p>这个问题在于 RocksDBStateBackend 使用的本地快照目录以 VertexID 而不是 Operator 作为生成目录的参数，当多个 Stateful Operator 被 chained 到一起时它们的本地快照目录会冲突，此时 RocksDB 会前一个 Operator 的状态会被后一个覆盖，导致状态丢失。FLINK-12296[7] 通过重构 RocksDB 本地快照目录的生成规则来解决了这个问题。</p><h2 id="已取消-Checkpoint-可能造成作业失败"><a href="#已取消-Checkpoint-可能造成作业失败" class="headerlink" title="已取消 Checkpoint 可能造成作业失败"></a>已取消 Checkpoint 可能造成作业失败</h2><p>当一个 checkpoint 被取消时，其 checkpoint 目录（比如常见的 HDFS 目录）会被删除，但 Task 本地仍有可能会访问这个被删除的目录，此时会抛出 <code>org.apache.hadoop.ipc.RemoteException: java.io.IOException: Path doesn&#39;t exist</code> 的异常，导致作业失败。FLINK-11662[8] 通过忽略已取消的 checkpoint 抛出的异常来修复了这个问题。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12344601" target="_blank" rel="external">Flink 1.9 Release Changelog</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-6962" target="_blank" rel="external">[FLINK-6962] Add a create table SQL DDL</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10556" target="_blank" rel="external">[FLINK-10556] Integration with Apache Hive</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103090212" target="_blank" rel="external">FLIP-34: Terminate/Suspend Job with Savepoint</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures" target="_blank" rel="external">FLIP-1 : Fine Grained Recovery from Task Failures</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-12219" target="_blank" rel="external">[FLINK-12219] Yarn application can’t stop when flink job failed in per-job yarn cluster mode</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-12296" target="_blank" rel="external">[FLINK-12296] Data loss silently in RocksDBStateBackend when more than one operator(has states) chained in a single task</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11662" target="_blank" rel="external">[FLINK-11662] Discarded checkpoint can cause Tasks to fail</a></li><li><a href="https://ververica.cn/developers/flink1-9-hive/" target="_blank" rel="external">如何在 Flink 1.9 中使用 Hive？</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;距离上个发行版近 4 个月后，不久前 Apache Flink 发行了 1.9 系列的首个版本。Flink 1.9 是个有重要意义的版本，它初步合并了 Blink 的大部分新特性（虽然是预览特性），其中包括 Blink planner、Hive 集成、Python Table API 和新版 Web UI。此外，1.9 版本正式引入了 Savepoint Processor 来提供离线访问和修改 State 的能力，这也是社区呼声比较高的一个特性。下文将选取一些笔者认为比较重要的特性、improvement 和 bugfix 进行解读（主要集中在实时场景），详细的变动进参考 [1]。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入理解 Flink 容错机制</title>
    <link href="https://link3280.github.io/2019/07/28/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-Flink-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"/>
    <id>https://link3280.github.io/2019/07/28/深入理解-Flink-容错机制/</id>
    <published>2019-07-28T01:26:55.000Z</published>
    <updated>2019-07-29T13:28:56.119Z</updated>
    
    <content type="html"><![CDATA[<p>作为分布式系统，尤其是对延迟敏感的实时计算引擎，Apache Flink 需要有强大的容错机制，以确保在出现机器故障或网络分区等不可预知的问题时可以快速自动恢复并依旧能产生准确的计算结果。事实上，Flink 有一套先进的快照机制来持久化作业状态[1]，确保中间数据不会丢失，这通常需要和错误恢复机制（作业重启策略或 failover 策略）配合使用。在遇到错误时，Flink 作业会根据重启策略自动重启并从最近一个成功的快照（checkpoint）恢复状态。合适的重启策略可以减少作业不可用时间和避免人工介入处理故障的运维成本，因此对于 Flink 作业稳定性来说有着举足轻重的作用。下文就将详细解读 Flink 的错误恢复机制。</p><a id="more"></a><p>Flink 容错机制主要有作业执行的容错以及守护进程的容错两方面，前者包括 Flink runtime 的 ExecutionGraph 和 Execution 的容错，后者则包括 JobManager 和 TaskManager 的容错。</p><h1 id="作业执行容错"><a href="#作业执行容错" class="headerlink" title="作业执行容错"></a>作业执行容错</h1><p>众所周知，用户使用 Flink 编程 API（DataStream/DataSet/Table/SQL）编写的作业最终会被翻译为 JobGraph 对象再提交给 JobManager 去执行，而后者会将 JobGraph 结合其他配置生成具体的 Task 调度到 TaskManager 上执行。</p><p>相信不少读者应该见过来自官网文档的这张架构图（图1），它清晰地描绘了作业的分布式执行机制: 一个作业有多个 Operator，相互没有数据 shuffle 、并行度相同且符合其他优化条件的相邻 Operator 可以合并成 OperatorChain，然后每个 Operator 或者 OperatorChain 称为一个 JobVertex；在分布式执行时，每个 JobVertex 会作为一个 Task，每个 Task 有其并行度数目的 SubTask，而这些 SubTask 则是作业调度的最小逻辑单元。</p><center><p><img src="/img/flink-recovery/img1.distributed-dataflow.png" alt="图1. 作业的分布式执行" title="图1. 作业的分布式执行"></p></center><p>该图主要从 TaskManager 角度出发，而其实在 JobManager 端也存在一个核心的数据结构来映射作业的分布式执行，即 ExecutionGraph。ExecutionGraph 类似于图中并行视角的 Streaming Dataflow，它代表了 Job 的一次执行。从某种意义上讲，如果 JobGraph 是一个类的话，ExecutionGraph 则是它的一个实例。ExecutionGraph 中包含的节点称为 ExecutionJobVertex，对应 JobGrap 的一个 JobVertex 或者说图中的一个 Task。ExecutionJobVertex 可以有多个并行实例，即 ExecutionVertex，对应图中的一个 SubTask。在一个 ExecutionGraph 的生命周期中，一个 ExecutionVertex 可以被执行（重启）多次，每次则称为一个 Execution。小结一下，ExecutionGraph 对应 Flink Job 的一次执行，Execution 对应 SubTask 的一次执行。</p><p>相对地，Flink 的错误恢复机制分为多个级别，即 Execution 级别的 Failover 策略和 ExecutionGraph 级别的 Job Restart 策略。当出现错误时，Flink 会先尝试触发范围小的错误恢复机制，如果仍处理不了才会升级为更大范围的错误恢复机制，具体可以用下面的序列图来表达（其中省略了Exection 和 ExecutionGraph 的非关键状态转换）。</p><p><img src="/img/flink-recovery/img2.fault-tolerance-overview.png" alt="图2. 作业执行容错" title="图2. 作业执行容错"><br>当 Task 发生错误，TaskManager 会通过 RPC 通知 JobManager，后者将对应 Execution 的状态转为 <code>failed</code> 并触发 Failover 策略。如果符合 Failover 策略，JobManager 会重启 Execution，否则升级为 ExecutionGraph 的失败。ExecutionGraph 失败则进入 <code>failing</code> 的状态，由 Restart 策略决定其重启（<code>restarting</code> 状态）还是异常退出（<code>failed</code> 状态）。</p><p>下面分别分析两个错误恢复策略的场景及实现。</p><h2 id="Task-Failover-策略"><a href="#Task-Failover-策略" class="headerlink" title="Task Failover 策略"></a>Task Failover 策略</h2><p>作为计算的最小执行单位，Task 错误是十分常见的，比如机器故障、用户代码抛出错误或者网络故障等等都可能造成 Task 错误。对于分布式系统来说，通常单个 Task 错误的处理方式是将这个 Task 重新调度至新的 worker 上，不影响其他 Task 和整体 Job 的运行，然而这个方式对于流处理的 Flink 来说并不可用。</p><p>Flink 的容错机制主要分为从 checkpoint 恢复状态和重流数据两步，这也是为什么 Flink 通常要求数据源的数据是可以重复读取的。对于重启后的新 Task 来说，它可以通过读取 checkpoint 很容易地恢复状态信息，但是却不能独立地重流数据，因为 checkpoint 是不包含数据的，要重流数据只可以要求依赖到的全部上游 Task 重新计算，通常来说会一直追溯到数据源 Task。熟悉 Spark 的同学大概会联想到 Spark 的血缘机制。简单来说，Spark 依据是否需要 shuffle 将作业分划为多个 Stage，每个 Stage 的计算都是独立的 Task，其结果可以被缓存起来。如果某个 Task 执行失败，那么它只要重读上个 Stage 的计算缓存结果即可，不影响其他 Task 的计算。Spark 可以独立地恢复一个 Task，很大程度上是因为它的批处理特性，这允许了作业通过缓存中间计算结果来解耦上下游 Task 的联系。而 Flink 作为流计算引擎，显然是无法简单做到这点的。</p><p>要做到细粒度的错误恢复机制，减小单个 Task 错误对于整体作业的影响，Flink 需要实现一套更加复杂的算法，也就是 FLIP-1 [2] 引入的 Task Failover 策略。Task Failover 策略目前有三个，分别是<br><code>RestartAll</code>、<code>RestartIndividualStrategy</code> 和 <code>RestartPipelinedRegionStrategy</code>。</p><center><p><img src="/img/flink-recovery/img3.task-failover-strategies.png" alt="图3. Restart Region 策略重启有数据交换的 Task" title="图3. Restart Region 策略重启有数据交换的 Task"></p></center><ul><li>RestartAll: 重启全部 Task，是恢复作业一致性的最安全策略，会在其他 Failover 策略失败时作为保底策略使用。目前是默认的 Task Failover 策略。</li><li>RestartPipelinedRegionStrategy: 重启错误 Task 所在 Region 的全部 Task。Task Region 是由 Task 的数据传输决定的，有数据传输的 Task 会被放在同一个 Region，而不同 Region 之间没有数据交换。</li><li>RestartIndividualStrategy: 恢复单个 Task。因为如果该 Task 没有包含数据源，这会导致它不能重流数据而导致一部分数据丢失。考虑到至少提供准确一次的投递语义，这个策略的使用范围比较有限，只应用于 Task 间没有数据传输的作业。不过也有部分业务场景可能需要这种 at-most-once 的投递语义，比如对延迟敏感而对数据一致性要求相对低的推荐系统。</li></ul><p>总体来说，<code>RestartAll</code> 较为保守会造成资源浪费，而 <code>RestartIndividualStrategy</code> 则太过激进不能保证数据一致性，而 <code>RestartPipelinedRegionStrategy</code> 重启的是所有 Task 里最小必要子集，其实是最好的 Failover 策略。而实际上 Apache 社区也正准备在 1.9 版本将其设为默认的 Failover 策略[3]。不过值得注意的是，在 1.9 版本以前 <code>RestartPipelinedRegionStrategy</code> 有个严重的问题是在重启 Task 时并不会恢复其状态[4]，所以请在 1.9 版本以后才使用它，除非你在跑一个无状态的作业。</p><h2 id="Job-Restart-策略"><a href="#Job-Restart-策略" class="headerlink" title="Job Restart 策略"></a>Job Restart 策略</h2><p>如果 Task 错误最终触发了 Full Restart，此时 Job Restart 策略将会控制是否需要恢复作业。Flink 提供三种 Job 具体的 Restart Strategy。</p><ul><li>FixedDelayRestartStrategy: 允许指定次数内的 Execution 失败，如果超过该次数则导致 Job 失败。FixedDelayRestartStrategy 重启可以设置一定的延迟，以减少频繁重试对外部系统带来的负载和不必要的错误日志。目前 FixedDelayRestartStrategy 是默认的 Restart Strategy。</li><li>FailureRateRestartStrategy: 允许在指定时间窗口内的指定次数内的 Execution 失败，如果超过这个频率则导致 Job 失败。同样地，FailureRateRestartStrategy 也可以设置一定的重启延迟。</li><li>NoRestartStrategy: 在 Execution 失败时直接让 Job 失败。</li></ul><p>目前的 Restart Strategy 可以基本满足“自动重启挂掉的作业”这样的简单需求，然而并没有区分作业出错的原因，这导致可能会对不可恢复的错误（比如用户代码抛出的 NPE 或者某些操作报 Permission Denied）进行不必要的重试，进一步的后果是没有第一时间退出，可能导致用户没有及时发现问题，其外对于资源来说也是一种浪费，最后还可能导致一些副作用（比如有些 at-leaset-once 的操作被执行多次）。</p><p>对此，社区在 1.7 版本引入了 Exception 的分类[5]，具体会将 Runtime 抛出的 Exception 分为以下几类:</p><ul><li>NonRecoverableError: 不可恢复的错误。不对此类错误进行重试。</li><li>PartitionDataMissingError: 当前 Task 读不到上游 Task 的某些数据，需要上游 Task 重跑和重发数据。</li><li>EnvironmentError: 执行环境的错误，通常是 Flink 以外的问题，比如机器问题、依赖问题。这种错误的一个明显特征是会在某些机器上执行成功，但在另外一些机器上执行失败。Flink 后续可以引入黑名单机器来更聪明地进行 Task 调度以暂时避免这类问题的影响。</li><li>RecoverableError: 可恢复错误。不属于上述类型的错误都暂设为可恢复的。</li></ul><p>其实这个分类会应用于 Task Failover 策略和 Job Restart 策略，不过目前只有后者会分类处理，而且 Job Restart 策略对 Flink 作业的稳定性影响显然更大，因此放在这个地方讲。值得注意的是，截至目前（1.8 版本）这个分类只处于很初级的阶段，像 NonRecoverable 只包含了作业 State 命名冲突等少数几个内部错误，而 PartitionDataMissingError 和 EnvironmentError 还未有应用，所以绝大多数的错误仍是 RecoverableError。</p><h1 id="守护进程容错"><a href="#守护进程容错" class="headerlink" title="守护进程容错"></a>守护进程容错</h1><p>对于分布式系统来说，守护进程的容错是基本要求而且已经比较成熟，基本包括故障检测和故障恢复两个部分：故障检测通常通过心跳的方式来实现，心跳可以在内部组件间实现或者依赖于 zookeeper 等外部服务；而故障恢复则通常要求将状态持久化到外部存储，然后在故障出现时用于初始化新的进程。</p><p>以最为常用的 on YARN 的部署模式来讲，Flink 关键的守护进程有 JobManager 和 TaskManager 两个，其中 JobManager 的主要职责协调资源和管理作业的执行分别为 ResourceManager 和 JobMaster 两个守护线程承担，三者之间的关系如下图所示。</p><center><p><img src="/img/flink-recovery/img4.flip-6-architecture.png" alt="图4. ResourceManager、JobMaster 和 TaskManager 三者关系" title="图4. ResourceManager、JobMaster 和 TaskManager 三者关系"></p></center><p>在容错方面，三个角色两两之间相互发送心跳来进行共同的故障检测[7]。此外在 HA 场景下，ResourceManager 和 JobMaster 都会注册到 zookeeper 节点上以实现 leader 锁。</p><h2 id="TaskManager-的容错"><a href="#TaskManager-的容错" class="headerlink" title="TaskManager 的容错"></a>TaskManager 的容错</h2><p>如果 ResouceManager 通过心跳超时检测到或者通过集群管理器的通知了解到 TaskManager 故障，它会通知对应的 JobMaster 并启动一个新的 TaskManager 以做代替。注意 ResouceManager 并不关心 Flink 作业的情况，这是 JobMaster 的职责去管理 Flink 作业要做何种反应。</p><p>如果 JobMaster 通过 ResouceManager 的通知了解到或者通过心跳超时检测到 TaskManager 故障，它首先会从自己的 slot pool 中移除该 TaskManager，并将该 TaskManager 上运行的所有 Tasks 标记为失败，从而触发 Flink 作业执行的容错机制以恢复作业。</p><p>TaskManager 的状态已经写入 checkpoint 并会在重启后自动恢复，因此不会造成数据不一致的问题。</p><h2 id="ResourceManager-的容错"><a href="#ResourceManager-的容错" class="headerlink" title="ResourceManager 的容错"></a>ResourceManager 的容错</h2><p>如果 TaskManager 通过心跳超时检测到 ResourceManager 故障，或者收到 zookeeper 的关于 ResourceManager 失去 leadership 通知，TaskManager 会寻找新的 leader ResourceManager 并将自己重启注册到其上，期间并不会中断 Task 的执行。</p><p>如果 JobMaster 通过心跳超时检测到 ResourceManager 故障，或者收到 zookeeper 的关于 ResourceManager 失去 leadership 通知，JobMaster 同样会等待新的 ResourceManager 变成 leader，然后重新请求所有的 TaskManager。考虑到 TaskManager 也可能成功恢复，这样的话 JobMaster 新请求的 TaskManager 会在空闲一段时间后被释放。</p><p>ResourceManager 上保持了很多状态信息，包括活跃的 container、可用的 TaskManager、TaskManager 和 JobMaster 的映射关系等等信息，不过这些信息并不是 ground truth，可以从与 JobMaster 及 TaskManager 的状态同步中再重新获得，所以这些信息并不需要持久化。</p><h2 id="JobMaster-的容错"><a href="#JobMaster-的容错" class="headerlink" title="JobMaster 的容错"></a>JobMaster 的容错</h2><p>如果 TaskManager 通过心跳超时检测到 JobMaster 故障，或者收到 zookeeper 的关于 JobMaster 失去 leadership 通知，TaskManager 会触发自己的错误恢复（目前是释放所有 Task），然后等待新的 JobMaster。如果新的 JobMaster 在一定时间后仍未出现，TaskManager 会将其 slot 标记为空闲并告知 ResourceManager。</p><p>如果 ResourceManager 通过心跳超时检测到 JobMaster 故障，或者收到 zookeeper 的关于 JobMaster 失去 leadership 通知，ResourceManager 会将其告知 TaskManager，其他不作处理。</p><p>JobMaster 保存了很多对作业执行至关重要的状态，其中 JobGraph 和用户代码会重新从 HDFS 等持久化存储中获取，checkpoint 信息会从 zookeeper 获得，Task 的执行信息可以不恢复因为整个作业会重新调度，而持有的 slot 则从 ResourceManager 的 TaskManager 的同步信息中恢复。</p><h2 id="并发故障"><a href="#并发故障" class="headerlink" title="并发故障"></a>并发故障</h2><p>在 on YARN 部署模式下，因为 JobMaster 和 ResourceManager 都在 JobManager 进程内，如果 JobManager 进程出问题，通常是 JobMaster 和 ResourceManager 并发故障，那么 TaskManager 会按以下步骤处理:</p><ol><li>按照普通的 JobMaster 故障处理。</li><li>在一段时间内不断尝试将 slot 提供给新的 JobMaster。</li><li>不断尝试将自己注册到 ResourceManager 上。</li></ol><p>值得注意的是，新 JobManager 的拉起是依靠 YARN 的 Application attempt 重试机制来自动完成的，而根据 Flink 配置的 YARN Application <code>keep-containers-across-application-attempts</code> 行为，TaskManager 不会被清理，因此可以重新注册到新启动的 Flink ResourceManager 和 JobMaster 中。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Flink 容错机制确保了 Flink 的可靠性和持久性，是 Flink 应用于企业级生产环境的重要保证，具体来说它包括作业执行的容错和守护进程的容错两个方面。在作业执行容错方面，Flink 提供 Task 级别的 Failover 策略和 Job 级别的 Restart 策略来进行故障情况下的自动重试。在守护进程的容错方面，在on YARN 模式下，Flink 通过内部组件的心跳和 YARN 的监控进行故障检测。TaskManager 的故障会通过申请新的 TaskManager 并重启 Task 或 Job 来恢复，JobManager 的故障会通过集群管理器的自动拉起新 JobManager 和 TaskManager 的重新注册到新 leader JobManager 来恢复。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="http://www.whitewood.me/2018/05/13/Flink-%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%BC%82%E6%AD%A5%E5%BF%AB%E7%85%A7-ABS-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" target="_blank" rel="external">Flink 轻量级异步快照 ABS 实现原理</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-1+%3A+Fine+Grained+Recovery+from+Task+Failures" target="_blank" rel="external">FLIP-1 : Fine Grained Recovery from Task Failures</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-13223" target="_blank" rel="external">[FLINK-13223] Set jobmanager.execution.failover-strategy to region in default flink-conf.yaml    </a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10712" target="_blank" rel="external">[FLINK-10712] RestartPipelinedRegionStrategy does not restore state</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10289" target="_blank" rel="external">[FLINK-10289] Classify Exceptions to different category for apply different failover strategy</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10288" target="_blank" rel="external">[FLINK-10288] Failover Strategy improvement</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc.</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为分布式系统，尤其是对延迟敏感的实时计算引擎，Apache Flink 需要有强大的容错机制，以确保在出现机器故障或网络分区等不可预知的问题时可以快速自动恢复并依旧能产生准确的计算结果。事实上，Flink 有一套先进的快照机制来持久化作业状态[1]，确保中间数据不会丢失，这通常需要和错误恢复机制（作业重启策略或 failover 策略）配合使用。在遇到错误时，Flink 作业会根据重启策略自动重启并从最近一个成功的快照（checkpoint）恢复状态。合适的重启策略可以减少作业不可用时间和避免人工介入处理故障的运维成本，因此对于 Flink 作业稳定性来说有着举足轻重的作用。下文就将详细解读 Flink 的错误恢复机制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 非 JVM 语言支持计划</title>
    <link href="https://link3280.github.io/2019/07/11/Flink-%E9%9D%9E-JVM-%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81%E8%AE%A1%E5%88%92/"/>
    <id>https://link3280.github.io/2019/07/11/Flink-非-JVM-语言支持计划/</id>
    <published>2019-07-11T12:52:04.000Z</published>
    <updated>2019-07-11T13:06:54.362Z</updated>
    
    <content type="html"><![CDATA[<p>众所周知，Apache Flink 是基于 JVM 语言开发的，所以提供的运行环境和编程 API 都是 JVM 语言（目前 Flink Python API 是用 Jython 实现的，因此也算 JVM 语言）。然而基于 JVM 开发的计算引擎普遍会遇到的一个问题是，做数据分析或机器学习的用户通常主要使用更声明式的语言，比如 Python 或者 R。因此为了支持多语言，尤其是非 JVM 语言，分布式计算领域业界在计算引擎的语言可移植性上做了不少的努力，其中比较出名的项目包括 SparkR、PySpark 和 Apache Beam。而目前 Apache Flink 社区也计划推进多语言支持，其中将优先支持 Python，下文将详细解析实现的关键点及具体方案。</p><a id="more"></a><h1 id="业界实践经验"><a href="#业界实践经验" class="headerlink" title="业界实践经验"></a>业界实践经验</h1><p>分布式计算引擎对语言的支持通常分为 SDK 和 UDF 两个部分: 其中 SDK 面向用户，用于构建作业逻辑；而 UDF 则面向运行时，为运行环境中实际调用的函数。相对地，实现多语言支持需要分为两个组件: 一是将提供目标语言和 JVM 之间的桥梁，令目标语言 SDK 可以调用 JVM SDK；二是提供目标语言 UDF 运行时环境及其和 JVM 交互的途径。值得注意的是，这里所说的 UDF 是与语言执行原生环境绑定的，而不是只在 JVM 里访问其他语言定义的函数。</p><p>以 SparkR 为例，其架构如下:</p><center><p><img src="/img/flink-non-jvm/img1.sparkr-architecture.png" alt="SparkR 架构" title="图1. SparkR 架构"></p></center><p>用户用 R 定义的作业逻辑拓扑会通过 R 与 JVM 间的语言桥梁翻译为 Java Spark SDK，即 Java Spark Context，后者与普通 Java 开发的作业一样启动 SparkExecutor，此为 SDK 部分；后续 SparkExecutor 并不是直接在进程内而是启动另外一个 R 进程来执行 UDF，此为 UDF 部分。</p><p>除计算引擎原生的支持外，Apache Beam 为计算引擎提供了另外一个更通用化的支持多语言的途径。Apache Beam 是一个统一流批处理并提供多语言支持的分布式计算框架面门（Facade），提供了计算逻辑在开发运行语言上和在计算引擎上的可移植性。Apache Beam 目前正在开发可移植性层框架，它主要由三个组件组成:</p><center><p><img src="/img/flink-non-jvm/img2.beam-protability-architecture.png" alt="Apache Beam 可移植层架构" title="图2. Apache Beam 可移植层架构"></p></center><ul><li>SDK: SDK 负责提供 Java、Python 和 Go 等语言的编程接口，用于定义数据处理管道（Pipeline）及设置执行选项，比如运行的 Runner、并行度等。</li><li>Runner: Runner 是底层负责将 Pipeline 翻译成执行引擎作业的组件。每个计算计引擎有对应的 Runner，目前已支持 Flink、Spark、Samza、Google Dataflow 等计算引擎。</li><li>SDK harness: SDK harness 是执行用户代码（Beam Fn API， 即 UDF）的组件。SDK harness 可以以独立进程或者容器化的方式实现，环境是与 Runner 相互独立的。SDK harness 会根据执行时的二进制协议与 Runner 交互，包括如何管理 Task、何如传输数据。</li></ul><p>不同组件间通过基于 protobuf 和 gPRC 的跨语言协议交流，以支持多种语言。</p><h1 id="Flink-社区-Roadmap"><a href="#Flink-社区-Roadmap" class="headerlink" title="Flink 社区 Roadmap"></a>Flink 社区 Roadmap</h1><p>基于上述背景，Flink 社区在支持非 JVM 语言上也分别有两种实现方案，即原生的 SDK 支持、原生的 UDF 支持或通过 Beam 来提供。其中社区可以选择其中一种或者多种都实现，关键点在于是否需要 Flink 自身的可移植性层，即 SDK 的可移植层和 UDF 的可移植层。</p><p>在 SDK 的可移植性上，Flink 社区基本认同这是必要的，因为 SDK 负责主要是定义作业逻辑拓扑和配置，比较轻量级，而且原生 SDK 作业用户入口需要尽可能完整地暴露 Flink 的功能，完全依赖 Beam 来实现是不可取的。</p><p>在 UDF 的可移植层上，社区则出现了两种观点：一种观点主要是来自同样活跃在 Beam 社区的工程师，他们认为可移植性层需要大量的工作，Beam 社区花费至少一年重构了大部分的代码才基本完成，所以比起重新发明可移植性层， Flink 社区更应该把精力用在和 Beam 的集成上；另一种观点主要来自阿里巴巴 Blink 团队的工程师，他们已经在 Blink 上实现了对 Python UDF 的支持，并认为 Flink 的可移植层可以更好地利用引擎原生的优化，所以可以在支持 Beam 的基础上也实现 Flink 本身的可移植层。</p><p>社区讨论最后并没有达成一致，不过作为进一步的探索，Flink 将首先在 Table API 上实现 Python SDK，用于满足数据分析用户的需求，不过 Python 或其他语言的 UDF 需等后续收集用户反馈再决定是否要提供。Python Table API 的设计文档见 FLIP-38 [2]，虽然目前只针对 Table API，但我们可以从中窥见 Flink 对非 JVM 语言支持的发展方向。</p><h1 id="Flink-Table-Python-支持"><a href="#Flink-Table-Python-支持" class="headerlink" title="Flink Table Python 支持"></a>Flink Table Python 支持</h1><p>之所以首先在 Table API 支持 Python，原因是 Table API 将逐渐成为数据分析用户主要使用的 API，Python Table API 将大大提升用户友好度，而对于其他语言的需求还没有那么迫切。此外，Table API 比起底层的 DataStream/DataSet API 更为声明式，而且更符合流批处理统一的原则，并且可以在翻译成 JobGraph 时灵活应用优化规则。</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>Python Table API 需要达成以下目标:</p><ul><li>用户可以在 Python 环境使用与 Java 版本相同的 Flink Table API。</li><li>用户可以通过现有所有途径提交以 Python Table API 编写的作业，包括通过脚本提交（类似于 <code>bin/flink run</code>），通过 REST 接口提交，以交互式 shell 的方式提交和本地 IDE 调试运行。</li><li>用户可以用 Python 编写 UDF、UDAF 和 UDTF。</li><li>Pandas 函数可以用于 Python Table API。</li></ul><p>简单来说，通过 Python Table API 用户可以不依赖 Java API 进行开发和部署，而运行时对于 Java 环境的依赖则是对用户透明的。另外，Python Table API 在首个 release 将只支持离线批处理模式和用于探索开发的交互式 shell 模式，后续再拓展至实时流处理模式。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="SDK"><a href="#SDK" class="headerlink" title="SDK"></a>SDK</h3><center><p><img src="/img/flink-non-jvm/img3.flink-sdk-protability.png" alt="Flink SDK 可移植层架构" title="图3. Flink SDK 可移植层架构"></p></center><p>类似于 SparkR 的实现，Python SDK 相当于是在 Java SDK 基础上进行的一层封装，它通过 rpc 协议来与 Java SDK 交互，并将后者的功能完全暴露至 Python 解释器。其中 rpc 协议是实现跨语言的关键，具体来说有两种 rpc 实现方案。</p><p>一是利用已有的语言桥梁 lib，比如 Py4j。Py4j 自动负责 Java class 和 Python class 之间的映射，这个映射是通过 Python 解释器中的 stub（Python Gateway）和 JVM 中的 stub（Java Gateway Server）完成的（要避免误会的一点是，Py4j 是双向的，因此也可以是 Java Gateway 和 Python Gateway Server）。因此，在 Python SDK 中我们可以完全复用 Java Table API 的 class，开发量最少。</p><center><p><img src="/img/flink-non-jvm/img4.py4j-approach.png" alt="基于 Py4j 的 Flink SDK 可移植层" title="图4. 基于 Py4j 的 Flink SDK 可移植层"></p></center><p>二是开发完整的 SDK 可移植层，每个语言的 SDK 可以独立建立 JobGraph 对象，而不是基于 Java 已有的实现。这要求将 DAG 生成和作业配置等 SDK 功能提到每个语言的 SDK 来实现，并且 JobGraph 需要重构为和具体语言无关的数据结构，用 protobuf 等序列化框架来表达。</p><center><p><img src="/img/flink-non-jvm/img5.jobgraph-approach.png" alt="原生实现的 Flink SDK 可移植层" title="图5. 原生实现的 Flink SDK 可移植层"></p></center><p>从长远来说方案二是模块化和可拓展性更好的架构，不过出于大方向仍未完全确定的考虑，Flink 在初期将先以开发成本较低的方案一实现 Python Table API。</p><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>UDF 则是支持非 JVM 语言最为复杂的一部分，尤其是对于 Flink 这样的有状态计算来说。从设计原则来说，UDF 会运行在一个类似容器的独立环境，需要通过 RPC 的方式来和 Flink Operator 进行通信，因此如何实现 RPC 服务将 Flink Operator 的功能暴露给外部是关键点。在这个问题上，Beam 社区已经有了很好的实践经验。对于有状态计算， Beam 将对 UDF 的支持抽象为四个服务:</p><center><p><img src="/img/flink-non-jvm/img6.beam-udf-architecture.png" alt="Beam Runner 的 UDF 架构" title="图6. Beam Runner 的 UDF 架构"></p></center><ul><li>Control Service: 负责 UDF 的管理，比如启停 SDC harness（运行 UDF 的环境）。</li><li>Data Service: 负责提供 Runner 和 UDF 间的数据流传输。</li><li>State Service: 负责提供 UDF 中 State 的管理。</li><li>Logging Service: 负责接收并处理 UDF 的输出日志。</li></ul><p>相比之下，Flink 提供多了一个 Metrics Service 来采集 UDF 产生的 metric。</p><center><p><img src="/img/flink-non-jvm/img7.flink-python-sdk-overview.png" alt="Flink Python UDF 架构" title="图7. Flink Python UDF 架构"></p></center><p>无论是 Beam 或事 Flink，UDF 架构中 Data Service 和 State Service 涉及到计算效率和数据准确性，是最为核心的服务。更加重要的是，两者的通信管道都是双工的，因此在实现上也最为复杂。下文将结合 UDF 的类型重点说明这两个服务的架构。</p><h4 id="Table-API-UDF-分类"><a href="#Table-API-UDF-分类" class="headerlink" title="Table API UDF 分类"></a>Table API UDF 分类</h4><p>熟悉 Hive 的同学应该知道用户定义的函数除了 UDF、还有 UDAF（User Defined Aggregate Function）和 UDTF（User Defined Table Function），不过为了方便起见都称为广义的 UDF 或者 UDX。三者主要不同在于输入行数与输出行数的关系，可以用以下的表格概括，其中 UDF、UDAF 和 UDTF 分别对应 Table API 的 ScalarFcuntion、AggregateFunction 和 TableFunction。</p><center><p><img src="/img/flink-non-jvm/img8.table-api-udx.png" alt="Flink Table API UDFs" title="图8. Flink Table API UDFs"></p></center><p>其中 ScalarFcuntion 和 TableFunction 输入均为一行，因此没有涉及到跨行的状态处理，属于无状态的 UDF；与之相对，AggregateFunction 以多行为输入，涉及中间状态，属于有状态的 UDF。下面分别讨论对于该两种 UDF 的实现。</p><h4 id="无状态的-UDF"><a href="#无状态的-UDF" class="headerlink" title="无状态的 UDF"></a>无状态的 UDF</h4><p>对于无状态的 UDF 来说，最为重要的服务是 DataService。DataService 负责 JVM 和 Python 解释器间的数据传输，性能极其关键并需要重点优化，这主要分为数据处理模式（Data Processing Mode）和数据传输模式（Data Transmission Mode）两个方面。</p><p>数据处理模式指的是 UDF 以同步还是异步的方式处理数据。同步的处理方式意味着 UDF 将逐条处理数据，在一条数据被处理完并输出前，Flink Operator 不会将下一条数据发送给 UDF。相对地，异步的处理方式意味着数据传输和 UDF 计算解耦，Flink Operator 可以将数据批量传输到 UDF 的输入队列缓存起来，UDF 计算的输出也可以先放入 UDF 的输出队列。</p><p>数据传输模式指的是 Flink Operator 与 UDF 间的数据传输是逐条传输还是批量传输。值得注意的是，逐条传输可以和同步数据处理和异步数据处理配合使用，批量传输则必须和异步的数据处理模式配合使用。</p><p>显而易见，性能关键的 DataService 使用异步数据处理模式加上批量传输是更合适的。总体的架构会如下:</p><center><p><img src="/img/flink-non-jvm/img9.stateless-udf-architecture.png" alt="Stateless UDF 架构" title="图9. Stateless UDF 架构"></p></center><p>首先 Flink Operator 将摄入的数据放到 input buffer 并发送至 Python Worker，随后该 buffer 变为 waiting buffer 仍保存原始数据；然后 Python Worker 端将 input buffer 的数据逐条交给 UDF 执行，其输出结果作为 result buffer 发回给 Flink Operator；最后 Flink Operator 将 result buffer 和 waiting buffer 合并生成最终结果并发给下游。对于为什么需要 waiting buffer，笔者猜测是 rpc 发给 Python Worker 的数据只包含了 UDF 调用的列，所以 result buffer 并不是完整数据，需要和 waiting buffer 合并才是最终结果。</p><h4 id="有状态的-UDF"><a href="#有状态的-UDF" class="headerlink" title="有状态的 UDF"></a>有状态的 UDF</h4><p>有状态的 UDF 是最为复杂的一类 UDF，因为 Python Worker 需要和 Flink Operator 同步 state 信息。</p><center><p><img src="/img/flink-non-jvm/img10.stateful-udf-architecture.png" alt="Stateful UDF 架构" title="图10. Stateful UDF 架构"></p></center><p>相比起无状态的 UDF，最大的不同有两点:</p><ul><li>数据先执行 group 后再发至 Python Worker。这样的好处是 UDF 输入是按 key 排序的，所以方便合并 UDF 的输出。另外一点是减轻了 Python Worker 的负担，Python Worker 在同一时间只会处理一个 key 的数据，不需要维持多个 key 的状态。</li><li>UDF 可以访问 Flink Operator 端的 state。Flink Operator 的 state 会通过 State Service 以 DataView 的形式暴露给 Python Worker，后者对 state 的操作会映射到 StateBackend。为了避免频繁的 state 访问，Python Worker 会缓存部分 state 数据，这部分 state 会在 checkpoint 快照期间同步会 Flink Operator 以确保一致性。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实时计算引擎对非 JVM 语言的支持将主要分为客户端的 SDK 和运行时的 UDF 两部分，其中最为核心的部分是定义跨语言的 rpc 协议层来负责不同语言运行环境间的数据通信。对于 Flink 来说，要支持非 JVM 语言有两种途径: 通过集成 Apache Beam 或者实现原生支持。从长远来说，和 Apache Beam 集成应该没有太大疑问，而对于原生支持则尚未确定，所以社区会先通过实现较为独立的 Python Table API 来作为后续计划的探索。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://beam.apache.org/roadmap/portability/" target="_blank" rel="external">Apache Beam - Portability Framework Roadmap</a></li><li><a href="https://docs.google.com/document/d/1ybYt-0xWRMa1Yf5VsuqGRtOfJBz4p74ZmDxZYg3j_h8/edit#heading=h.p6h40rdmqvbx" target="_blank" rel="external">[DISCUSS] FLIP-38 Support python language in flink Table API</a></li><li><a href="https://docs.google.com/document/d/1XYzb1Fnt2sam7u2MsGFaZp-2qSIGxUn66VLer-bcXAk/edit#heading=h.p6lvszfbmyj6" target="_blank" rel="external">Apache Beam Fn API Overview</a></li><li><a href="https://docs.google.com/document/d/1cKOB9ToasfYs1kLWQgffzvIbJx2Smy4svlodPRhFrk4/edit#" target="_blank" rel="external">Apache Beam Portability API</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;众所周知，Apache Flink 是基于 JVM 语言开发的，所以提供的运行环境和编程 API 都是 JVM 语言（目前 Flink Python API 是用 Jython 实现的，因此也算 JVM 语言）。然而基于 JVM 开发的计算引擎普遍会遇到的一个问题是，做数据分析或机器学习的用户通常主要使用更声明式的语言，比如 Python 或者 R。因此为了支持多语言，尤其是非 JVM 语言，分布式计算领域业界在计算引擎的语言可移植性上做了不少的努力，其中比较出名的项目包括 SparkR、PySpark 和 Apache Beam。而目前 Apache Flink 社区也计划推进多语言支持，其中将优先支持 Python，下文将详细解析实现的关键点及具体方案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink State As Database</title>
    <link href="https://link3280.github.io/2019/06/02/Flink-State-As-Database/"/>
    <id>https://link3280.github.io/2019/06/02/Flink-State-As-Database/</id>
    <published>2019-06-02T07:02:03.000Z</published>
    <updated>2019-06-02T07:07:37.664Z</updated>
    
    <content type="html"><![CDATA[<p>有状态的计算作为容错以及数据一致性的保证，是当今实时计算必不可少的特性之一，流行的实时计算引擎包括 Google Dataflow、Flink、Spark (Structure) Streaming、Kafka Streams 都分别提供对内置 State 的支持。State 的引入使得实时应用可以不依赖外部数据库来存储元数据及中间数据，部分情况下甚至可以直接用 State 存储结果数据，这让业界不禁思考: State 和 Database 是何种关系？有没有可能用 State 来代替数据库呢？</p><a id="more"></a><p>在这个课题上，Flink 社区是比较早就开始探索的。总体来说，Flink 社区的努力可以分为两条线: 一是在作业运行时通过作业查询接口访问 State 的能力，即 QueryableState；二是通过 State 的离线 dump 文件（Savepoint）来离线查询和修改 State 的能力，即即将引入的 Savepoint Connector。</p><h1 id="QueryableState"><a href="#QueryableState" class="headerlink" title="QueryableState"></a>QueryableState</h1><p>在 2017 年发布的 Flink 1.2 版本，Flink 引入了 QueryableState 的特性以允许用户通过特定的 client 查询作业 State 的内容 [1]，这意味着 Flink 应用可以在完全不依赖 State 存储介质以外的外部存储的情况下提供实时访问计算结果的能力。</p><p><img src="/img/QS-NoKV-Architecture-QS-Post-1.png" alt="只通过 Queryable State 提供实时数据访问" title="只通过 Queryable State 提供实时数据访问"></p><p>然而，QueryableState 虽然设想上比较理想化，但由于依赖底层架构的改动较多且功能也比较受限，它一直处于 Beta 版本并不能用于生产环境。针对这个问题，在前段时间腾讯的工程师杨华提出 QueryableState 的改进计划 [2]。在邮件列表中，社区就 QueryableState 是否可以用于代替数据库作了讨论并出现了不同的观点。笔者结合个人见解将 State as Database 的主要优缺点整理如下。</p><p>优点: </p><ul><li>更低的数据延迟。一般情况下 Flink 应用的计算结果需要同步到外部的数据库，比如定时触发输出窗口计算结果，而这种同步通常是定时的会带来一定的延迟，导致计算是实时的而查询却不是实时的尴尬局面，而直接 State 则可以避免这个问题。</li><li>更强的数据一致性保证。根据外部存储的特性不同，Flink Connector 或者自定义的 SinkFunction 提供的一致性保障也有所差别。比如对于不支持多行事务的 HBase，Flink 只能通过业务逻辑的幂等性来保障 Exactly-Once 投递。相比之下 State 则有妥妥的 Exactly-Once 投递保证。</li><li>节省资源。因为减少了同步数据到外部存储的需要，我们可以节省序列化和网络传输的成本，另外当然还可以节省数据库成本。</li></ul><p>缺点:</p><ul><li>SLA 保障不足。数据库技术已经非常成熟，在可用性、容错性和运维上都很多的积累，在这点上 State 还相当于是处于原始人时期。另外从定位上来看，Flink 作业有版本迭代维护或者遇到错误自动重启带来的 down time，并不能达到数据库在数据访问上的高可用性。</li><li>可能导致作业的不稳定。未经过考虑的 Ad-hoc Query 可能会要求扫描并返回夸张量级的数据，这会系统带来很大的负荷，很可能影响作业的正常执行。即使是合理的 Query，在并发数较多的情况下也可能影响作业的执行效率。</li><li>存储数据量不能太大。State 运行时主要存储在 TaskManager 本地内存和磁盘，State 过大会造成 TaskManager OOM 或者磁盘空间不足。另外 State 大意味着 checkpoint 大，导致 checkpoint 可能会超时并显著延长作业恢复时长。</li><li>只支持最基础的查询。State 只能进行最简单的数据结构查询，不能像关系型数据库一样提供函数等计算能力，也不支持谓词下推等优化技术。</li><li>只可以读取，不能修改。State 在运行时只可以被作业本身修改，如果实在要修改 State 只能通过下文的 Savepoint Connector 来实现。</li></ul><p>总体来说，目前 State 代替数据库的缺点还是远多于其优点，不过对于某些对数据可用性要求不高的作业来说，使用 State 作为数据库还是完全合理的。由于定位上的不同，Flink State 在短时间内很难看到可以完全替代数据库的可能性，但在数据访问特性上 State 往数据库方向发展是无需质疑的。</p><h1 id="Savepoint-Connector"><a href="#Savepoint-Connector" class="headerlink" title="Savepoint Connector"></a>Savepoint Connector</h1><p>Savepoint Connector 是社区最近提出的一个新特性（见 FLIP-42 [3]），用于离线对 State 的 dump 文件 Savepoint 进行分析、修改或者直接根据数据构建出一个初始的 Savepoint。Savepoint Connector 属于 Flink State Evolution 的 State Management。如果说 QueryableState 是 DSL 的话，Flink State Evolution 就是 DML，而 Savepoint Connector 就是 DML 中最为重要的部分。</p><p>Savepoint Connector 的前身是第三方的 Bravo 项目 [4]，主要思路提供 Savepoint 和 DataSet 相互转换的能力，典型应用是 Savepoint 读取成 DataSet，在 DataSet 上进行修改，然后再写为一个新的 Savepoint。这适合用于以下的场景:</p><ul><li>分析作业 State 以研究其模式和规律</li><li>排查问题或者审计</li><li>为新的应用构建的初始 State</li><li>修改 Savepoint，比如:<ul><li>改变作业最大并行度</li><li>进行巨大的 Schema 改动</li><li>修正有问题的 State</li></ul></li></ul><p>Savepoint 作为 State 的 dump 文件，通过 Savepoint Connector 可以暴露数据查询和修改功能，类似于一个离线的数据库，但 State 的概念和典型关系型数据的概念还是有很多不同，FLIP-43 也对这些差异进行了类比和总结。</p><p>首先 Savepoint 是多个 operator 的 state 的物理存储集合，不同 operator 的 state 是独立的，这类似于数据库下不同 namespace 之间的 table。我们可以得到 Savepoint 对应数据库，单个 operator 对应 Namespace。</p><table><thead><tr><th>Database</th><th>Savepoint</th></tr></thead><tbody><tr><td>Namespace</td><td>Uid</td></tr><tr><td>Table</td><td>State</td></tr></tbody></table><p>但就 table 而言，其在 Savepoint 里对应的概念根据 State 类型的不同而有所差别。State 有 Operator State、Keyed State 和 Broadcast State 三种，其中 Operator State 和 Broadcast State 属于 non-partitioned state，即没有按 key 分区的 state，而相反地 Keyed State 则属于 partitioned state。对于 non-partitioned state 来说，state 是一个 table，state 的每个元素即是 table 里的一行；而对于 partitioned state 来说，同一个 operator 下的所有 state 对应一个 table。 这个 table 像是 HBase 一样有个 row key，然后每个具体的 state 对应 table 里的一个 column。</p><p>举个例子，假设有一个游戏玩家得分和在线时长的数据流，我们需要用 Keyed State 来记录玩家所在组的分数和游戏时长，用 Operator State 记录玩家的总得分和总时长。</p><p>在一段时间内数据流的输入如下:</p><table><thead><tr><th>user_id</th><th>user_name</th><th>user_group</th><th>score</th></tr></thead><tbody><tr><td>1001</td><td>Paul</td><td>A</td><td>5,000</td></tr><tr><td>1002</td><td>Charlotte</td><td>A</td><td>3,600</td></tr><tr><td>1003</td><td>Kate</td><td>C</td><td>2,000</td></tr><tr><td>1004</td><td>Robert</td><td>B</td><td>3,900</td></tr></tbody></table><table><thead><tr><th>user_id</th><th>user_name</th><th>user_group</th><th>time</th></tr></thead><tbody><tr><td>1001</td><td>Paul</td><td>A</td><td>1,800</td></tr><tr><td>1002</td><td>Charlotte</td><td>A</td><td>1,200</td></tr><tr><td>1003</td><td>Kate</td><td>C</td><td>600</td></tr><tr><td>1004</td><td>Robert</td><td>B</td><td>2,000</td></tr></tbody></table><p>用 Keyed State ，我们分别注册 <code>group_score</code> 和 <code>group_time</code> 两个 MapState<string, integer=""> 表示组总得分和组总时长，并根据 <code>user_group</code> keyby 数据流之后将两个指标的累积值更新到 State 里，得到的表如下:</string,></p><table><thead><tr><th>user_group</th><th>group_score</th><th>group_time</th></tr></thead><tbody><tr><td>A</td><td>8,600</td><td>3,000</td></tr><tr><td>C</td><td>2,00</td><td>600</td></tr><tr><td>B</td><td>3,900</td><td>2,000</td></tr></tbody></table><p>相对地，假如用 Operator State 来记录总得分和总时长（并行度设为 1），我们注册 <code>total_score</code> 和 <code>total_time</code> 两个 State，得到的表有两个:</p><table><thead><tr><th>total_score</th></tr></thead><tbody><tr><td>14,500</td></tr></tbody></table><table><thead><tr><th>total_time</th></tr></thead><tbody><tr><td>5,600</td></tr></tbody></table><p>至此 Savepoint 和 Database 的对应关系应该是比较清晰明了的。而对于 Savepoint 来说还有不同的 StateBackend 来决定 State 具体如何持续化，这显然对应的是数据库的存储引擎。在 MySQL 中，我们可以通过简单的一行命令 <code>ALTER TABLE xxx ENGINE = InnoDB;</code> 来改变存储引擎，在背后 MySQL 会自动完成繁琐的格式转换工作。而对于 Savepoint 来说，由于 StateBackend 各自的存储格式不兼容，目前尚不能方便地切换 StateBackend。为此，社区在不久前创建 FLIP-41 [5] 来进一步完善 Savepoint 的可操作性。Savepoint 相关的这两个 FLIP 的目标版本都是预计在 7 月发布的 1.9 版本，敬请期待。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>State as Database 是实时计算发展的大趋势，它并不是要代替数据库的使用，而是借鉴数据库领域的经验拓展 State 接口使其操作方式更接近我们熟悉的数据库。对于 Flink 而言，State 的外部使用可以分为在线的实时访问和离线的访问和修改，分别将由 Queryable State 和 Savepoint Connector 两个特性支持。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://www.ververica.com/blog/queryable-state-use-case-demo" target="_blank" rel="external">Queryable State in Apache Flink® 1.2.0: An Overview &amp; Demo</a></li><li><a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Improve-Queryable-State-and-introduce-a-QueryServerProxy-component-td28578.html#a28581" target="_blank" rel="external">Improve Queryable State and Introduce a QueryServerProxy Component</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-43%3A+Savepoint+Connector" target="_blank" rel="external">FLIP-43: Savepoint Connector</a></li><li><a href="https://github.com/king/bravo" target="_blank" rel="external">Bravo: Utilities for processing Flink checkpoints/savepoints</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-41%3A+Unify+Keyed+State+Snapshot+Binary+Format+for+Savepoints" target="_blank" rel="external">FLIP-41: Unify Keyed State Snapshot Binary Format for Savepoints</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有状态的计算作为容错以及数据一致性的保证，是当今实时计算必不可少的特性之一，流行的实时计算引擎包括 Google Dataflow、Flink、Spark (Structure) Streaming、Kafka Streams 都分别提供对内置 State 的支持。State 的引入使得实时应用可以不依赖外部数据库来存储元数据及中间数据，部分情况下甚至可以直接用 State 存储结果数据，这让业界不禁思考: State 和 Database 是何种关系？有没有可能用 State 来代替数据库呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.8 Release 解读</title>
    <link href="https://link3280.github.io/2019/04/14/Flink-1-8-Release-%E8%A7%A3%E8%AF%BB/"/>
    <id>https://link3280.github.io/2019/04/14/Flink-1-8-Release-解读/</id>
    <published>2019-04-14T11:58:26.000Z</published>
    <updated>2019-04-14T12:28:28.939Z</updated>
    
    <content type="html"><![CDATA[<p><center><p><img src="/img/Flink-1.8.png" alt="Flink 1.8"></p></center></p><p></p><p>在距离上个 feature 版本发布近四个月之后， 近日 Apache Flink 发布了 1.8 版本。该版本处理了 420 个 issue，其中新 feature 及改进主要集中在 State、Connector 和 Table API 三者上，并 fix 了一些在生产部署中较为常见的问题。下文将选取一些笔者认为重要的新特性、improvement 和 bugfix 进行解读，详细的改动请参照 release notes <a href="https://flink.apache.org/news/2019/04/09/release-1.8.0.html" target="_blank" rel="external">[1]</a>。</p><a id="more"></a><h2 id="State"><a href="#State" class="headerlink" title="State"></a>State</h2><h3 id="State-Schema-Evolution-新增对内置序列化器的支持"><a href="#State-Schema-Evolution-新增对内置序列化器的支持" class="headerlink" title="State Schema Evolution 新增对内置序列化器的支持"></a>State Schema Evolution 新增对内置序列化器的支持</h3><p>Flink 1.7 版本新增 State Schema Evolution 的特性，可以在支持 State POJO 随业务逻辑演变（见之前一篇博客<a href="http://www.whitewood.me/2019/03/17/%E4%BB%80%E4%B9%88%E6%98%AF-Flink-State-Evolution/" target="_blank" rel="external">[9]</a>），但要求序列化器向后兼容，而当时 Flink 内置的 POJOSerializer 并不能做到这点，因此 1.7 版本只提供了 AvroSerializer 的 State Schema Evolution。1.8 版本修复了这个尴尬的情况，现在用户新增或者移除 POJO 的任意字段不会不破坏 Savepoint 兼容性。</p><h3 id="State-支持基于-TTL-的持续增量清理"><a href="#State-支持基于-TTL-的持续增量清理" class="headerlink" title="State 支持基于 TTL 的持续增量清理"></a>State 支持基于 TTL 的持续增量清理</h3><p>Flink 自 1.6 版本提供的 State TTL 现已被广泛使用，但是用户经常反馈的一个痛点是 Keyed State 在一个 key 过期之后，只有被再次访问后才会被发现，然后通过后台线程物理上清理掉。这会造成一个问题是有的 key 只是在一段时期内出现，如果依赖 State TTL 的话它可能永远不会被发现和清理，造成 State size 可能一直增长，因此在实践中用户常常还要通过设置各种 Timer 来手动清理不会再出现的 key 的 State。</p><p>Flink 1.8 实现了 State 的持续增量清理（continuous cleanup），即每次 full snapshot（即一般 checkpoint/savepoint，或者 incremental checkpoint 的 full snapshot 运行周期）的时候扫描全部的 entry 来检查和清理其中过期的部分，不过代价也是显而易见的更长的 checkpoint 时间。</p><h2 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h2><h3 id="FlinkKafkaConsumer-支持访问-Kafka-消息-header"><a href="#FlinkKafkaConsumer-支持访问-Kafka-消息-header" class="headerlink" title="FlinkKafkaConsumer 支持访问 Kafka 消息 header"></a>FlinkKafkaConsumer 支持访问 Kafka 消息 header</h3><p>Flink 1.8 新增了 KafkaDeserializationSchema 来给予用户对 Kafka 消息的完整访问权限，在这之前用户只可以访问反序列后的消息 body 而不能访问 header。这个 feature 使得用户可以在 Flink Application 里实现更多的定制化需求，比如我们正想做的基于 Kafka header timestamp 的类似于 Prevega 的 StreamCut 功能，以满足重流数据的场景。</p><h3 id="将从-State-恢复的-Kafka-topic-partition-从消费列表过滤掉"><a href="#将从-State-恢复的-Kafka-topic-partition-从消费列表过滤掉" class="headerlink" title="将从 State 恢复的 Kafka topic partition 从消费列表过滤掉"></a>将从 State 恢复的 Kafka topic partition 从消费列表过滤掉</h3><p>在 1.8 版本之前，FlinkKafkaConsumer 从 savepoint/checkpoint 恢复时会将读取所有 Kafka topic partition offset，在 topic name 或者 pattern 出现变化时，比如用户想从 topic A 改为 topic B，恢复后的作业会读取同时两个 topic。如果用户不再需要读取 topic A，一个办法是将 Kafka source 的 uid 改变来避免 FlinkKafkaConsumer 恢复状态。</p><p>Flink 1.8 修复了这个问题，并默认会根据新的 topic name 或者 pattern 判断需要恢复哪些 offset。如果用户希望保留之前的行为，可以通过 <code>FlinkKafkaConsumer#disableFilterRestoredPartitionsWithSubscribedTopics()</code> 来禁用过滤。</p><h3 id="StreamingFileSink-支持-SequenceFile"><a href="#StreamingFileSink-支持-SequenceFile" class="headerlink" title="StreamingFileSink 支持 SequenceFile"></a>StreamingFileSink 支持 SequenceFile</h3><p>Flink 1.6 新增了 StreamingFileSink 来代替 BucketingSink 成为写文件或对象到下游系统的主流 connector，但是并不支持 Hadoop SequenceFile 格式（虽然支持了 Parquet 和 ORC），这个问题同样在 1.8 版本得到修复。</p><h2 id="Table-API"><a href="#Table-API" class="headerlink" title="Table API"></a>Table API</h2><h3 id="重构-Table-API"><a href="#重构-Table-API" class="headerlink" title="重构 Table API"></a>重构 Table API</h3><p>Table API 的重构主要是为 Merge 阿里巴巴贡献的 Blink 做准备。Blink 在 SQL 层和 Table 层上做了比较多的改进，因此为了实现一个更为平滑的 Merge 进程，社区提出了 FLIP-32 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="external">[3]</a> 以重构 Table API，主要目的降低 Table API 内部以及自身和其他模块（DataSet/DataStream 等）的耦合性。</p><p>Merge 的进程会分为以下几个步骤实现:  </p><ol><li>移除 Table API 对于 Scala 的依赖。由于历史原因，Table API 主要使用 Scala 来编写，其中主要问题是 Scala class 中的 private 或者 protected 变量对于 Java class 来说是可见的，因此会造成潜在的安全问题且对用户不友好。  </li><li>优化 API 设计。目前 flink-table 模块有 7 个 TableEnvironment（3 个 base class + 4 个 Java/Scala 的 TableEnvironment）作为编程接口，不够高内聚。  </li><li>Table API 与 DataStream/DataSet API 解耦。让 Table API self-container，在编程接口上不需要依赖 DataStream/DataSet。  </li><li>统一批处理和流处理。这是 Blink 团队一直向社区推的一个重要 feature，也是 Flink 的核心理念”批处理是流处理的特例”的体现。这个目标会将 DataSet 从直接基于 runtime 迁移到 DataStream 之上，因此毋需额外为 DataSet 维护一个执行层栈，详情可见<a href="https://docs.google.com/document/d/1G0NUIaaNJvT6CMrNCP6dRXGv88xNhDQqZFrQEuJ0rVU/edit#heading=h.ob9i0lcn7ulz" target="_blank" rel="external">[4]</a>。</li><li>处理 Table API 向后兼容性。以上的步骤涉及到 TableEnvironmnet 等面向用户的 API 改动，因此需要提前识别出来并在过渡版本标记为 @Deprecated，以便在后续版本移除。</li><li>Merga Blink 的 SQL feature 以及架构上的改进。该步也会分成多个阶段进行，比如首先是 Blink planer（计划器），然后再逐个 Operator 改动。</li></ol><p>Flink 1.8 目前来说在步骤 1 2 3 上都有初步的进度。</p><h3 id="Table-API-支持-Kafka-CSV-格式数据的-Schema"><a href="#Table-API-支持-Kafka-CSV-格式数据的-Schema" class="headerlink" title="Table API 支持 Kafka CSV 格式数据的 Schema"></a>Table API 支持 Kafka CSV 格式数据的 Schema</h3><p>在 1.8 版本以前，在 Table API 使用 CSV 格式数据时没有充分利用 CVS 的 header 来定义 Schema，导致用户需要在 Table 定义中和 CSV 格式中重复定义 Schema，比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.withFormat(</div><div class="line">  new Csv()</div><div class="line">    .field(&quot;field1&quot;, Types.STRING)    // required: ordered format fields</div><div class="line">    .field(&quot;field2&quot;, Types.TIMESTAMP)</div><div class="line">    .fieldDelimiter(&quot;,&quot;)              // optional: string delimiter &quot;,&quot; by default</div><div class="line">    .lineDelimiter(&quot;\n&quot;)              // optional: string delimiter &quot;\n&quot; by default</div><div class="line">    .quoteCharacter(&apos;&quot;&apos;)              // optional: single character for string values, empty by default</div><div class="line">    .commentPrefix(&apos;#&apos;)               // optional: string to indicate comments, empty by default</div><div class="line">    .ignoreFirstLine()                // optional: ignore the first line, by default it is not skipped</div><div class="line">    .ignoreParseErrors()              // optional: skip records with parse error instead of failing by default</div><div class="line">)</div></pre></td></tr></table></figure><p>1.8 版本引入符合 <a href="https://tools.ietf.org/html/rfc4180" target="_blank" rel="external">RFC-4180</a> 的新版本 CVS 格式，当 CSV header 和 Table 定义的 Schema 吻合时，可以自动生成数据 Schema，因此代码可以简化为: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">.withFormat(</div><div class="line">  new Csv()</div><div class="line"></div><div class="line">    // required: define the schema either by using type information</div><div class="line">    .schema(Type.ROW(...))</div><div class="line"></div><div class="line">    // or use the table&apos;s schema</div><div class="line">    .deriveSchema()</div><div class="line"></div><div class="line">    .fieldDelimiter(&apos;;&apos;)         // optional: field delimiter character (&apos;,&apos; by default)</div><div class="line">    .lineDelimiter(&quot;\r\n&quot;)       // optional: line delimiter (&quot;\n&quot; by default;</div><div class="line">                                 //   otherwise &quot;\r&quot; or &quot;\r\n&quot; are allowed)</div><div class="line">    .quoteCharacter(&apos;\&apos;&apos;)        // optional: quote character for enclosing field values (&apos;&quot;&apos; by default)</div><div class="line">    .allowComments()             // optional: ignores comment lines that start with &apos;#&apos; (disabled by default);</div><div class="line">                                 //   if enabled, make sure to also ignore parse errors to allow empty rows</div><div class="line">    .ignoreParseErrors()         // optional: skip fields and rows with parse errors instead of failing;</div><div class="line">                                 //   fields are set to null in case of errors</div><div class="line">    .arrayElementDelimiter(&quot;|&quot;)  // optional: the array element delimiter string for separating</div><div class="line">                                 //   array and row element values (&quot;;&quot; by default)</div><div class="line">    .escapeCharacter(&apos;\\&apos;)       // optional: escape character for escaping values (disabled by default)</div><div class="line">    .nullLiteral(&quot;n/a&quot;)          // optional: null literal string that is interpreted as a</div><div class="line">                                 //   null value (disabled by default)</div><div class="line">)</div></pre></td></tr></table></figure><h3 id="SQL-CEP-函数支持-UDF"><a href="#SQL-CEP-函数支持-UDF" class="headerlink" title="SQL CEP 函数支持 UDF"></a>SQL CEP 函数支持 UDF</h3><p>Flink 1.7 支持了 SQL <code>MATCH_RECOGNIZE</code> 函数，用户可以在 SQL 中使用 CEP，但仍不能在 <code>MATCH_RECOGNIZE</code> 语句里使用 UDF，这会给使用场景带来比较大的局限。1.8 版本解决了这个问题。</p><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><h3 id="完全移除-legacy-部署模式"><a href="#完全移除-legacy-部署模式" class="headerlink" title="完全移除 legacy 部署模式"></a>完全移除 legacy 部署模式</h3><p>自 1.4 版本，Flink 社区一直在重构 Flink 的 runtime 架构，并在 1.5 版本后提供了 legacy 和 new 来种运行模式。在 1.8 版本，Flink 已经完全移除掉 legacy 的 runtime 架构。详情请见 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">[5]</a>。</p><h3 id="修复-on-YARN-模式请求过多-container-的问题"><a href="#修复-on-YARN-模式请求过多-container-的问题" class="headerlink" title="修复 on YARN 模式请求过多 container 的问题"></a>修复 on YARN 模式请求过多 container 的问题</h3><p>在 1.8 版本之前，Flink 存在的一个问题是当 JobManager 向 YARN RM 请求 container 成功，但是 TaskManager 无法利用分配到的 container 启动时（比如 Kerberos 权限错误或者无法读取用于恢复作业的 Savepoint），JobManager 没有移除相应 container request。这样的话 YARN RM 一直认为 Flink 的 AM 持有这些 container，导致 Flink 占用的 container 数会随着 JobManager 不断重试而增长，最终可能吃完整个队列。Flink 1.8 修复了这个问题。</p><h3 id="资源清理问题"><a href="#资源清理问题" class="headerlink" title="资源清理问题"></a>资源清理问题</h3><p>Flink on YARN 模式通常依赖于 Zookeeper 和 HDFS 来做元数据的持久化，但是在这些资源的管理上并不是很优雅，作业运行过后偶尔会留下一些痕迹需要手动异步清理，这在大规模部署的情况下可能会造成意想不到的问题。</p><p>Flink 1.8 修复了部分这样的问题，其中包括修复 Application 提交失败后 blob server 未清理的问题<a href="https://issues.apache.org/jira/browse/FLINK-10848" target="_blank" rel="external">[6]</a>和修复作业结束后 ZK 元数据未清理的问题<a href="https://issues.apache.org/jira/browse/FLINK-11383" target="_blank" rel="external">[7]</a>，但仍有 checkpoint 目录未清理的问题<a href="https://issues.apache.org/jira/browse/FLINK-11789" target="_blank" rel="external">[8]</a>。</p><h2 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h2><h3 id="默认不再提供包含-Hadoop-的发行包"><a href="#默认不再提供包含-Hadoop-的发行包" class="headerlink" title="默认不再提供包含 Hadoop 的发行包"></a>默认不再提供包含 Hadoop 的发行包</h3><p>在之前版本为方便用户部署，Flink 在发行包中默认包括了 Hadoop，但这导致发行包十分臃肿，并且在使用率上可能并不太高，造成资源的浪费。自 Flink 1.8 开始，Apache Flink 默认不再提供包含 Hadoop 的发行包。</p><h3 id="移除-flink-storm"><a href="#移除-flink-storm" class="headerlink" title="移除 flink-storm"></a>移除 flink-storm</h3><p>在 Flink 发展的初期，为了方便用户从 Storm 迁移过来，Flink 提供了 flink-storm 的兼容包来运行用 Storm 编写的作业。但时过境迁，flink-storm 无法跟上 Flink 本身的迭代，现在 flink-storm 只能利用 Flink 的一些低级功能，而且维护成本也逐渐增加，因此社区在调查之后决定废弃掉 flink-storm 模块。</p><h3 id="flink-python-变为可选"><a href="#flink-python-变为可选" class="headerlink" title="flink-python 变为可选"></a>flink-python 变为可选</h3><p>在之前的发行版，Flink lib 默认包含 flink-dist、flink-shaded-hadoop2-uber 和 flink-python 三个 Flink 模块，然而 flink-python 的使用率似乎并不高，因此后续会将其移到 opt 下。加上移除 Hadoop shaded 包，之后 lib 下应该只有 flink-dist 和日志相关的 lib。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://flink.apache.org/news/2019/04/09/release-1.8.0.html" target="_blank" rel="external">Apache Flink 1.8.0 Release Announcement</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-8354" target="_blank" rel="external">Add KafkaDeserializationSchema that directly uses ConsumerRecord</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-32%3A+Restructure+flink-table+for+future+contributions" target="_blank" rel="external">FLIP-32: Restructure flink-table for future contributions</a></li><li><a href="https://docs.google.com/document/d/1G0NUIaaNJvT6CMrNCP6dRXGv88xNhDQqZFrQEuJ0rVU/edit#heading=h.ob9i0lcn7ulz" target="_blank" rel="external">Unified Core API for Streaming and Batch</a></li><li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc.</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10848" target="_blank" rel="external">FLINK-10848: Flink’s Yarn ResourceManager can allocate too many excess containers</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11383" target="_blank" rel="external">FLINK-11383: Dispatcher does not clean up blobs of failed submissions</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-11789" target="_blank" rel="external">FLINK-11789: Flink HA didn’t remove ZK metadata</a></li><li><a href="http://www.whitewood.me/2019/03/17/%E4%BB%80%E4%B9%88%E6%98%AF-Flink-State-Evolution/" target="_blank" rel="external">什么是 Flink State Evolution</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;center&gt;&lt;p&gt;&lt;img src=&quot;/img/Flink-1.8.png&quot; alt=&quot;Flink 1.8&quot;&gt;&lt;/center&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;在距离上个 feature 版本发布近四个月之后， 近日 Apache Flink 发布了 1.8 版本。该版本处理了 420 个 issue，其中新 feature 及改进主要集中在 State、Connector 和 Table API 三者上，并 fix 了一些在生产部署中较为常见的问题。下文将选取一些笔者认为重要的新特性、improvement 和 bugfix 进行解读，详细的改动请参照 release notes &lt;a href=&quot;https://flink.apache.org/news/2019/04/09/release-1.8.0.html&quot;&gt;[1]&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 网络传输优化技术</title>
    <link href="https://link3280.github.io/2019/04/03/Flink-%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/"/>
    <id>https://link3280.github.io/2019/04/03/Flink-网络传输优化技术/</id>
    <published>2019-04-03T12:01:59.000Z</published>
    <updated>2019-04-14T13:02:28.734Z</updated>
    
    <content type="html"><![CDATA[<p>作为工业级的流计算框架，Flink 被设计为可以每天处理 TB 甚至 PB 级别的数据，所以如何高吞吐低延迟并且可靠地在算子间传输数据是一个非常重要的课题。此外，Flink 的数据传输还需要支持框架本身的特性，例如反压和用于测量延迟的 latency marker。在社区不断的迭代中，Flink 逐渐积累了一套值得研究的网络栈（Network Stack），本文将详细介绍 Flink Network Stack 的实现细节以及关键的优化技术。</p><a id="more"></a><p>本文主要基于 Nico Kruber 在去年 9 月 Flink Forward Berlin 上的分享 <a href="https://www.ververica.com/flink-forward-berlin/resources/improving-throughput-and-latency-with-flinks-network-stack" target="_blank" rel="external">[1]</a>，涉及到的技术主要有 1.5 版本引入的 Credit-based 数据流控制以及在延迟和吞吐方面做的优化。在开始之前，我们首先来回顾下 Flink 计算模型里的核心概念，这写概念会在后续被频繁地提及。</p><h2 id="Flink-计算模型"><a href="#Flink-计算模型" class="headerlink" title="Flink 计算模型"></a>Flink 计算模型</h2><p>Flink 计算模型分为逻辑层和执行层，逻辑层主要用于描述业务逻辑，而执行层则负责作业具体的分布式执行。</p><p>用户提交一个作业以后，Flink 首先在 client 端执行用户 main 函数以生成描述作业逻辑的拓扑（StreaGraph），其中 StreamGraph 的每个节点是用户定义的一个算子（Operator）。随后 Flink 对 StreamGraph 进行优化，默认将不涉及 shuffle 并且并行度相同的相邻 Operator 串联起来成为 OperatorChain 形成 JobGraph，其中的每个节点称为 Vertice，是 OperatorChain 或独立的 Operator。</p><p><center><p><img src="/img/network-stack/distributed-runtime.png" alt="图1.分布式运行时" title="图1.分布式运行时"></p></center></p><p></p><p>每个 Vertice 在执行层会被视为一个 Task，而一个 Task 对应多个 Subtask，Subtask 的数目即是用户设置的并行度。Subtask 根据 Flink 的调度策略和具体的部署环境及配置，会被分发到相同或者不同的机器或者进程上，其中有上下游依赖关系的 Subtask 会有数据传输的需要，这是通过基于 Netty 的 Network Stack 来完成的。</p><p>Network Stack 主要包括三项内容，Subtask 的输出模式（数据集是否有界、阻塞或非阻塞）、调度类型（立即调度、等待上一阶段完成和等待上一阶段有输出）和数据传输的具体实现（buffer 和 buffer timeout）。</p><p><center><p><img src="/img/network-stack/network-stack-overview.png" alt="图2.网络栈概览" title="图2.网络栈概览"></p></center></p><p></p><p>下文的内容会主要围绕数据传输部分展开，逐一介绍其中的优化技术。</p><h2 id="Credit-based-数据流控制"><a href="#Credit-based-数据流控制" class="headerlink" title="Credit-based 数据流控制"></a>Credit-based 数据流控制</h2><p>在上文图二左半部分可以看到 Subtask 之间有一条独立的数据传输管道，其实这是逻辑视图，而在物理层 Flink 并不会为维护 Subtask 级别的 TCP 连接，Flink 的 TCP 连接是 TaskManager 级别的。对于每个 Subtask 来说，根据 key 的不同它可以输出数据到下游任意的 Subtask，因此 Subtask 在内部会维护下游 Subtask 数目的发送队列，相对地，下游 Subtask 也会维护上游 Subtask 数目的接收队列。相同两个 TaskManager 上不同的 Subtask 的数据传输会通过 Netty 实现复用和分用跑在同一条 TCP 连接上。</p><p><center><p><img src="/img/network-stack/physical-view.png" alt="图3.网络传输物理视图" title="图3.网络传输物理视图"></p></center></p><p></p><p>这种实现的问题在于当某个 Subtask 出现反压时，反压不仅会作用于该 Subtask 的 Channel，还会误伤到这个 TaskManager 上的其他 Subtask，因为整个 TCP 连接都被阻塞了。比如在图 3 中，因为 Subtask 4 一个 Channel 没有空闲 Buffer，使用同一连接的其他 3 个 Channel 也无法通信。为了解决这个问题，Flink 自 1.5 版本引入了 Credit-based 数据流控制为 TCP 连接提供更加细粒度的控制。</p><p><center><p><img src="/img/network-stack/initial-channel.png" alt="图4.Channel 初始状态" title="图4.Channel 初始状态"></p></center></p><p></p><p>具体来说，在接受端的 Buffer 被划分为 Exclusive Buffer 和 Floating Buffer 两种，前者是固定分配到每条接受队列里面的，后者是在 Subtask 级别的 Buffer Pool 里供动态分配。发送队列里的数据称为 Blacklog，而接收队列里的 Buffer 称为 Credit。Credit-Based 数据流控制的核心思想则是根据接收端的空闲 Buffer 数（即 Credit）来控制发送速率，这和 TCP 的速率控制十分类似，不过是作用在应用层。</p><p>假设当前发送队列有 5 个 Blacklog，而接收队列有 2 个空闲 Credit。首先接收端会通知发送端可以发送 2 个 Buffer，这个过程称为 Announce Credit。随后发送端接收到请求后将 Channel Credit 设为 2，并发送 1 个 Buffer（随后 Channel Credit 减为 1 ），并将剩余 4 个 Backlog 的信息随着数据一起发给接收端，这个两个过程分为称为 Send Buffer 和 Announce Blacklog Size。接收端收到 Backlog Size 之后会向 Buffer Pool 申请 Buffer 以将队列拓展至可以容纳 Backlog Size 的数据，但不一定能全部拿到。因为队列目前有一个空闲 Buffer，因此只需要向 Buffer Pool 申请 3 个 Buffer。假设 3 个 Buffer 都成功申请到，它们会成为 Unannounced Credit，并在下一轮请求中被 Announce。</p><p><center><p><img src="/img/network-stack/credit-based-flow-control.png" alt="图5.Credit-based 流控制" title="图5.Credit-based 流控制"></p></center></p><p></p><p>当一条 Channel 发送端的 Announced Credit 与 接收端的 Unannounced Credit 之和不小于 Blacklog Size 时，该 Channel 处于正常状态，否则处于反压状态。</p><p>从总体上讲，Credit-based 数据流控制避免了阻塞 TCP 连接，使得资源可以更加充分地被利用，另外通过动态分配 Buffer 和拓展队列长度，可以更好地适应生产环境中的不断变化的数据分布及其带来的 Channel 状态抖动，也有利于缓减部分 Subtask 遇到错误或者处理速率降低造成的木桶效应。然而这个新的机制也会引入额外的成本，即每传输一个 Buffer 要额外一轮 Announce Credit 请求来协商资源，不过从官方的测试来看，整体性能是有显著提升的。</p><p><center><p><img src="/img/network-stack/ccfc-performance.png" alt="图6.Credit-based 流控制性能提升" title="图6.Credit-based 流控制性能提升"></p></center></p><p></p><h2 id="重构-Task-Thread-和-IO-Thread-的协作模型"><a href="#重构-Task-Thread-和-IO-Thread-的协作模型" class="headerlink" title="重构 Task Thread 和 IO Thread 的协作模型"></a>重构 Task Thread 和 IO Thread 的协作模型</h2><p>熟悉网络传输的同学应该对高吞吐和低延迟两者的 trade-off 十分熟悉。网络是以 batch 的形式来传输数据的，而每个 batch 都会带来额外的空间开销（header 等元数据）和时间开销（发送延迟、序列化反序列化延等），因此 batch size 越大则传输的开销越小，但是这也会导致延时更高，因为数据需要在缓存中等待的时间越久。对于实时类应用来说，我们通常希望延迟可以被限定在一个合理的范围内，因此业界大多数的做法是设置一个 batch timeout 来强制发送低于 batch size 的数据 batch，这通常需要额外设置设置一个线程来实现。</p><p>Flink 也不例外。在上图的 TCP 连接发送端是 Netty Server，而接收端是 Netty Client，两者都会有 event loop 不断处理网络 IO。以实时作业为例子，与 Netty 组件直接交互的是 StreamRecordWriter 和 StreamRecordReader (现已被 StreamWriter 和 StreamInputProcessor 代替)，前者负责将 Subtask 最终输出的用 StreamRecord 包装的数据序列化为字节数组并交给 Netty Server，后者负责从 Netty Client 读取数据并反序列化为 StreamRecord。</p><p><center><p><img src="/img/network-stack/record-writer.png" alt="图7.StreamRecordWriter" title="图7.StreamRecordWriter"></p></center></p><p></p><p>当发送数据时，StreamRecordWriter 将记录反序列化为字节数组，并拷贝至 Netty Server 的 Channel 的一个 Buffer 中，如果 Buffer 满了它会提醒 Netty Server 将其发送。此后 StreamRecordWriter 会重新从 BufferPool 申请一个空的 Buffer 来重复上述过程，直至作业停止。为了实现 batch timeout，Flink 设置了一个 OutputFlusher 线程，它会定时 flush 在 Channel 中的 Buffer，也就是通知 Netty Server 有新的数据需要处理。Netty Server 会在额外分配线程来读取该 Buffer 到其已写的位置并将相关内容发送，其后该未写满 Buffer 会继续停留在 Channel 中等待后续写入。</p><p><center><p><img src="/img/network-stack/output-flusher.png" alt="图8.OutputFlusher" title="图8.OutputFlusher"></p></center></p><p></p><p>这种实现主要有两个问题: 一是 OutputFlusher 和 StreamRecordWriter 主线程在 Buffer 上会有竞争条件，因此需要同步操作，当 Channel 数量很多时这会带来性能上的损耗；二是当我们需要延迟尽可能小时，会将 timeout 设为 0 (实际上提供了 flushAlways 选项)，然后每写一条记录就 flush 一次，这样会带来很高的成本，最坏的情况下会造成 Netty Server 频繁触发线程来读取输入，相当于为每个 Buffer 设置一个 event loop。一个简单的优化想法是，既然 Netty Server 本来就有 event loop，为什么不让 Netty 线程自己去检测是否有新数据呢？因此 Flink 在 1.5 版本重构了这部分的架构，弃用了要求同步的 OutputFlusher 线程，改为使用 StreamRecordWriter 和 Netty 线程间的非线程安全交互方式来提高效率，其中核心设计是 BufferBuilder 和 BufferConsumer。</p><p><center><p><img src="/img/network-stack/buffer-builder-consumer.png" alt="图9.BufferBuilder &amp; BufferConsumer" title="图9.BufferBuilder &amp; BufferConsumer"></p></center></p><p></p><p>BufferBuilder 和 BufferConsumer 以生产者消费者的模式协作，前者是会被 StreamRecordWriter 调用来写入 Buffer，后者会被 Netty Server 线程调用，两者通过 volatile int 类型的位置信息来交换信息。通过这种方式，StreamRecordWriter 不会被 OutputFlusher 阻塞，资源利用率更高，网络传输的吞吐量和延迟均可受益。</p><p><center><p><img src="/img/network-stack/rework-performance.png" alt="图10.重构前后性能对比" title="图10.重构前后性能对比"></p></center></p><p></p><h2 id="避免不必要的序列化和反序列化"><a href="#避免不必要的序列化和反序列化" class="headerlink" title="避免不必要的序列化和反序列化"></a>避免不必要的序列化和反序列化</h2><p>众所周知，序列化和反序列化是成本很高的操作，尤其是对于实时计算来说，因此 Flink 在避免不必要的序列化和反序列化方面做了不少优化工作。</p><h3 id="Object-Reuse-模式（Stream-API）"><a href="#Object-Reuse-模式（Stream-API）" class="headerlink" title="Object Reuse 模式（Stream API）"></a>Object Reuse 模式（Stream API）</h3><p>在作业拓扑优化阶段，Flink 会尽可能将多个 Operator 合并为 Operator Chain 来减少 Task 数，因为 Subtask 内的 Operator 运行在同一个线程，不需要经过网络传输。尽管 Chained Operator 之间没有网络传输，但不同 Operator 直接共享对象实例并不安全，因为对象可能同时被多个算子并发访问造成意想不到的后果，并且按照函数式编程的理念，Operator 不应该对外界造成副作用，一个典型的正面例子就是 Scala 中的 Pure Function <a href="https://hello-scala.com/405-pure-functions.html" target="_blank" rel="external">[5]</a>，因此默认情况下两个 Chained Operator 的数据对象传递是通过深拷贝来完成的，而深拷贝则是通过一轮序列化和反序列实现。不过出于性能考虑，自 Flink 提供了 <code>Object Resue Mode</code> 来关闭 Chained Operator 间的数据拷贝。</p><p><center><p><img src="/img/network-stack/object-reuse.png" alt="图11.Object Reuse Mode" title="图11.Object Reuse Mode"></p></center></p><p></p><p><code>Object Resue Mode</code> 属于高级选项，当使用 Object Reuse 时用户函数必须符合 Flink 要求的规范 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/#operating-on-data-objects-in-functions" target="_blank" rel="external">[2]</a>，比如不能将输入的数据对象存到 State 中，再比如不能在输出对象之后仍对其进行修改。</p><p>要注意的是，<code>Object Resue Mode</code> 在 Stream API 中的行为和在 Batch API 中的行为并不完全一致，前者是避免了 Chained Operator 之间的深拷贝，但不同 Subtask 之间（即使在同一 JVM 内）仍然需要深拷贝，而后者是每一步都是复用之前的对象，是真正的意义上的 Object Reuse。为此了统一 Object Reuse 在两个 API 的语义，Flink 社区提出了 FLIP-21 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=71012982" target="_blank" rel="external">[3]</a>，但由于具体方案没有达成共识目前还没有实现的计划。</p><h3 id="输出到多个-Channel-时只序列化一次"><a href="#输出到多个-Channel-时只序列化一次" class="headerlink" title="输出到多个 Channel 时只序列化一次"></a>输出到多个 Channel 时只序列化一次</h3><p>由于 Flink 维护的 RecordWriter 是 Channel 级别的，当一条数据需要被输出到多个 Channel 时（比如 broadcast），同样的数据会被序列化多次，导致性能上的浪费。因此在 1.7 版本，Flink 将 RecordWriter 的写 Buffer 操作分为将数据反序列化为字节数组和将字节数组拷贝到 Channel 里两步，从而使得多个 Channel 可以复用同一个反序列化结果 <a href="https://issues.apache.org/jira/browse/FLINK-9913" target="_blank" rel="external">[4]</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在版本迭代中，Network Stack 一直在不断改进来适应新的特性或者提高性能。其中在 1.5 版本进行了比较多的改进，包括最重要的 Credit-based 流控制和重构 Task Thread 和 IO Thread 的协作模型。</p><p>作为底层基础架构，Network Stack 设计的好坏很大程度上决定了一个计算框架的性能上限，其重要性对于 Flink 开发者或者有意贡献代码的用户而言不必多说。而对于 Flink 用户而言，熟悉 Network Stack 也可以让你在开发阶段提前预计或者部署后及时发现应用的瓶颈，从而在应对生产环境的部署复杂性时更加游刃有余。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.ververica.com/flink-forward-berlin/resources/improving-throughput-and-latency-with-flinks-network-stack" target="_blank" rel="external">Improving throughput and latency with Flink’s network stack</a><br>2.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/batch/#operating-on-data-objects-in-functions" target="_blank" rel="external">Operating on data objects in functions</a><br>3.<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=71012982" target="_blank" rel="external">FLIP-21 - Improve object Copying/Reuse Mode for Streaming Runtime</a><br>4.<a href="https://issues.apache.org/jira/browse/FLINK-9913" target="_blank" rel="external">FLINK-9913 - Improve output serialization only once in RecordWriter</a><br>4.<a href="https://hello-scala.com/405-pure-functions.html" target="_blank" rel="external">Pure Functions</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为工业级的流计算框架，Flink 被设计为可以每天处理 TB 甚至 PB 级别的数据，所以如何高吞吐低延迟并且可靠地在算子间传输数据是一个非常重要的课题。此外，Flink 的数据传输还需要支持框架本身的特性，例如反压和用于测量延迟的 latency marker。在社区不断的迭代中，Flink 逐渐积累了一套值得研究的网络栈（Network Stack），本文将详细介绍 Flink Network Stack 的实现细节以及关键的优化技术。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>什么是 Flink State Evolution?</title>
    <link href="https://link3280.github.io/2019/03/17/%E4%BB%80%E4%B9%88%E6%98%AF-Flink-State-Evolution/"/>
    <id>https://link3280.github.io/2019/03/17/什么是-Flink-State-Evolution/</id>
    <published>2019-03-17T06:55:51.000Z</published>
    <updated>2019-04-14T13:02:35.873Z</updated>
    
    <content type="html"><![CDATA[<p>State Evolution 是 Apache Flink （下简称 Flink）1.7 版本引入的新特性，目的是为用户提供迭代或修改 State 的方法，以适应长期运行的作业的版本迭代需求，比如迁移 State 到不同的序列化框架，或者对 State 的数据结构进行改变，甚至直接对 State 的内容进行修改。该特性对于企业级应用来说有着重大的意义。</p><p>作为 Statefull 计算框架，Flink 作业的状态通常用 State API 来保存而不是存在的数据库等外部存储，这样在获得更好的一致性和数据本地性的同时，也牺牲了使用数据库的灵活性和可访问性。State 以序列化的形式（Savepoint/Checkpoint）存在，对于很多不熟悉 Flink 序列化的用户来说相当于黑盒子，难以验证其中的数据正确性，更不用说修改其数据结构或者序列化格式，而 State Evolution 则是 Flink 在 State 管理上的一次很好的探索。</p><a id="more"></a><p>根据 Gordon Tai 在 2018 年 Flink Forward Berlin 的分享[1]，State Evolution 主要分为 State Schema Migration、Savepoint Management 和 Upgradability Dry-Runs 三个部分，其中 State Schema Migration 已经在 1.7 版本支持，其余的官方支持尚在开发中。</p><h2 id="State-Schema-Migration"><a href="#State-Schema-Migration" class="headerlink" title="State Schema Migration"></a>State Schema Migration</h2><p>State Schema Migration 的目的在于使得 State 可以随作业程序更新而更新，其工作原理和 Flink 流处理作业其他类型的更新一样，都是利用 Savepoint 来迁移作业状态。作业运行时，随着计算的进行 Flink 会持续地读写本地的 State，然后在 Savepoint 的时候将本地的 State 上传至分布式的文件系统，新版本的作业启动时只需要读取 Savepoint 即可恢复原先的作业状态。</p><p><center><p><img src="/img/state-evolution/flink-job-update-1.png" alt="旧版本作业 cancel with savepoint" title="旧版本作业 cancel with savepoint"></p></center></p><p></p><p><center><p><img src="/img/state-evolution/flink-job-update-2.png" alt="新版本作业 start with savepoint" title="新版本作业 start with savepoint"></p></center></p><p></p><p>不过由于 State 涉及到不同的序列化框架，比起像改变 JobGraph 或者作业并行度等其他类型的更新，它的更新方法更加复杂一些，主要问题在于在从 Savepoint 恢复时我们如何读取之前的版本的序列化对象。这需要将 State 更新分为 State Schema 更新和 State 序列化框架的更新这两种情况考虑。</p><p>State Schema 的更新主要是和业务逻辑的变更相关，比如新增一个字段或者移除一个字段，这种版本升级主要需要考虑序列化框架的兼容性，比如 Java 默认的对象序列化框架，可以兼容新增字段但不能兼容移除或重命名字段。如果变更是可兼容的，那么无需额外操作即可迁移，否则可能需要考虑更换序列号框架。然而 Flink 默认情况下会根据 State 的 POJO 类型来生成 Adhoc 的（反）序列化器（PojoSerializer），任何 POJO 的变更都会导致不同的序列化器，因此对于 Flink 来说 State Schema 的更新通常等同于 State 序列化框架的更新。</p><p>更新 State 序列化框架的实现里有很多有意思的细节，其中一个是不同的 StateBackend 对于序列化器的使用方式不同。基于内存的 StateBackend （Heap Based StateBackend），比如 FsStateBackend，属于 “Lazy serialization, eager deserialzition”，意思是仅当 Savepoint 时才会将内存的 State 序列化，而读取 Savepoint 时会全部反序列化到内存。</p><p><center><p><img src="/img/state-evolution/heap-based-backend-1.png" alt="Heap-based StateBackend snapshot" title="Heap-based StateBackend "></p></center></p><p></p><p>举个例子，假设我们从 v1 序列化器迁移至 v2 序列化器，在 Savepoint 的时候 Heap Based StateBackend 会将所有 key 的 State 全部用 v1 序列化器写出，在作业重启恢复时序列化器已经更新为 v2，但是只要我们还可以得到 v1 的（反）序列化器就可以顺利迁移，因此一个简单的方法是每次 Savepoint 时将当前使用的序列化器也一并使用 Java 序列化来写入 Savepoint，然后在读取 Savepoint 时我们首先提取（反）序列化器，再通过这个（反）序列化器来读取 State。</p><p><center><p><img src="/img/state-evolution/heap-based-backend-2.png" alt="Heap-based StateBackend restore" title="Heap-based StateBackend restore"></p></center></p><p></p><p>然而在使用 Off Heap StateBackend 的时候，情况则变得更加复杂。因为 Off Heap StateBackend，比如 RocksDBStateBackend，是 “Eager serialization, lazy deserialization” 的，即每当 State 有更新时就会将对应 key 的 State 写到 Savepoint，而读取 Savepoint 时仅会读取被访问到的 key 的 State。</p><p><center><p><img src="/img/state-evolution/off-heap-backend-1.png" alt="Off-heap StateBackend snapshot" title="Off-heap StateBackend snapshot"></p></center></p><p></p><p>这样造成的一个问题就是在一次作业运行中，可能同时存在使用不同版本的序列化器的 State。比如在前一次运行的 Savepoint，Flink 使用 v1 序列化器写了 5 个 key 的 State，而在第二次运行时我们只更新了其中 2 个 key，那么被更新的 key 是使用 v2 序列化器，而没有被更新的 key 仍然使用 v1 序列化器。因此在使用 Off Heap StateBackend 的情况下，我们不仅需要上一次运行使用的序列化器，还需要之前所有运行使用的序列化器。</p><p><center><p><img src="/img/state-evolution/off-heap-backend-2.png" alt="Off-heap StateBackend restore" title="Off-heap StateBackend restore"></p></center></p><p></p><p>这种情况下保存每次运行时使用的序列化器显然是不现实的，因此 Flink 提供了两种办法来更新序列化器: 一是序列化器保证向后兼容，二是 State Migration Process。对于序列化器来说，向后兼容性是十分重要的特性，大部分流行的序列化器，比如 ProtoBuf、Avro、Thrift，都提供了向后兼容的能力。如果用户使用定制化的序列化器，Flink 也提供了编程 API 的支持，让用户可以根据序列化器的版本号来维护向后兼容性。具体来说，每次 Savepoint snapshot 的时候，Flink 会将序列化器的配置信息也存储下来，在作业读取 Savepoint 时用户定制的序列化器可以根据相关信息来配置自己，再反序列化 State。</p><p><center><p><img src="/img/state-evolution/serializer-compatability.png" alt="serializer backward compatabilitye" title="serializer backward compatability"></p></center></p><p></p><p>但目前来说 Flink 默认的 PojoSerializer 并不能提供这样的向后兼容性，这就需要下面的 State Migration Process。State Migration Process 即在恢复作业状态时扫描所有 key 的 State 并强制用上次运行的序列化器反序列化，从而保证使用 Off Heap StateBackend 的情况下 State 不会存在多个序列化版本。但是在 State 比较大的情况下，在作业启动时进行 State Migration Process 可能会带来很长的恢复时间，因此该方法还是需要结合实际慎重使用。</p><h2 id="Savepoint-Management"><a href="#Savepoint-Management" class="headerlink" title="Savepoint Management"></a>Savepoint Management</h2><p>不同于 State Schema Migration 在作业恢复时执行，Savepoint Management 的目的是提供离线读写 State 的能力。这种对 State 的管理能力对于生产级应用是至关重要的，因为程序 bug 是不能完全避免的，而当 bug 导致作业 State 错误时我们需要有可以修复这种错误的能力。在之前我们大多数情况下的做法是修复程序 bug 并重流数据来重新计算正确的 State，但是这种做法成本太高，甚至有时是不可行的，比如 Kafka 的消息已经被清理或者作业外部依赖的状态已经改变。而通过 Savepoint Management，只要知道如何根据错误的 State 计算得出正确的 State，我们就可以离线地修复 State 的问题并应用到线上。此外，我们甚至可以用外部的数据来计算出一个之前不存在的 State，然后用于作业首次运行时的算子状态的初始化。</p><p>截止至 Flink 1.7 版本，官方还没有这方面的支持，不过社区有个可用的项目 Bravo [2]可以一定程度上满足这方面的需求，在和 Flink 社区达成共识之后 Bravo 也会被合并到 Flink 项目里。Bravo 是由荷兰的 King 银行，Flink 的重度用户之一，开发的一个用于读写 Savepoint 的工具。其原理是利用 Flink 序列化器提供 OperatorStateReader 和 OperatorStateWriter 两个主要 API，用户可以利用它们来将 Savepoint 读入转为 Flink DataSet 或者将 DataSet 写出为 Savepoint。</p><p>State 的读取是以算子为单位的，我们需要指定 uid、State 类型、POJO 类型来定位具体的 State。一个简单的使用 Demo 如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// First we start by taking a savepoint/checkpoint of our running job...</div><div class="line">// Now it&apos;s time to load the metadata</div><div class="line">Savepoint savepoint = StateMetadataUtils.loadSavepoint(savepointPath);</div><div class="line"></div><div class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</div><div class="line"></div><div class="line">// We create a KeyedStateReader for accessing the state of the operator with the UID &quot;CountPerKey&quot;</div><div class="line">OperatorStateReader reader = new OperatorStateReader(env, savepoint, &quot;CountPerKey&quot;);</div><div class="line"></div><div class="line">// The reader now has access to all keyed states of the &quot;CountPerKey&quot; operator</div><div class="line">// We are going to read one specific value state named &quot;Count&quot;</div><div class="line">// The DataSet contains the key-value tuples from our state</div><div class="line">DataSet&lt;Tuple2&lt;Integer, Integer&gt;&gt; countState = reader.readKeyedStates(</div><div class="line">KeyedStateReader.forValueStateKVPairs(&quot;Count&quot;, new TypeHint&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;&#125;));</div></pre></td></tr></table></figure><p>Bravo 是基于 Flink 1.5 版本开发的，目前已经支持 1.6 版本引入的 State TTL，不过用户还是比较少，建议大家去尝试下（Bravo 维护者 Gyula Fora 非常 nice，提 issue 很快可以得到解决）。</p><h2 id="Upgradability-Dry-Runs"><a href="#Upgradability-Dry-Runs" class="headerlink" title="Upgradability Dry-Runs"></a>Upgradability Dry-Runs</h2><p>State Evolution 还提供的一项很方便的功能是 Upgradability Dry-Runs，这项功能用于离线检查作业的版本兼容性，以帮助用户提前发现兼容性问题。常见的兼容性问题主要有:</p><ul><li>作业拓扑的变更。这可能会导致算子 uid 的匹配失败，进而导致作业恢复后状态的不完整。目前来说 Flink 默认会在作业提交时进行安全性检查，用户可以通过 <code>-n,--allowNonRestoredState</code> 参数来允许不完整的状态恢复，但是这种检查应该测试阶段而不是在部署阶段完成。</li><li>State Schema 的变更。如上文所说，这会导致新版本的作业恢复失败并需要马上回滚。</li></ul><p>目前来说社区还没有这方面的讨论和开发计划，但 Gordon 分享了一些可行的办法。离线验证的关键点在于从用户作业程序提取出需要的信息，并对比找出其中不兼容的点。对于作业拓扑图来说，我们可以计算出两者的 StreamGraph 并比较它们的 uid。而对于 State Schema 来说则可能绕一点，因为按照 Flink 目前的设计注册 State 的过程是只对 StateBackend 可见，因此可能需要入侵 StateBackend 来支持兼容性检查。而另外一个办法则是引入新的 annotation 并在声明 State 的时候标记这个 State，这样我们就可以在解析用户代码时得出 State 的信息，并用于检测兼容性。这种功能暂时被命名为 Eager State Declaration，具体的一个 Demo 如下。</p><p><center><p><img src="/img/state-evolution/eager-state-declaration.png" alt="eager-state-declaration" title="eager state declaration"></p></center></p><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上所述，State Evolution 是关注于 State 如何管理的一系列特性，用户可以利用这些特性来获得在 State 版本演进或者类似传统数据库的 CRUD 上的更强的控制能力和更大操作空间，以更好地维护长期运行并不断迭代的作业。另外，从 1.7 版本开始我们可以看到 Flink 社区越来越重视版本兼容和跨版本迁移，包括 State Evolution 和 REST 等 API 的版本化，这也是一个项目走向成熟和准备好企业级生产的标志之一。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://www.ververica.com/flink-forward-berlin/resources/upgrading-apache-flink-applications-state-of-the-union" target="_blank" rel="external">Upgrading Apache Flink Applications: State of the Union</a><br>2.<a href="https://github.com/king/bravo" target="_blank" rel="external">Bravo: Utilities for processing Flink checkpoints/savepoints</a><br>3.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/schema_evolution.html" target="_blank" rel="external">Apache Flink 官方文档: State Schema Evolution</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;State Evolution 是 Apache Flink （下简称 Flink）1.7 版本引入的新特性，目的是为用户提供迭代或修改 State 的方法，以适应长期运行的作业的版本迭代需求，比如迁移 State 到不同的序列化框架，或者对 State 的数据结构进行改变，甚至直接对 State 的内容进行修改。该特性对于企业级应用来说有着重大的意义。&lt;/p&gt;
&lt;p&gt;作为 Statefull 计算框架，Flink 作业的状态通常用 State API 来保存而不是存在的数据库等外部存储，这样在获得更好的一致性和数据本地性的同时，也牺牲了使用数据库的灵活性和可访问性。State 以序列化的形式（Savepoint/Checkpoint）存在，对于很多不熟悉 Flink 序列化的用户来说相当于黑盒子，难以验证其中的数据正确性，更不用说修改其数据结构或者序列化格式，而 State Evolution 则是 Flink 在 State 管理上的一次很好的探索。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>深入理解流计算中的 Watermark</title>
    <link href="https://link3280.github.io/2019/02/24/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84-Watermark/"/>
    <id>https://link3280.github.io/2019/02/24/深入理解流计算中的-Watermark/</id>
    <published>2019-02-24T08:47:39.000Z</published>
    <updated>2019-04-14T13:02:44.833Z</updated>
    
    <content type="html"><![CDATA[<p>近年来流计算技术发展迅猛，甚至有后来居上一统原本批处理主导的分布式计算之势，其中 Watermark 机制作为流计算结果准确性和延迟的桥梁扮演着不可或缺的角色。然而由于缺乏高质量的学习资源加上计算 Watermark 确实不是一件容易的事情，不少有着批处理计算背景的用户在流计算作业的开发中可能并不理解 Watermark 的重要意义，从而多走了很多弯路。为此，本文将基于笔者的学习积累和开发经验，谈谈个人对 Watermark 的理解，希望起到抛砖引玉的作用。</p><a id="more"></a><p>本文将首先说明 Watermark 提出的背景，然后详细解析 Watermark 的原理，最后结合工业案例说明 Watermark 在实践中如何被应用。</p><h1 id="Watermark-背景"><a href="#Watermark-背景" class="headerlink" title="Watermark 背景"></a>Watermark 背景</h1><p>自 Google 的三篇论文和 Hadoop 出现后，工业界的分布式计算技术进入了百花齐放的时期，然而相比于离线批处理计算的蓬勃发展，作为后来者的流计算却有点停滞不前。流计算和批处理在对于每条记录的单独处理上基本一致，不同之处在于聚合类的计算。批处理计算结果的输出依赖于输入数据集合的结束，而流计算的输入数据集通常是无边界的，不可能等待输入结束再输出结果。针对这个问题流处理引入了窗口的特性，简单来说就是将无限的数据流按照时间范围切分为一个个有限的数据集，所以我们依然能够沿用批处理的计算模型。来到这时，业界在流计算和批处理的关系上出现了两种截然不同的观点，一个观点认为流计算是批处理的特例，另一个观点则认为批处理是流处理的特例。</p><h2 id="实时计算与离线计算的分离"><a href="#实时计算与离线计算的分离" class="headerlink" title="实时计算与离线计算的分离"></a>实时计算与离线计算的分离</h2><p>流计算是批处理的特例的观点在早期占据了主导的地位，其中最为典型的便是以 Spark Streaming 为代表的 micro-batching 类型的实时处理框架的流行。Micro-batching 的主要思想是以分钟甚至秒级别的执行间隔来将批处理应用到数据流上，但不久后人们意识到这种计算模型依然不能完全满足低延迟高准确性的要求，主要问题除了批处理调度导致的延迟外，还有一点是窗口变小后，数据收集延迟对结果准确性的影响大大增强了。比如说计算一个游戏服务器每 5 min 的新登录玩家数，但因为网络或者客户端设备故障等因素，12:00 的玩家登录日志可能在 12:10 才被收集到服务器，如果实时计算在 12:05:00 就输出结果，必然会漏掉这条迟到的数据。在离线计算中这样的问题并不明显，因为一个批次的时间跨度较大且对延迟要求不高，因此计算的时间可以设置一个安全的延迟，比如 1 个小时，确保数据都已经收集完成后再开始计算，即使有大量数据是在 1 个小时后才收集到，只需要重算结果即可。然而这样的实践经验并不能应用于实时计算，一是引入额外的安全延迟对于很多对延迟敏感的场景不可接受，二是实时计算的重算要比批处理重算的成本高出很多。因此业界普遍是采用 结合离线和实时处理的 Lambda 架构来应对这个问题，其主要思想是同时运行实时和离线两个数据处理管道，实时管道提供最近小时内的临时结算结果，而离线管道提供小时以前的计算结果并覆盖掉对应时间段的实时计算结果，查询时将两者的结果再进行合并产生最终的结果<a href="">[1]</a>。</p><h2 id="实时计算与离线计算的融合"><a href="#实时计算与离线计算的融合" class="headerlink" title="实时计算与离线计算的融合"></a>实时计算与离线计算的融合</h2><p>实时计算与离线计算的分离说明了用批处理模型不足以表达流计算，于是人们开始探索批处理是流计算特例的模型。2015 年 Google 发表名为 The Dataflow Model 的论文，这篇论文较为详细地阐述了实时流计算和离线批计算的统一模型（出于篇幅原因不展开讲，详情请见<a href="">[2]</a>），而该模型基于批处理是流计算特例的观点。The Dataflow Model 将计算分为四个要素，即 what、where、when 和 how:</p><ul><li>what 表示要计算什么结果，即对数据的一系列转换操作；</li><li>where 表示结果计算上下文，即窗口如何定义；</li><li>when 表示何时输出和物化计算结果；</li><li>how 表示如何清理已经输出的结果。</li></ul><p>在 what 和 where 两点上流计算和批处理是相似的，而主要不同之处在于 when 和 how 两点，这两点在批处理里基本不会涉及，但在流计算里却影响着计算结果的准确性，实际上它们分别对应了上文所说的批处理经验不能应用于实时计算的两个问题。本文主要讨论的 watermark 就是属于 when 要素里的一种技术，因而下文将主要关注 when。</p><p>在批处理中 when 是输入数据集结束的时候，how 是以覆盖的形式来清理之前的输出结果，处理模式都是固定的，因此用户并不需要考虑。举个例子，假设要计算一个游戏每天的玩家充值金额，用离线计算时我们会考虑如何将充值金额从日志中提取出来并累加到一起，此为 what；再考虑批处理的运行时间，比如每天 00:30，所以每次计算是处理 24 小时采集到的数据，此为 where；而批处理的 when 是和 where 绑定的，即 00:30 计算开始，结束后马上输出结果；至于 how，不同批次的批处理运行的结果是互不相干的，同一批次的运行结果会覆盖前一次运行的结果。</p><p>然而如果游戏策划急于知道某个活动是否有带动玩家充值，希望看到每分钟更新的实时数据，那么上述题目改为用实时流计算去实现，此时要考虑的东西会复杂一点。首先，我们可以依旧可以复用批处理的 what 和 where，即定义一个时间范围为 24 小时的窗口，计算逻辑和之前一样；在 when 方面，为了可以实时地得到最新的计算结果，我们需要定义每分钟输出一次最新的计算结果，直到达到 24 小时后输出最终结果；而在 how 方面，我们每次的输出结果只需要覆盖之前的结果即可。然而 when 的问题并没有这么简单。还记得我们之前说过数据采集延迟吗？可能一个用户充值的时间在 16:00，但中间采集的延迟可能有 1 min，导致到达服务器却是 16:01 分，如果基于充值记录被处理的时间（即 processing time）来进行窗口划分，用户充值记录可能会被计入错误的窗口，所以我们应该以用户充值这个时间（即 event time）发生的时间为准。这里的难点在于我们计算时并不能判断所有 event time 窗口内的数据被收集完，因为数据的延迟是不可预知的，这被称为窗口完整性问题。针对窗口完整性问题，The Dataflow Model 提出了 Watermark 的解决方案。</p><h1 id="Watermark-原理解析"><a href="#Watermark-原理解析" class="headerlink" title="Watermark 原理解析"></a>Watermark 原理解析</h1><p>Watermark 并没有很正式的官方定义，最接近定义的是 Streaming 102<a href="">[3]</a> 里的一段描述。</p><blockquote><p>A watermark is a notion of input completeness with respect to event times. A watermark with a value of time X makes the statement: “all input data with event times less than X have been observed.” As such, watermarks act as a metric of progress when observing an unbounded data source with no known end.                                                                    </p></blockquote><p>简单来说 Watermark 是一个时间戳，表示已经收集完毕的数据的最大 event time，换句话说 event time 小于 Watermark 的数据不应该再出现，基于这个前提我们才有可能将 event time 窗口视为完整并输出结果。Watermark 设计的初衷是处理 event time 和 processing time 之间的延迟问题，三者的关系可以用下图展示:</p><center><p><img src="/img/streaming-watermark/streaming-system-watermark.png" alt="图 1. Event Time/Processing Time/Watermark 三者关系" title="图 1. Event Time/Processing Time/Watermark 三者关系"></p></center><p>理想的情况下数据没有延迟，因此 processing time 是等于 event time 的，理想的 Watermark 应该是斜率为 45 度的直线。然而在真实环境下，processing time 和 event time 之间总有不确定的延迟，表现出来的 Watermark 会类似图 1 中的红色的曲线。其中红色曲线与理想 Watermark 的纵坐标差值称为 processing-time lag，表示在真实世界中的数据延迟，而横坐标的差值表示 event-time skew，表示该延迟带来的 event-time 落后量。</p><p>Watermark 通常是基于已经观察到的数据的 event time 来判断（当然也可以引入 processing time 或者其他外部参数），具体需要用户根据数据流的 event time 特征来决定，比如最简单的算法就是取目前为止观察到的最大 event time。在数据流真实 event time 曲线是单调非减的情况下，比如 event time 是 Kafka producer timestamp 时，我们是可以计算出完美符合实际的 Watermark 的，然而绝大多数情况下数据流的 event time 都是乱序的，因此计算完美的 Watermark 是不现实的（实际上也是没有必要的），通常我们会以启发性的 Watermark 算法来代替。</p><p>启发性的 Watermark 算法目的在于在计算结果的延迟和准确性之间找到平衡点。如果采用激进的 Watermark 算法，那么 Watermark 会快于真实的 event time，导致在窗口数据还不完整的情况下过早输地出计算结果，影响数据的准确性；如果采用保守的 Watermark 算法，那么 Watermark 会落后于真实的 event time，导致窗口数据收集完整后不能及时输出计算结果，造成数据的延迟。实际上上文所说的 Watermark 取观察到的最大 event time 和批处理使用的设置一个足够大的安全延迟的办法分别就属于 Watermark 算法的两个极端。很多情况下用户偏向于牺牲一定的延时来换取准确性，不过在像金融行业的欺诈检测场景中，低延迟是首要的，否则准确性再高也没有意义。针对这种情况 The Dataflow Model 提供了 allow lateness 的机制，工作的原理是用户可以设置一个时间阈值，如果在计算结果输出后的这个阈值时间内发现迟到的数据，计算结果会被重新计算和输出，但如果超出这个阈值的迟到数据就会被丢弃。</p><p>这时你们可以看到要开发一个高质量的实时作业是多么不易了，这也是很多实时应用开发者最为头疼的地方，或许以后利用机器学习去计算 Watermark 是个不错的主意（然后我们的工作就可以愉快地从调 Watermark 算法参数变为调机器学习模型参数了 :) ）。</p><h1 id="Watermark-实践"><a href="#Watermark-实践" class="headerlink" title="Watermark 实践"></a>Watermark 实践</h1><p>接下来我们将结合工业生产的案例来说明实战中 Watermark 是如何影响流计算的。Watermark 在不同计算引擎的实现并不相同，本文将以笔者使用最多的 Apache Flink （下文简称 Flink）作为例子来说明。</p><p>对于游戏行业来说，游戏的日活跃玩家数是个很常见的指标，游戏策划或者运营通常可以根据日活跃玩家数的变动来实时地监控某个活动是否收到玩家欢迎的程度，但是游戏可能有海外服务器，数据收集的延迟可能差别较大，造成数据流 event time 乱序比较严重，在这种情况下设计 Watermark 算法是个比较大的挑战。</p><p>假设我们有 A、B、C 共 3 台服务器，其中 A、B 为国内服务器，延迟较低且稳定，而 C 为海外服务器，延迟较高且不稳定，而我们需要计算每分钟内的登录玩家数。</p><center><p><img src="/img/streaming-watermark/data-latency-of-servers.png" alt="图 2. 数据流延迟" title="图 2. 数据流延迟"></p></center><p>我们现在面临两种可能带来 event time 乱序的因素：一是不同服务器间的延迟不同，比如可能先收到服务器 A 在 t2 的数据，再收到服务 C 在 t1 的数据；二是同一服务器的不同数据的延迟不同，比如可能先收到服务器 C t2 的数据再收到 t1 的数据。针对第二种因素，我们可以对不同服务器的数据分别计算 Watermark，再取其中的最小值作为 Watermark，而针对第一种因素，我们则需要设计出针对单个服务器数据流的合理 Watermark 算法。</p><p>在算法实现上，Flink 提供两种触发 Watermark 更新的方法，即在收到特殊的消息时触发或者定时触发，我们这里将选用定时触发的方法。因为窗口是一分钟比较小，我们这里将定时的间隔设为 5 秒，也就是说 Watermark 大约落后真实 Watermark 5 秒，然后这 5 秒内 Watermark 是不会提升的，所以可以容忍局部的 processing lag。</p><p>我们试着取目前为止观察到的最大时间戳作为 Watermark，那么 Watermark 的效果如下(为了在消费端更加直观，我们将坐标系调转，现在 x 轴表示 processing time）。</p><center><p><img src="/img/streaming-watermark/watermark-implemented.png" alt="图 3. Watermark 算法实现" title="图 3. Watermark 算法实现"></p></center><p>其中 t0-t3 分别表示 Watermark 提升的时间点，黄虚线表示在一个 Watermark 周期内的最大 event time，红线表示 Watermark。可以看到在 t0-t1 的 Watermark 周期内出现了轻微的 event time 乱序，但是并不影响计算的准确性。接下来在 t1-t2 和 t2-t3 两个周期间也发生了相似的乱序，但是这个乱序并不在同一个 Watermark 周期，因此导致正常延迟的数据被误认为是迟到数据。解决方法是引入一定可容忍的 event time skew，比如说最简单的设置一个 skew 阈值，即每次计算 Watermark 的结果都减去这个值。根据数据流延迟的不同，我们还可以给不同服务器设置不同的 skew 阈值。</p><p>上述 Watermark 算法代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">public class WatermarkProcessor implements AssignerWithPeriodicWatermarks&lt;UserLogin&gt; &#123;</div><div class="line"></div><div class="line">    private static final long ALLOWED_EVENT_TIME_SKEW = 1000L;</div><div class="line">    private static final Map&lt;String, Long&gt; maxTimestampPerServer = new HashMap&lt;&gt;(3);</div><div class="line"></div><div class="line">    @Nullable</div><div class="line">    public Watermark getCurrentWatermark() &#123;</div><div class="line">        Optional&lt;Long&gt; maxTimestamp  = maxTimestampPerServer.values().stream()</div><div class="line">                .min(Comparator.comparingLong(Long::valueOf));</div><div class="line">        if (maxTimestamp.isPresent()) &#123;</div><div class="line">            return new Watermark(maxTimestamp.get() - ALLOWED_EVENT_TIME_SKEW);</div><div class="line">        &#125; else&#123;</div><div class="line">            return null;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public long extractTimestamp(UserLogin userLogin, long previousElementTimestamp) &#123;</div><div class="line">        String server = userLogin.getServer();</div><div class="line">        long eventTime = userLogin.getEventTime();</div><div class="line">        if (!maxTimestampPerServer.containsKey(server) || </div><div class="line">                userLogin.getEventTime() &gt; maxTimestampPerServer.get(server)) &#123;</div><div class="line">            maxTimestampPerServer.put(server, eventTime);</div><div class="line">        &#125;</div><div class="line">        return eventTime;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>流计算和批处理谁是表达能力更强的计算模式，这个问题或许还将继续被争论下去，不过根据 The Dataflow Model 我们已经有足够的理论支撑来开发低延迟高准确并且可容错的流计算应用。其中流计算的准确性很大程度上决定于数据流时间的乱序程度，因此我们在开发实时流计算应用时，比起开发离线批处理应用，很大的一个不同是要考虑数据是以什么顺序到达，并针对性地设计 Watermark 算法来处理数据流时间的乱序。Watermark 算法需要平衡低延迟和高准确性两者，在引入最低延迟成本的情况下准确判断窗口的计算和输出结果的时机，通常可以从 processing lag 和 event time skew 两者的容忍阈值入手。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html" target="_blank" rel="external">How to beat the CAP theorem</a><br>2.<a href="https://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf" target="_blank" rel="external">The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive Scale, Unbounded, Out of Order Data Processing</a><br>3.<a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102" target="_blank" rel="external">Streaming 102: The world beyond batch</a><br>4.Tyler AkidauSlava, Chernyak, Reuven Lax. (2018). Streaming Systems.  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近年来流计算技术发展迅猛，甚至有后来居上一统原本批处理主导的分布式计算之势，其中 Watermark 机制作为流计算结果准确性和延迟的桥梁扮演着不可或缺的角色。然而由于缺乏高质量的学习资源加上计算 Watermark 确实不是一件容易的事情，不少有着批处理计算背景的用户在流计算作业的开发中可能并不理解 Watermark 的重要意义，从而多走了很多弯路。为此，本文将基于笔者的学习积累和开发经验，谈谈个人对 Watermark 的理解，希望起到抛砖引玉的作用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="实时计算" scheme="https://link3280.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://link3280.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink on YARN Security 浅析(Flink Part)</title>
    <link href="https://link3280.github.io/2019/01/06/Flink-on-YARN-Security-%E6%B5%85%E6%9E%90-Flink-Part/"/>
    <id>https://link3280.github.io/2019/01/06/Flink-on-YARN-Security-浅析-Flink-Part/</id>
    <published>2019-01-06T13:05:28.000Z</published>
    <updated>2019-04-14T13:03:26.497Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇关于 YARN 系统 Security 的博客，我们解析了通过 YARN 提供的 Security API，Application 已经在 RM 注册并且可以顺利地申请到 container，但 YARN 对 container 后续的凭证刷新（reacquire）并不能作用到已经在运行的 Application 进程，因此对于长期运行的 Application 而言要开发者自己实现认证和后续凭证刷新的逻辑。本文将接着分析 Flink 如何在申请到的 container 启动 jobmanger 和 taskmanager 并完成认证，也就是 Flink Application 自身的认证，以及后续的凭证刷新方法，最后再讲述最近社区对于 Flink on YARN Security 改进提案。</p><a id="more"></a><h2 id="长时间运行的-YARN-Application-的四种认证方法"><a href="#长时间运行的-YARN-Application-的四种认证方法" class="headerlink" title="长时间运行的 YARN Application 的四种认证方法"></a>长时间运行的 YARN Application 的四种认证方法</h2><h3 id="在-YARN-集群机器上预安装-keytab"><a href="#在-YARN-集群机器上预安装-keytab" class="headerlink" title="在 YARN 集群机器上预安装 keytab"></a>在 YARN 集群机器上预安装 keytab</h3><p>将 Application 可能会用到的 keytab 预先安装到 YARN 集群的所有机器的本地文件系统并设置好目录权限，然后将相关路径作为作为 Application 的配置提供给 AM 和其他普通 container。用户进程启动时通过 <code>UserGroupInformation.loginUserFromKeytab()</code> 来加载凭证和认证，并且后续通过 keytab 来刷新 kerberos 凭证。具体的安全凭证分布如图一所示，其中实线表示永久凭证 keytab 的传递，虚线表示临时凭证 token 的传递。</p><center><p><img src="/img/flink-on-yarn-security/Approach1-pre-installed-keytab.png" alt="图一. pre-installed-keytab" title="图一. pre-installed-keytab"></p></center><p>这种方式直接使用 Kerberos keytab ，绕开了 Hadoop delegation token，相当于 后者只用于 container 的申请。因此这样的优点是避开了 token 最大生命周期的问题，而缺点在于没有了 delegation token 的优点，即每次 TGT 刷新需要请求 KDC，而且 keytab 也需要比较高的运维成本。</p><h3 id="通过-YARN-分发-keytab-给-AM-和其他-container"><a href="#通过-YARN-分发-keytab-给-AM-和其他-container" class="headerlink" title="通过 YARN 分发 keytab 给 AM 和其他 container"></a>通过 YARN 分发 keytab 给 AM 和其他 container</h3><p>首先将 Application 客户端将 keytab 上传至 HDFS，并在提交 Application 时将其作为AM 需要本地化的资源。AM container 初始化时 NodeManager 会负责将 keytab 拷贝至 container 的资源目录，AM 启动时通过 <code>UserGroupInformation.loginUserFromKeytab()</code> 来重新认证。当 AM 需要申请 container 时，也将 HDFS 上的 keytab 列为需要本地化的资源，因此 container 也可以仿照 AM 进行认证。此外 AM 和 container 都必须额外实现一个线程来定时刷新 Kerberos TGT。</p><center><p><img src="/img/flink-on-yarn-security/Approach2-distribute-via-YARN.png" alt="图二. distribute-via-yarn" title="图二. distribute-via-yarn"></p></center><p>Apache Flink 目前使用的正是这种方法。比起第一种方式，优点在于 keytab 只需要被安装在 client 端，YARN 集群上的机器只在有一个用户的作业在运行时才会有该用户的 keytab，作业完成后 keytab 也会随 container 被清理掉。</p><h3 id="通过-YARN-分发-keytab-给-AM；AM-为-container-生成-delegation-token"><a href="#通过-YARN-分发-keytab-给-AM；AM-为-container-生成-delegation-token" class="headerlink" title="通过 YARN 分发 keytab 给 AM；AM 为 container 生成 delegation token"></a>通过 YARN 分发 keytab 给 AM；AM 为 container 生成 delegation token</h3><p>从 client 上传 keytab 到 AM 获得 keytab 的流程都与第二种方法相同，区别在于后续 AM 申请 container 时并不是将 keytab 列本地化资源，而是请求 container 需要的 delegation token（比如最基础的 HDFS_DELEGATION_TOKEN）并将这些 token 作为 ContainerLaunchContext 的安全凭证。由于 token 的最大生命周期问题，container 后续需要继续从 AM 获取新的 token，实现方式通常是 AM 和 container 通过 IPC 来定时更新 token。</p><center><p><img src="/img/flink-on-yarn-security/Approach3-am-generates-token.png" alt="图三. am-generates-token" title="图三. am-generates-token"></p></center><p>这种方式比起前两种方式的好处在于 keytab 只存在于 AM 机器上似乎更加安全，不过由于 container 和 AM 相同的 HDFS 访问权限，实际上它们还是可以访问到 keytab，除非 AM 启动后将 keytab 从 HDFS 删除，不过这样在 AM 崩溃的情况下 YARN 就没有办法进行重试了。目前 Apache Spark 是使用这种方式认证。</p><h3 id="Client-端推送-token-给-AM；AM-推送-token-给-container"><a href="#Client-端推送-token-给-AM；AM-推送-token-给-container" class="headerlink" title="Client 端推送 token 给 AM；AM 推送 token 给 container"></a>Client 端推送 token 给 AM；AM 推送 token 给 container</h3><p>如果将 YARN 集群视为不受信任的环境，严格限制将 keytab 分发到集群上，在 client 端推送 token 会是唯一的方式，即通过 Hadoop delegation token 启动 AM 和 container，随后 client 定时用 keytab 认证并重新获取 AM 的 token 并通过 IPC 的方式传递给它，AM 再通过 IPC 将 token 传递给 container。</p><center><p><img src="/img/flink-on-yarn-security/Approach4-client-generates-token.png" alt="图四. client-generates-token" title="图四. client-generates-token"></p></center><p>这种方式把 keytab 限制在 client 机器上最为安全，但是 client 端的实现比较重，对于作业数成千上万的部署规模来说并不合适，而且 client 会成为一个故障单点。如果要一条路走到黑，部署多个 client 和高可用，那就相当于又一个安全的小分布式集群。</p><h2 id="Flink-Application-认证源码解析"><a href="#Flink-Application-认证源码解析" class="headerlink" title="Flink Application 认证源码解析"></a>Flink Application 认证源码解析</h2><p>如上文所说，Flink 使用通过 YARN 分发 keytab 给 AM 和其他 container 的方式来认证，下面从源码角度来解析具体的实现，这会分为 Flink client 、AM（YarnClusterEntrypoint） 和 container（YarnTaskExecutorRunner）三个部分，源码以 1.6.2 版本为例。</p><h3 id="Flink-Client"><a href="#Flink-Client" class="headerlink" title="Flink Client"></a>Flink Client</h3><p>篇幅起见这里只给出核心代码和以注释形式说明，完整代码见 AbstractYarnClusterDescriptor#startAppMaster。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">// 从配置读取 keytab 路径，若不为空则注册为 container 的 LocalResource</div><div class="line">Path remotePathKeytab = null;</div><div class="line">String keytab = configuration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);</div><div class="line">if (keytab != null) &#123;</div><div class="line">LOG.info(&quot;Adding keytab &#123;&#125; to the AM container local resource bucket&quot;, keytab);</div><div class="line">remotePathKeytab = setupSingleLocalResource(</div><div class="line">Utils.KEYTAB_FILE_NAME,</div><div class="line">fs,</div><div class="line">appId,</div><div class="line">new Path(keytab),</div><div class="line">localResources,</div><div class="line">homeDir,</div><div class="line">&quot;&quot;);</div><div class="line">&#125;</div><div class="line">// 根据配置和环境变量生成 AM 的启动命令，并设置到 AM ContainerLaunchContext 里</div><div class="line">final ContainerLaunchContext amContainer = setupApplicationMasterContainer(</div><div class="line">yarnClusterEntrypoint,</div><div class="line">hasLogback,</div><div class="line">hasLog4j,</div><div class="line">hasKrb5,</div><div class="line">clusterSpecification.getMasterMemoryMB());</div><div class="line"></div><div class="line">// 如果是 Kerberized 集群，向 NameNode 申请 HDFS_DELEGATION_TOKEN，并设置到 AM ContainerLaunchContext 里</div><div class="line">if (UserGroupInformation.isSecurityEnabled()) &#123;</div><div class="line">// set HDFS delegation tokens when security is enabled</div><div class="line">LOG.info(&quot;Adding delegation token to the AM container..&quot;);</div><div class="line">Utils.setTokensFor(amContainer, paths, yarnConfiguration);</div><div class="line">&#125;</div><div class="line"></div><div class="line">amContainer.setLocalResources(localResources);</div></pre></td></tr></table></figure><p>其中 Utils 类包含了许多 helper method，获取 token 的方法 #setTokensFor 源码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">public static void setTokensFor(ContainerLaunchContext amContainer, List&lt;Path&gt; paths, Configuration conf) throws IOException &#123;</div><div class="line">Credentials credentials = new Credentials();</div><div class="line">// for HDFS</div><div class="line">TokenCache.obtainTokensForNamenodes(credentials, paths.toArray(new Path[0]), conf);</div><div class="line">// for HBase</div><div class="line">obtainTokenForHBase(credentials, conf);</div><div class="line">// for user</div><div class="line">UserGroupInformation currUsr = UserGroupInformation.getCurrentUser();</div><div class="line"></div><div class="line">Collection&lt;Token&lt;? extends TokenIdentifier&gt;&gt; usrTok = currUsr.getTokens();</div><div class="line">for (Token&lt;? extends TokenIdentifier&gt; token : usrTok) &#123;</div><div class="line">final Text id = new Text(token.getIdentifier());</div><div class="line">LOG.info(&quot;Adding user token &quot; + id + &quot; with &quot; + token);</div><div class="line">credentials.addToken(id, token);</div><div class="line">&#125;</div><div class="line">try (DataOutputBuffer dob = new DataOutputBuffer()) &#123;</div><div class="line">credentials.writeTokenStorageToStream(dob);</div><div class="line"></div><div class="line">if (LOG.isDebugEnabled()) &#123;</div><div class="line">LOG.debug(&quot;Wrote tokens. Credentials buffer length: &quot; + dob.getLength());</div><div class="line">&#125;</div><div class="line"></div><div class="line">ByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());</div><div class="line">amContainer.setTokens(securityTokens);</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="YarnClusterEntrypoint"><a href="#YarnClusterEntrypoint" class="headerlink" title="YarnClusterEntrypoint"></a>YarnClusterEntrypoint</h3><p>YarnClusterEntrypoint 是 Flink 的 AM 主类，它在启动时会先初始化 SecurityContext。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">protected void startCluster() &#123;</div><div class="line">LOG.info(&quot;Starting &#123;&#125;.&quot;, getClass().getSimpleName());</div><div class="line"></div><div class="line">try &#123;</div><div class="line">configureFileSystems(configuration);</div><div class="line"></div><div class="line">SecurityContext securityContext = installSecurityContext(configuration);</div><div class="line"></div><div class="line">securityContext.runSecured((Callable&lt;Void&gt;) () -&gt; &#123;</div><div class="line">runCluster(configuration);</div><div class="line"></div><div class="line">return null;</div><div class="line">&#125;);</div><div class="line">&#125; catch (Throwable t) &#123;</div><div class="line">LOG.error(&quot;Cluster initialization failed.&quot;, t);</div><div class="line"></div><div class="line">shutDownAndTerminate(</div><div class="line">STARTUP_FAILURE_RETURN_CODE,</div><div class="line">ApplicationStatus.FAILED,</div><div class="line">t.getMessage(),</div><div class="line">false);</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>而 #installSecurityContext 方法通过 SecurityUtils 逐个调用了安全模块的认证，其中最重要的一个就是 HadoopModule 。HadoopModule 的核心代码如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">String keytabPath = (new File(securityConfig.getKeytab())).getAbsolutePath();</div><div class="line"></div><div class="line">// 用 keytab 认证</div><div class="line">UserGroupInformation.loginUserFromKeytab(securityConfig.getPrincipal(), keytabPath);</div><div class="line"></div><div class="line">loginUser = UserGroupInformation.getLoginUser();</div><div class="line"></div><div class="line">// supplement with any available tokens</div><div class="line">String fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);</div><div class="line">if (fileLocation != null) &#123;</div><div class="line"></div><div class="line">try &#123;</div><div class="line">Method readTokenStorageFileMethod = Credentials.class.getMethod(&quot;readTokenStorageFile&quot;,</div><div class="line">File.class, org.apache.hadoop.conf.Configuration.class);</div><div class="line">Credentials cred =</div><div class="line">(Credentials) readTokenStorageFileMethod.invoke(</div><div class="line">null,</div><div class="line">new File(fileLocation),</div><div class="line">hadoopConfiguration);</div><div class="line"></div><div class="line">// 由于 Hadoop 认证机制更偏好 delegation token，使用 kerberos keytab 认证时需要过滤掉 delegation token</div><div class="line">// 以下代码从 container 的本地 token 文件读取 token ，过滤掉 HDFS_DELEGATION_TOKEN 后覆盖掉原本的 container credentail</div><div class="line">Method getAllTokensMethod = Credentials.class.getMethod(&quot;getAllTokens&quot;);</div><div class="line">Credentials credentials = new Credentials();</div><div class="line">final Text hdfsDelegationTokenKind = new Text(&quot;HDFS_DELEGATION_TOKEN&quot;);</div><div class="line">Collection&lt;Token&lt;? extends TokenIdentifier&gt;&gt; usrTok = (Collection&lt;Token&lt;? extends TokenIdentifier&gt;&gt;) getAllTokensMethod.invoke(cred);</div><div class="line">//If UGI use keytab for login, do not load HDFS delegation token.</div><div class="line">for (Token&lt;? extends TokenIdentifier&gt; token : usrTok) &#123;</div><div class="line">if (!token.getKind().equals(hdfsDelegationTokenKind)) &#123;</div><div class="line">final Text id = new Text(token.getIdentifier());</div><div class="line">credentials.addToken(id, token);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">Method addCredentialsMethod = UserGroupInformation.class.getMethod(&quot;addCredentials&quot;,</div><div class="line">Credentials.class);</div><div class="line">addCredentialsMethod.invoke(loginUser, credentials);</div><div class="line">&#125; catch (NoSuchMethodException e) &#123;</div><div class="line">LOG.warn(&quot;Could not find method implementations in the shaded jar. Exception: &#123;&#125;&quot;, e);</div><div class="line">&#125; catch (InvocationTargetException e) &#123;</div><div class="line">throw e.getTargetException();</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>此外 AM 还要负责申请 container 时设置好 container 的安全凭证，具体可见 Utils#createTaskExecutorContext，以下是核心代码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">// 从 AM container 的本地 token 文件读取 token 并设置给 taskmanager container </div><div class="line">// 其中这里有个问题是连同 AMRMToken 一齐传递给了 taskmanager，而 AMRMToken 顾名思义应该只有 AM 有权限使用，详见 [FLINK-11126](https://issues.apache.org/jira/browse/FLINK-11126)</div><div class="line">final String fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);</div><div class="line"></div><div class="line">if (fileLocation != null) &#123;</div><div class="line">log.debug(&quot;Adding security tokens to TaskExecutor&apos;s container launch context.&quot;);</div><div class="line"></div><div class="line">try (DataOutputBuffer dob = new DataOutputBuffer()) &#123;</div><div class="line">Method readTokenStorageFileMethod = Credentials.class.getMethod(</div><div class="line">&quot;readTokenStorageFile&quot;, File.class, org.apache.hadoop.conf.Configuration.class);</div><div class="line"></div><div class="line">Credentials cred =</div><div class="line">(Credentials) readTokenStorageFileMethod.invoke(</div><div class="line">null,</div><div class="line">new File(fileLocation),</div><div class="line">HadoopUtils.getHadoopConfiguration(flinkConfig));</div><div class="line"></div><div class="line">cred.writeTokenStorageToStream(dob);</div><div class="line">ByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());</div><div class="line">ctx.setTokens(securityTokens);</div><div class="line">&#125; catch (Throwable t) &#123;</div><div class="line">log.error(&quot;Failed to add Hadoop&apos;s security tokens.&quot;, t);</div><div class="line">&#125;</div><div class="line">&#125; else &#123;</div><div class="line">log.info(&quot;Could not set security tokens because Hadoop&apos;s token file location is unknown.&quot;);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="YarnTaskExecutorRunner"><a href="#YarnTaskExecutorRunner" class="headerlink" title="YarnTaskExecutorRunner"></a>YarnTaskExecutorRunner</h3><p>YarnTaskExecutorRunner 是 Flink TaskManager 的主类，它初始化时的认证过程和 YarnClusterEntrypoint 相似，都是调用各个 SecurityModule 来认证。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">String keytabPath = null;</div><div class="line">if (remoteKeytabPath != null) &#123;</div><div class="line">File f = new File(currDir, Utils.KEYTAB_FILE_NAME);</div><div class="line">keytabPath = f.getAbsolutePath();</div><div class="line">LOG.info(&quot;keytab path: &#123;&#125;&quot;, keytabPath);</div><div class="line">&#125;</div><div class="line"></div><div class="line">UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();</div><div class="line"></div><div class="line">LOG.info(&quot;YARN daemon is running as: &#123;&#125; Yarn client user obtainer: &#123;&#125;&quot;,</div><div class="line">currentUser.getShortUserName(), yarnClientUsername);</div><div class="line"></div><div class="line">if (keytabPath != null &amp;&amp; remoteKeytabPrincipal != null) &#123;</div><div class="line">configuration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);</div><div class="line">configuration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);</div><div class="line">&#125;</div><div class="line"></div><div class="line">SecurityConfiguration sc = new SecurityConfiguration(configuration);</div><div class="line"></div><div class="line">final String containerId = ENV.get(YarnFlinkResourceManager.ENV_FLINK_CONTAINER_ID);</div><div class="line">Preconditions.checkArgument(containerId != null,</div><div class="line">&quot;ContainerId variable %s not set&quot;, YarnFlinkResourceManager.ENV_FLINK_CONTAINER_ID);</div><div class="line"></div><div class="line">// use the hostname passed by job manager</div><div class="line">final String taskExecutorHostname = ENV.get(YarnResourceManager.ENV_FLINK_NODE_ID);</div><div class="line">if (taskExecutorHostname != null) &#123;</div><div class="line">configuration.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, taskExecutorHostname);</div><div class="line">&#125;</div><div class="line"></div><div class="line">SecurityUtils.install(sc);</div><div class="line"></div><div class="line">SecurityUtils.getInstalledContext().runSecured(new Callable&lt;Void&gt;() &#123;</div><div class="line">@Override</div><div class="line">public Void call() throws Exception &#123;</div><div class="line">TaskManagerRunner.runTaskManager(configuration, new ResourceID(containerId));</div><div class="line">return null;</div><div class="line">&#125;</div><div class="line">&#125;);</div></pre></td></tr></table></figure><h2 id="社区的-Flink-on-YARN-Security-改进提案"><a href="#社区的-Flink-on-YARN-Security-改进提案" class="headerlink" title="社区的 Flink on YARN Security 改进提案"></a>社区的 Flink on YARN Security 改进提案</h2><p>Uber 是 Apache Flink 的重度用户，而且由于技术栈和架构的关系 Uber 对于 Security 这块的积累比较多，最近（2108 年年底） Uber 的工程师向社区提出了 Security 的新改进方案<a href="https://docs.google.com/document/d/1rBLCpyQKg6Ld2P0DEgv4VIOMTwv4sitd7h7P5r202IE/edit#heading=h.vcblrwijpe4n" target="_blank" rel="external">[3]</a>。虽然这个改进方案目前还没有得到广泛的关注（毕竟没有很多人熟悉安全模块）并且部分列举的场景比较小众，但其中的一些设计还是很有参考意义。</p><p>这个提案包含两个部分: 1.支持多种 YARN Application 认证方式；2.支持超级用户伪装为普通用户（impersonation）。下面将分点讲述。</p><h3 id="支持多种-YARN-Application-认证方式"><a href="#支持多种-YARN-Application-认证方式" class="headerlink" title="支持多种 YARN Application 认证方式"></a>支持多种 YARN Application 认证方式</h3><p>该提案建议将 client 端、AM 端和普通 container 端的认证方法通过 CredentialFactory 来解耦，因此用户可以灵活定制三者的认证方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">/** </div><div class="line"> * Kerberos Credential Factory</div><div class="line"> */</div><div class="line">public interface CredentialFactory &#123;</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on client side. Setup environment properties which will be shipped</div><div class="line">   * to the cluster side.</div><div class="line">   */</div><div class="line">  Map&lt;String, String&gt; createEnvCredentialProperties(Configuration conf);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on client side. Setup environment credential by creating file cache or</div><div class="line">   * validate provided credentials are accessible on cluster side.</div><div class="line">   */</div><div class="line">  void prepareEnvCrediential(Configuration conf, FileSystem fs);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on application master. Derive &#123;@link SecurityConfiguration&#125;, used by</div><div class="line">   * &#123;@link SecureUtils&#125; and &#123;@link SecureModule&#125; to install credentials.</div><div class="line">   */</div><div class="line">  SecurityConfiguration prepareApplicationMasterCredentials(Map&lt;String, String&gt; envProperties);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Run on task executor. Derive &#123;@link SecurityConfiguration&#125;, used by</div><div class="line">   * &#123;@link SecureUtils&#125; and &#123;@link SecureModule&#125; to install credentials.</div><div class="line">   */</div><div class="line">  SecurityConfiguration prepareTaskExecutorCredentials(Map&lt;String, String&gt; envProperties);</div><div class="line"></div><div class="line">  /** </div><div class="line">   * Checker function to ensure proper credentials are configured within the </div><div class="line">   * current cluster environment.</div><div class="line">   */</div><div class="line">  boolean isCredentialEnvConfigured(Map&lt;String, String&gt; envProperties);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>默认情况下提供除了 client 生成 token 外的三种常见认证策略。</p><h3 id="支持超级用户-Impersonation"><a href="#支持超级用户-Impersonation" class="headerlink" title="支持超级用户 Impersonation"></a>支持超级用户 Impersonation</h3><p>目前来说其实 Flink 的 Impersonation 需要通过 client 操作系统来实现（主要是因为 keytab 的权限是 600），比如 root 用户 su 为 joe 用户再使用 joe 用户的名义提交作业，这种方法会造成潜在的权限滥用。该提案提出我们可以使用 Hadoop 的 proxy user API<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html" target="_blank" rel="external">[4]</a>来实现 Flink 层面的 Impersonation 以避免这个问题。</p><p>要实现超级用户 Impersonation 首先要设置一个用户，比如 flink，为 Hadoop 的超级用户，这样他就可以申请到其他用户的 delagtion token，因此 flink 用户不需要访问一般用户joe 的 keytab 就可以通过 delegation token 来提交作业。后续 Flink 需要使用上述 Application 认证方式的第三或者第四种来持续刷新 delegation tokem，所以提案第一点支持多种 YARN Application 认证方式也是第二点 Impersonation 的前置条件。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>YARN 原本不是为长时间运行的实时应用设计的，尤其是在安全认证这块尤为明显，因此各种实时计算框架的 on YARN 认证都是神仙过海各显神通，没有统一的方式。Flink on YARN Security 目前已经比较稳定但是仍然不够灵活，导致在其上构建平台的企业级用户多少有些束手束脚的感觉。Uber 的提案是目前看上去是比较合理的，期待 Uber 这个提案可以顺利实现并被社区接受。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="external">Hadoop 官方文档: YARN Security</a><br>2.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/security-kerberos.html" target="_blank" rel="external">Flink 官方文档: Kerberos Authentication Setup and Configuration</a><br>3.<a href="https://docs.google.com/document/d/1rBLCpyQKg6Ld2P0DEgv4VIOMTwv4sitd7h7P5r202IE/edit#heading=h.vcblrwijpe4n" target="_blank" rel="external">Flink Kerberos Improvement Design</a><br>4.<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html" target="_blank" rel="external">Hadoop 官方文档: Proxy User</a><br>5.<a href="https://docs.google.com/document/d/10V7LiNlUJKeKZ58mkR7oVv1t6BrC6TZi3FGf2Dm6-i8/edit#heading=h.jxspj25an0dn" target="_blank" rel="external">Flink Security Improvements</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇关于 YARN 系统 Security 的博客，我们解析了通过 YARN 提供的 Security API，Application 已经在 RM 注册并且可以顺利地申请到 container，但 YARN 对 container 后续的凭证刷新（reacquire）并不能作用到已经在运行的 Application 进程，因此对于长期运行的 Application 而言要开发者自己实现认证和后续凭证刷新的逻辑。本文将接着分析 Flink 如何在申请到的 container 启动 jobmanger 和 taskmanager 并完成认证，也就是 Flink Application 自身的认证，以及后续的凭证刷新方法，最后再讲述最近社区对于 Flink on YARN Security 改进提案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink on YARN Security 浅析 (YARN Part)</title>
    <link href="https://link3280.github.io/2018/12/23/Flink-on-YARN-Security-%E6%B5%85%E6%9E%90-YARN-Part/"/>
    <id>https://link3280.github.io/2018/12/23/Flink-on-YARN-Security-浅析-YARN-Part/</id>
    <published>2018-12-23T02:32:43.000Z</published>
    <updated>2019-04-14T13:03:37.015Z</updated>
    
    <content type="html"><![CDATA[<p>自我们将作业迁移到 Kerberized 的 YARN 新集群后，运行中的作业有一定几率出现出现认证失败，其中失败有 AMRMToken 过期导致 AM 无法与 RM 通信而被 kill 掉和 AM 申请的新 container 启动时报缺少 credential 两种表现。出现了这个问题后只能重启作业解决，这严重影响了平台的稳定性，因此笔者前后投入了接近两周的时间去研究这个问题，最后定位到问题在于 RM 刷新 viewfs token 时只会刷新第一个挂载的 FS 的 token。另一方面，笔者对于 YARN Security 和 Flink 与 YARN 在安全机制上的集成已经有了一定的积累，所以先记录并分享出来，以便后续有需要时检索（retrive）和与其他同学讨论交流。</p><a id="more"></a><p>下文将先解释 Hadoop delegation token 的作用和种类，然后介绍 YARN Application 是如何应用这些 token 的，最后再解析 Flink 如何使用 YARN 的 API 来完成安全认证。值得注意的是，YARN 框架的系统级认证和 Application 的用户级认证是有所不同的。前者是 YARN 如何代表用户完成系统级的工作（下载所需资源、以用户身份启动进程等），更侧重于 YARN，后者是用户应用自身的安全操作（以什么身份访问 Hive、Hbase 等)，更侧重于 Application（Flink）。本文将主要关注前者，而关于 Flink on YARN 应用安全机制的内容将留到下一篇博客。</p><h2 id="Hadoop-Delegation-Token"><a href="#Hadoop-Delegation-Token" class="headerlink" title="Hadoop Delegation Token"></a>Hadoop Delegation Token</h2><p>关于 Hadoop delegation token 的背景和实现，Cloudera 有一篇很棒博客<a href="https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/" target="_blank" rel="external">[1]</a>，这里简单总结下。</p><p>Delegatin token 是 Hadoop 生态圈广泛使用的一种认证机制，包括 HDFS、YARN、HBase 等都支持 delegation token。Delegation token 机制基于 Kerberos，类似于 Kerberos TGT （Ticket Granting Ticket）。区别在于 TGT 的 renewal 需要请求 KDC 完成，是个三方协议，而 delegation token 的 renewal 只涉及到用户和服务，是个两方协议，这样的好处是避免了集群上所有机器初始化作业或后续 renew TGT 对 KDC 造成的巨大请求量。另外由于 delegation token 是服务级别并且由相应的服务管理，delegation token 的泄漏只会造成对应服务在 token 有效期（3-7天）内的不安全，相比分发 keytab 或者 TGT 到机器上更加安全。</p><center><p><img src="/img/hadoop-authentication.png" alt="Hadoop 安全认证" title="Hadoop 安全认证"></p></center><p>Delegation token 的工作方式是（见图一）: 用户先通过 Kerberos 认证并向需要访问的服务（比如 NameNode、ResourceManager）申请 delegation token，在提交作业时将这些 token 一并附上，然后作业被分发到集群机器上后自动通过附带的 token 来认证。</p><p>在 Hadoop 内部 delegation token 有很多种，HDFS 使用的 token 主要是 HDFS delegation token，YARN 使用的 token 可参见董西成的一篇博客<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-security/" target="_blank" rel="external">[2]</a>（版本是较老的 Hadoop 2.0 ，但 2.x 系列变动不大）。下文会主要围绕开发 YARN application 涉及到的最基础的两个 token，即 HDFS delegation token 和 AMRMToken 展开。</p><h2 id="YARN-Application-的认证方式"><a href="#YARN-Application-的认证方式" class="headerlink" title="YARN Application 的认证方式"></a>YARN Application 的认证方式</h2><p>YARN Security 在官网<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="external">[3]</a>有详细的描述，这里结合我们的应用场景简单总结下。</p><h3 id="Application-首次运行的认证"><a href="#Application-首次运行的认证" class="headerlink" title="Application 首次运行的认证"></a>Application 首次运行的认证</h3><p>在提交作业时，YARN client 需要获取作业运行时用到的所有 delegation token 并设置到 ApplicationSubmissionContext 里面。其中的 token 可以分为两类，一是系统 token，包括 HDFS delegation token 和 AMRMToken，二是用户 token，包括访问 HBase 或其他服务的 token。两者的区别在于系统 token 是 YARN 内部使用并负责管理的，而用户 token 则是可选的。</p><p>HDFS delegation token 是 NameNode 赋予的访问 HDFS 的凭证。YARN Client 提交作业前需要先向 NameNode 申请 HDFS delegation token，将作业所需资源上传到 HDFS，然后将 token 设置到 ApplicationSubmissionContext 里。对于 YARN 来说 HDFS delegation token 主要用在两个地方: log aggregation 和 container localization。log aggregation 是指收集用户作业日志上传到 HDFS；container localization 是指在 container 启动前将用户指定的资源从 HDFS 上下载到本地。两者都发生在 container 启动前，但 log aggregation 的失败不会影响 container 的启动，而 localization 则是必须的。</p><p>AMRMToken 是 ApplicationMaster 与 ResourceManager 通信的凭证。YARN Client 提交作业后会收到 ResourceManager 生成的 AMRMToken，这个 token 会被设置到 ApplicationSubmissionContext 里面传递给 AM 用于后续的通信。此后每次 AM 和 RM 的心跳或者其他通信，AM 都要附上 AMRMToken。</p><h3 id="Application-后续运行的凭证刷新"><a href="#Application-后续运行的凭证刷新" class="headerlink" title="Application 后续运行的凭证刷新"></a>Application 后续运行的凭证刷新</h3><p>虽然首次运行没有问题，但随着作业运行时间变长 token 会失效，因此需要相应的 token 刷新机制来保证作业认证凭证的有效性。HDFS delegation token 和 AMRMToken 均属于系统 token，因此都由 YARN 来刷新。</p><p>对于 HDFS delegation token，YARN 在 [YARN-2704]<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-security/" target="_blank" rel="external">[2]</a> 引入了 proxy-user 的功能，即 YARN RM 可以代替用户刷新 HDFS delegation token。这里的刷新有两种方式，一是 renew，即在 HDFS delegation token 失效前向 RM 续订（默认 1 天），延长 token 的有效期；二是当 token 达到最大生命周期（默认 7 天）后不能再被续订，此时需要申请一个新的 token 来代替旧 token。</p><p>对于 AMRMToken，RM 会每天刷新一次，刷新的方式是更新用于生成 AMRMToken 的 master key。在准备启用新 master key 的一个窗口内（默认 15 分钟），当 RM 发现 AM 心跳使用的是旧 master key，就会返回新的 master key 给 AM 通知它更新。过了这个更新窗口，AM 再使用旧 master key 生成的 token 通信，就会得到 Invalid Token 的报错。</p><h2 id="Flink-on-YARN-作业提交时认证"><a href="#Flink-on-YARN-作业提交时认证" class="headerlink" title="Flink on YARN 作业提交时认证"></a>Flink on YARN 作业提交时认证</h2><center><p><img src="/img/flink-job-submission.png" alt="Flink on YARN 作业提交流程" title="Flink on YARN 作业提交流程"></p></center><p>Flink on YARN 的 Application 提交流程如下<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="external">[4]</a>:</p><ol><li>Flink client（同时也是 YARN client）会先通过 Kerberos 认证，向 Namenode 申请 HDFS delegation token 并上传作业依赖到 HDFS。  </li><li>Flink client 将相应的资源和 token 设置到 ApplicationSubmissionContext 里面，提交 Application。</li><li>YARN RM 收到提交作业请求后记录下 Application 对应的 token ，并分配一个 container 启动 ApplicationMaster，其中包括了 Flink JobManager。  </li><li>JobManager 启动后会将自己的 HDFS delegation token 和 TaskManager 启动的资源设置到 ContainerLaunchContext 里面，然后提交给 YARN RM。RM 收到请求后分配 container token 给 AM，AM 再凭借这个 token 向 NodeManager 要求并启动 container。</li></ol><p>其中每个 container （包括 AM container 和一般 container）启动前 RM 会将这个 Application 的 token 发送给 NodeManager，因此 NodeManager 能以用户的身份完成 container 启动的前置工作以及最终启动 container。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>YARN Security 的设计思想在于用户在 Client 端先通过 Kerberos 认证获取基本服务(YARN、HDFS)的 delegation token，并作为 Application Submission Context 的凭证提交给 RM。如果 Application 可以成功运行则证明了用户凭证的有效性，token 后续的刷新和重新获取都由 RM 来负责。不过新 token 仅限 YARN 内部使用，用户应用本身的 token 失效问题仍需要用户自己处理。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://blog.cloudera.com/blog/2017/12/hadoop-delegation-tokens-explained/" target="_blank" rel="external">Hadoop Delegation Tokens Explained</a><br>2.<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-security/" target="_blank" rel="external">Hadoop 2.0（YARN）中的安全机制概述</a><br>3.<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html" target="_blank" rel="external">Apache Hadoop 官方文档: YARN Security</a><br>4.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="external">Apache Flink 官方文档: on YARN 模式</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自我们将作业迁移到 Kerberized 的 YARN 新集群后，运行中的作业有一定几率出现出现认证失败，其中失败有 AMRMToken 过期导致 AM 无法与 RM 通信而被 kill 掉和 AM 申请的新 container 启动时报缺少 credential 两种表现。出现了这个问题后只能重启作业解决，这严重影响了平台的稳定性，因此笔者前后投入了接近两周的时间去研究这个问题，最后定位到问题在于 RM 刷新 viewfs token 时只会刷新第一个挂载的 FS 的 token。另一方面，笔者对于 YARN Security 和 Flink 与 YARN 在安全机制上的集成已经有了一定的积累，所以先记录并分享出来，以便后续有需要时检索（retrive）和与其他同学讨论交流。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="YARN" scheme="https://link3280.github.io/tags/YARN/"/>
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 1.7 Release 解读</title>
    <link href="https://link3280.github.io/2018/12/02/Flink-1-7-Release-%E8%A7%A3%E8%AF%BB/"/>
    <id>https://link3280.github.io/2018/12/02/Flink-1-7-Release-解读/</id>
    <published>2018-12-02T05:04:43.000Z</published>
    <updated>2019-04-14T13:03:47.216Z</updated>
    
    <content type="html"><![CDATA[<center><p><img src="/img/Flink-1.7-release-announcement-blog-1-768x591.png" alt="Flink 1.7 Release"></p></center><p>自 feature freezed 以后经过近一个月的努力，Flink 社区在十一月的最后一天终于发布 Flink 1.7.0 版本。该版本处理了 420 个 issue，其中新特性或者改进主要集中在 SQL 和 State 两个模块上，另外从 1.7 开始更多的 API （包括 REST、State 还有正在讨论的 runtime）会考虑版本兼容性，以便用户更重度地依赖 Flink 做上层的开发。</p><a id="more"></a><p>下文将选取一些笔者认为重要的新特性、improvement 和 bugfix 进行解读，详细的改动请参照 release notes<a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12343585" target="_blank" rel="external">1</a>。</p><h2 id="SQL-模块"><a href="#SQL-模块" class="headerlink" title="SQL 模块"></a>SQL 模块</h2><h3 id="CEP-与-SQL-的集成"><a href="#CEP-与-SQL-的集成" class="headerlink" title="CEP 与 SQL 的集成"></a>CEP 与 SQL 的集成</h3><p>CEP (Complex Event Processing) 是 Flink 常见的使用场景，其 DSL 语法与 SQL 类似但仍需要以编程 API 的方式调用。因此在 SQL 里支持 CEP 将大大降低使用门槛，对于用户来说仅需学习一个新的 SQL 函数。在 2016 年年底国际标准组织(IOS)发布了 <a href="https://standards.iso.org/ittf/PubliclyAvailableStandards/c065143_ISO_IEC_TR_19075-5_2016.zip" target="_blank" rel="external">Row Pattern Recognition in SQL [3]</a>，这为在 SQL 中实现 CEP 奠定了基础。 </p><p>Flink CEP 的 SQL 支持是基于 Apache Calcite 的新函数 <code>MATCH_RECOGNIZE</code> 实现的，具体语法见下面的案例。场景是检测某个商品最近一次连续降价的开始时间和结束时间，给定输入为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// 输入</div><div class="line">symbol         rowtime         price    tax</div><div class="line">======  ====================  ======= =======</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:00&apos;   12      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:01&apos;   17      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:02&apos;   19      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:03&apos;   21      3</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:04&apos;   25      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:05&apos;   18      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:06&apos;   15      1</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:07&apos;   14      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:08&apos;   24      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:09&apos;   25      2</div><div class="line">&apos;ACME&apos;  &apos;01-Apr-11 10:00:10&apos;   19      1</div></pre></td></tr></table></figure><p>CEP SQL 语句如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">SELECT *</div><div class="line">FROM Ticker</div><div class="line">MATCH_RECOGNIZE (</div><div class="line">    PARTITION BY symbol</div><div class="line">    ORDER BY rowtime</div><div class="line">    MEASURES</div><div class="line">        START_ROW.rowtime AS start_tstamp,</div><div class="line">        LAST(PRICE_DOWN.rowtime) AS bottom_tstamp,</div><div class="line">        LAST(PRICE_UP.rowtime) AS end_tstamp</div><div class="line">    ONE ROW PER MATCH</div><div class="line">    AFTER MATCH SKIP TO LAST PRICE_UP</div><div class="line">    PATTERN (START_ROW PRICE_DOWN+ PRICE_UP)</div><div class="line">    DEFINE</div><div class="line">        PRICE_DOWN AS</div><div class="line">            (LAST(PRICE_DOWN.price, 1) IS NULL AND PRICE_DOWN.price &lt; START_ROW.price) OR</div><div class="line">                PRICE_DOWN.price &lt; LAST(PRICE_DOWN.price, 1),</div><div class="line">        PRICE_UP AS</div><div class="line">            PRICE_UP.price &gt; LAST(PRICE_DOWN.price, 1)</div><div class="line">    ) MR;</div></pre></td></tr></table></figure><p>SQL 语句具体的执行顺序如下: </p><ol><li>依据 <code>PARTITION BY</code> 和 <code>ORDER BY</code> 语句将 <code>MATCH_RECOGNIZE</code> 语句的输入数据按 Key 逻辑切分和排序。</li><li>用 <code>PATTERN</code> 语句定义模式序列，语法与正则表达式一致。</li><li>根据 <code>DEFINE</code> 语句的条件将一行数据映射为模式变量，供后续使用。</li><li>根据 <code>MEASURES</code> 语句将模式变量进行表达式计算转换为最终数据结果。</li></ol><p>上述案例的对应输出如下，分别是(商品标识, 降价开始时间, 最低价时间, 价格回升时间):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// 输出</div><div class="line"> symbol       start_tstamp       bottom_tstamp         end_tstamp</div><div class="line">=========  ==================  ==================  ==================</div><div class="line">ACME       01-APR-11 10:00:04  01-APR-11 10:00:07  01-APR-11 10:00:08</div></pre></td></tr></table></figure><h3 id="Streaming-SQL-支持-Temporal-Tables-和-Temporal-Joins"><a href="#Streaming-SQL-支持-Temporal-Tables-和-Temporal-Joins" class="headerlink" title="Streaming SQL 支持 Temporal Tables 和 Temporal Joins"></a>Streaming SQL 支持 Temporal Tables 和 Temporal Joins</h3><p>Temporal Table 是 Flink 1.7 版本引入的新特性，简单来说它是有时间版本的 Streaming Table，可以用于基于时间版本的 Join（Temporal Joins）。</p><p>在之前的版本，Streaming SQL 中任何对 Table 的更新都会触发 Join 结果的更新，这意味着我们无法反映历史版本数据。举个例子，我们需要将订单中的价格按实时汇率转换为本地货币，其中订单数据来自是一个只会追加（append-only）的交易订单数据流，货币汇率是依据实时更新事件的数据流。此前如果直接将两个数据流 join 起来，当汇率进行变化时，之前订单的本地货币金额也会随之更新，这显然是不符合需求的。</p><p>为了实现支持时间版本的 Join，我们需要保存其中一个（维度）表的全部时间版本，然后根据事件数据流的时间决定具体使用哪个一个版本的数值。这体现在 SQL 上新增了一个 UDF API 来定义 Temporal Table，这需要配合 <code>LATERAL TABLE</code> 使用。</p><p>首先需要在 Table API 注册目标表并调用 API 创建针对该表的 UDF:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">// 创建汇率历史表</div><div class="line">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; ratesHistoryStream = env.fromCollection(ratesHistoryData);</div><div class="line">Table ratesHistory = tEnv.fromDataStream(ratesHistoryStream, &quot;r_currency, r_rate, r_proctime.proctime&quot;);</div><div class="line"></div><div class="line">tEnv.registerTable(&quot;RatesHistory&quot;, ratesHistory);</div><div class="line"></div><div class="line">// 创建并注册该表的 temporal table function</div><div class="line">// 将 `r_proctime` 作为时间字段，`r_currency` 作为主键</div><div class="line">TemporalTableFunction rates = ratesHistory.createTemporalTableFunction(&quot;r_proctime&quot;, &quot;r_currency&quot;); // &lt;==== (1)</div><div class="line">tEnv.registerFunction(&quot;Rates&quot;, rates);</div></pre></td></tr></table></figure><p>然后可以在 SQL 里使用 <code>Rates</code> UDF：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">SELECT</div><div class="line">  SUM(o.amount * r.rate) AS amount</div><div class="line">FROM Orders AS o</div><div class="line">LATERAL JOIN (TABLE Rates(o.rowtime)) AS r</div><div class="line">ON r.currency = o.currency;</div></pre></td></tr></table></figure><h3 id="新增-SQL-内置函数"><a href="#新增-SQL-内置函数" class="headerlink" title="新增 SQL 内置函数"></a>新增 SQL 内置函数</h3><table><thead><tr><th>函数名</th><th>描述</th></tr></thead><tbody><tr><td>TO_BASE64(string)</td><td>Returns the base64-encoded result from string; returns NULL if string is NULL.</td></tr><tr><td>LOG2(numeric)</td><td>Returns the base 2 logarithm of numeric.</td></tr><tr><td>LTRIM(string)</td><td>Returns a string that removes the left whitespaces from string.</td></tr><tr><td>RTRIM(string)</td><td>Returns a string that removes the right whitespaces from string.</td></tr><tr><td>REPEAT(string, integer)</td><td>Returns a string that repeats the base string integer times.</td></tr><tr><td>REPLACE(string1, string2, string3)</td><td>Returns a new string which replaces all the occurrences of string2 with string3 (non-overlapping) from string1.</td></tr><tr><td>COSH(numeric)</td><td>Returns the hyperbolic cosine of NUMERIC.</td></tr><tr><td>SINH(numeric)</td><td>Returns the hyperbolic sine of numeric.</td></tr><tr><td>TANH(numeric)</td><td>Returns the hyperbolic tangent of numeric.</td></tr></tbody></table><h3 id="SQL-Client-优化"><a href="#SQL-Client-优化" class="headerlink" title="SQL Client 优化"></a>SQL Client 优化</h3><p>SQL Client 是 Flink 1.5.0 引入的实验性特性，目的是为用户提供执行探索性查询的 SQL shell，而在 Flink 1.7.0 版本中 SQL Client 得到进一步的改进。Flink SQL 一直缺乏 DDL 的实现（目前正在开发中），定义表是通过启动 SQL Client 时指定的配置文件来完成，因此创建数据源一直是个比较繁琐的操作，尤其是在探索性的场景中，用户一般需要经常变换数据源或者表格式。不过现在虽然仍不支持在 SQL Client Session 中创建表，但可以用视图(View)来缓解这个问题。SQL Client 可以在配置文件或者 Session 中创建虚拟视图。视图会立即被解析和校验，但是”懒执行”的，即知道被调用输出结果的时候才会真正被计算。</p><h2 id="State-模块"><a href="#State-模块" class="headerlink" title="State 模块"></a>State 模块</h2><h3 id="State-Evolution"><a href="#State-Evolution" class="headerlink" title="State Evolution"></a>State Evolution</h3><p>对于 Stateful 作业来说 State 是极为重要的信息，用户一般会期望可以在程序升级、State Schema 变化的情况下，原有的 State 仍可以向后兼容。原先的 State Schema 变化的兼容性需要依赖序列化器本身提供，换句话说我们是不能直接将 State 迁移到一个不兼容的序列号器上的，而是需要经过以下的步骤:</p><ol><li>用旧反序列化器读取 State。</li><li>实现一个“迁移 map 函数”将每个 State 对象转换为新序列化器格式。</li><li>用新序列化器将 State 写回外部存储。</li></ol><p>State Evolution 的目的在于封装整个流程，只暴露简单接口给用户。因为步骤 2 根据不同的序列号器/反序列化器不同而不同，所以目前限制了 State Evolution 只对方便灵活改变 Schema 的 Avro 类型的 State 有效，不过之后会拓展到更多的 State 类型。</p><h3 id="Local-Recovery"><a href="#Local-Recovery" class="headerlink" title="Local Recovery"></a>Local Recovery</h3><p>Flink 1.7.0 加入了 State 的 Local Recovery 特性，即每次 checkpoint 时 TaskManager 都会本地保存一份本地的 State，当作业重启恢复时 Flink 调度器会优先考虑将 Task State 的本地性，这样可以减少网络流量提高恢复的效率，对于并行度较高的作业来说是十分重要的。</p><p>值得注意的是在 1.7.0 版本 Flink 的 failover strategy 因为重大的 bug 被暂时移除了<a href="RestartPipelinedRegionStrategy does not restore state">[6]</a>。因此基本所有的 Flink 都是全局恢复模式，一个 task 的失败会导致全部 task 的重新调度，Local Recovery 在这种场景下可以大大减少恢复时长和减缓网络压力。</p><h3 id="修复-Cancel-with-Savepoint-连接过早关闭的问题"><a href="#修复-Cancel-with-Savepoint-连接过早关闭的问题" class="headerlink" title="修复 Cancel with Savepoint 连接过早关闭的问题"></a>修复 Cancel with Savepoint 连接过早关闭的问题</h3><p>这是由于在 per-job cluster 模式下，cancel with savepoint 会导致 cluster 在取消作业后马上关闭，但这是 Flink Client 可能还处在轮询 savepoint 路径的过程中。这会导致 <code>java.net.ConnectException</code>，并且 Client 端无法得到 savepoint 的路径。这也是我们在做 Flink 作业平台时遇到的一个问题，并在内部分支的 1.5.3 版本和 1.6.0 版本用 <code>closeTimeWait</code> 的方式来简单修复。Flink 1.7 的实现会更加优雅: 用 CompletableFuture 确保 REST Server 在处理完所有的 pending 请求后再关闭，以保证 Client 在 Cluster 关闭前可以轮询到 Savepoint 路径。详情可见 [FLINK-10309]<a href="https://issues.apache.org/jira/browse/FLINK-10309" target="_blank" rel="external">[8]</a></p><h2 id="Connector-模块"><a href="#Connector-模块" class="headerlink" title="Connector 模块"></a>Connector 模块</h2><h3 id="Exactly-once-S3-StreamingFileSink"><a href="#Exactly-once-S3-StreamingFileSink" class="headerlink" title="Exactly-once S3 StreamingFileSink"></a>Exactly-once S3 StreamingFileSink</h3><p>自 Flink 1.6 引入的 StreamingFileSink 现在新增了 S3 的 exactly-once 支持。此前用户需要使用 BucketingFileSink 来写 S3，而 BucketingFileSink 并不支持 Flink FileSystem，因此许多 Flink 层面的支持都没有办法使用，而现在用户可以安全地迁移到 StreamingFileSink 上。</p><h3 id="Kafka-2-0-Connector"><a href="#Kafka-2-0-Connector" class="headerlink" title="Kafka 2.0 Connector"></a>Kafka 2.0 Connector</h3><p>Flink 1.7 将抛弃掉针对每个版本 Kafka 定制 connector 的模式，转而提供新的 Kafka connector (modern Kafka connector) 来支持 1.0+ 版本的 Kafka <a href="https://blog.csdn.net/yanghua_kobe/article/details/83210897" target="_blank" rel="external">[7]</a>。新 connector 由腾讯的工程师团队贡献，对不同版本的 Kafka Client 实现了一个统一的 Facade，可以自动适配不同版本的 Kafka broker。不过由于兼容性问题，1.0 以下版本的 Kafka client 并不在支持列表内，而对应的 connector 也将一直保留。</p><h2 id="Scala-2-12-支持"><a href="#Scala-2-12-支持" class="headerlink" title="Scala 2.12 支持"></a>Scala 2.12 支持</h2><p>Flink 1.7 全面支持了 Scala 2.12，但由于 Scala 2.12 与 Scala 2.11 的语法有不兼容，所以部分 Public API 的一致性会被破坏，迁移到新版本的 Scala 用户要注意。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12343585" target="_blank" rel="external">Flink 1.7.0 Release Notes</a></li><li><a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-20%3A+Integration+of+SQL+and+CEP" target="_blank" rel="external">FLIP-20: Integration of SQL and CEP</a></li><li><a href="https://standards.iso.org/ittf/PubliclyAvailableStandards/c065143_ISO_IEC_TR_19075-5_2016.zip" target="_blank" rel="external">Row Pattern Recognition in SQL</a></li><li><a href="https://docs.google.com/document/d/1KaAkPZjWFeu-ffrC9FhYuxE6CIKsatHTTxyrxSBR8Sk/edit#heading=h.96en6mr8aklf" target="_blank" rel="external">Enrichment joins with Table Version Functions in Flink</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/stream/state/schema_evolution.html" target="_blank" rel="external">State Schema Evolution</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10712" target="_blank" rel="external">FLINK-10712 RestartPipelinedRegionStrategy does not restore state</a></li><li><a href="https://blog.csdn.net/yanghua_kobe/article/details/83210897" target="_blank" rel="external">Flink即将在1.7版本发布全新的Kafka连接器</a></li><li><a href="https://issues.apache.org/jira/browse/FLINK-10309" target="_blank" rel="external">FLINK-10309 Cancel with savepoint fails with java.net.ConnectException when using the per job-mode</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;p&gt;&lt;img src=&quot;/img/Flink-1.7-release-announcement-blog-1-768x591.png&quot; alt=&quot;Flink 1.7 Release&quot;&gt;&lt;/p&gt;&lt;/center&gt;

&lt;p&gt;自 feature freezed 以后经过近一个月的努力，Flink 社区在十一月的最后一天终于发布 Flink 1.7.0 版本。该版本处理了 420 个 issue，其中新特性或者改进主要集中在 SQL 和 State 两个模块上，另外从 1.7 开始更多的 API （包括 REST、State 还有正在讨论的 runtime）会考虑版本兼容性，以便用户更重度地依赖 Flink 做上层的开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink Event Time 倾斜</title>
    <link href="https://link3280.github.io/2018/10/28/Flink-Event-Time-%E5%80%BE%E6%96%9C/"/>
    <id>https://link3280.github.io/2018/10/28/Flink-Event-Time-倾斜/</id>
    <published>2018-10-28T14:27:02.000Z</published>
    <updated>2019-04-14T13:04:55.431Z</updated>
    
    <content type="html"><![CDATA[<p>数据倾斜是分布式领域最为常见及棘手的问题之一。通常来说分布式系统会以数据的 key 来对数据进行 partition (分区)，不同 key 的计算和状态都是独立，因此可以用分治的方式并行处理。当发生数据倾斜时，数据会集中到少部分的 key 上，导致这些 key 对应的 subtask 负载比其他 subtask 高得多。而在使用 event time 时间特性和 watermak 机制的实时计算系统里面，数据倾斜有了新的表现形式，即 event time 倾斜（event time skew），不同数据流的 event time 存在差异。</p><a id="more"></a><h2 id="并行数据流中的-watermark"><a href="#并行数据流中的-watermark" class="headerlink" title="并行数据流中的 watermark"></a>并行数据流中的 watermark</h2><p>首先来简单回顾下 Flink 的 watermark 机制，如果读者已经比较熟悉可以跳过这一节。在存在多并行度 Source 的 Flink 作业中，每个 Soure 实例（准确来说应该是 watermark assigner）会独立产生 watermark。watermark 会以广播的形式传播到下游，下游算子的每个实例收到后更新自己的 low watermark，并将自己的 low watermark 作为新的 watermark 广播给下游。如果一个算子实例有多个输入数据流，它的 low watermark 会以最小的一个为准。</p><center><img src="/img/watermark-in-paralle-stream.png" alt="并行视图的watermak" title="并行视图的watermak"></center><p>如图 1 所示，算子实例右上角的黄色框数字表示算子实例的 low watermark，数据管道末端的黄色框数字表示该数据管道的 low watermark，数据管道中的白色框表示 (id|timestamp) 形式的数据元素，数据管道中的虚线段表示 watermark 元素。在 map 算子后面接着一个 keyBy 操作，因此下游的 window 算子的实例会接受上游多个输入数据流。</p><p>可以看到 Source(1) 的 watermark 提升得比较快已经达到 33（到达 window 算子的为29），但受限于 Source(2) 的 watermark 还在 17（到达 window 算子的为14），最下游 window 实例的 low watermark 均为 14。顺带一提，window 算子实例的 low watermark 总是相同的，因为如果是 keyed window 每个实例的 low watermark 均取上游算子实例 low watermark 最小的一个；如果是 all window 则该算子只有一个实例。</p><h2 id="Event-Time-倾斜"><a href="#Event-Time-倾斜" class="headerlink" title="Event Time 倾斜"></a>Event Time 倾斜</h2><p>在读取历史数据时，比如从某个 Kafka topic 的最早 offset 开始读，各个 partition 的event time 很可能是不同的。如果有一个 partition 的 event time 明显比其他 partition 慢，那么下游所以算子的 low watermark 都会被其拖慢，而其他 partition 的新数据又会一直读入，这对于部分依赖 event time 进行计算的算子来说会造成不必要的数据缓存。</p><p>比如对于使用了低级 API ProcessingFunction 的 event time window 算子，它的触发条件是 low watermark 达到某个值，在此之前它会缓存该窗口内的所有数据。在 event time 倾斜的情况下，它一直得不到触发，但又要一直摄入新数据导致 State 不断增长。这会给内存和 checkpoint 机制带来很大的压力，使得作业的可能最终因 OOM 崩溃并且重启恢复需要的时间也异常地长。</p><p>比起传统的数据倾斜，event time 数据倾斜更加难以预料和处理。从原因来说，数据倾斜是逻辑上的分布不均匀，常常是由于业务的流量分布造成的，比如微博大 V 用户和普通用户的访问流量肯定不在一个等级，这种分布是比较容易提前预计的和稳定的；而 event time 数据倾斜通常和数据的物理存储相关，更加底层，因此原因也更动态和多样。比如 Kafka topic 扩容时新增的 partiton，从 earliest 消费得到的 event time 可能是更新的。再比如在一个公用的 topic 下，以游戏代号作为 key 写入 partition，如果某个游戏搞活动，流量在一小段时间内增加，那么这段时间这个 partition 的 event time 就比其他 partition 要慢。从处理的办法来说，数据倾斜主要是在业务层，用户可以在计算时再 re-partition 处理，而 event time 倾斜发生在数据摄入阶段，用户的控制权比较少，更多是需要计算框架层面的支持。</p><h2 id="应对方案"><a href="#应对方案" class="headerlink" title="应对方案"></a>应对方案</h2><p>Flink 社区对于 event time 倾斜问题的重要性已经达成共识，在应对方案上也有了基础的思路。最基本的思路就是，既然拉取更多的其他 partition 的数据并不能提升下游的 low watermark，为什么不减缓或者阻塞他们的消费呢？最好的情况下，Source 实例的 watermark 可以像 checkpoint 一样校准。</p><p>围绕这个思路主要有两种实现方式:</p><ol><li><p>从下游算子入手，利用反压的机制来抑制对 watermark 前进过快的数据管道的摄入，比方说下游读取数据时对 watermark 更低的输入流给予更高的优先级。实际上这也是 Kafka Stream 的处理方式（见<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-353%3A+Improve+Kafka+Streams+Timestamp+Synchronization" target="_blank" rel="external">KIP-353</a>）。这种方式比较嵌合 Flink 现有的机制，但也存在两个关键的问题。首先，减缓读取部分输入流的同时，会使得输入流中的 barrier 也被延缓，导致 checkpoint 校准产生一定的数据缓存。其次，这种方式只是尽力而为，并不保证反压到 Source 端的效果，有可能反压之后 Source 实例间的 watermark 依然有较大的差距。</p></li><li><p>直接增加一个协调者来为 Source 的 watermark 进行校准，比如 SourceCoodinator。SourceCoodinator 可以是 JobManager 的一部分，就像 CheckpointCoordinator 一样。每个 Source 实例需要定期向 SourceCoodinator 报告目前的 watermark，并接受 SourceCoodinator 的返回决定是否需要继续拉取数据。这种方式并不会影响 checkpoint 机制，准确来说应该说是建立了 checkpoint 并行的机制，不过需要对目前的 Source API 进行一定的重构。</p></li></ol><p>从反馈意见来说，社区大多数是偏向用第二种方式来实现，因为第一种方式的耦合性和复杂度太高，而且对于 stateful streaming 来说任何影响到准确性的改动都应该十分谨慎。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Event time 数据倾斜是基于 event time 和 watermark 机制的实时计算面临的新问题，主要表现不同 Source 实例的 watermark 不同，导致下游基于 event time 的算子需要不断缓存 watermark 较高的实例的数据。这个问题发生在数据摄入阶段，因此需要从计算框架层面提供支持。Kafka Stream 的处理办法是通过优先消费时间戳更小的输入流来平衡不同 partition 的 event time，但对于 Flink 来说会影响到 checkpoint 机制，所以 Flink 社区目前更希望通过引入 Source 实例的协调机制来解决。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Sharing-state-between-subtasks-td24489.html" target="_blank" rel="external">Flink mail list: Sharing state between subtasks</a></li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/event_time.html#watermarks-in-parallel-streams" target="_blank" rel="external">Watermarks in Parallel Streams</a></li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-353%3A+Improve+Kafka+Streams+Timestamp+Synchronization" target="_blank" rel="external">KIP-353: Improve Kafka Streams Timestamp Synchronization</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据倾斜是分布式领域最为常见及棘手的问题之一。通常来说分布式系统会以数据的 key 来对数据进行 partition (分区)，不同 key 的计算和状态都是独立，因此可以用分治的方式并行处理。当发生数据倾斜时，数据会集中到少部分的 key 上，导致这些 key 对应的 subtask 负载比其他 subtask 高得多。而在使用 event time 时间特性和 watermak 机制的实时计算系统里面，数据倾斜有了新的表现形式，即 event time 倾斜（event time skew），不同数据流的 event time 存在差异。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink Exactly-Once 投递实现浅析</title>
    <link href="https://link3280.github.io/2018/10/16/Flink-Exactly-Once-%E6%8A%95%E9%80%92%E5%AE%9E%E7%8E%B0%E6%B5%85%E6%9E%90/"/>
    <id>https://link3280.github.io/2018/10/16/Flink-Exactly-Once-投递实现浅析/</id>
    <published>2018-10-16T13:49:53.000Z</published>
    <updated>2019-04-14T13:03:08.742Z</updated>
    
    <content type="html"><![CDATA[<p>随着近来越来越多的业务迁移到 Flink 上，对 Flink 作业的准确性要求也随之进一步提高，其中最为关键的是如何在不同业务场景下保证 exactly-once 的投递语义。虽然不少实时系统（e.g. 实时计算/消息队列）都宣称支持 exactly-once，exactly-once 投递似乎是一个已被解决的问题，但是其实它们更多是针对内部模块之间的信息投递，比如 Kafka 生产（producer 到 Kafka broker）和消费（broker 到 consumer）的 exactly-once。而 Flink 作为实时计算引擎，在实际场景业务会涉及到很多不同组件，由于组件特性和定位的不同，Flink 并不是对所有组件都支持 exactly-once（见[1]），而且不同组件实现 exactly-once 的方法也有所差异，有些实现或许会带来副作用或者用法上的局限性，因此深入了解 Flink exactly-once 的实现机制对于设计稳定可靠的架构有十分重要的意义。</p><a id="more"></a><p>下文将基于 Flink 详细分析 exactly-once 的难点所在以及实现方案，而这些结论也可以推广到其他实时系统，特别是流式计算系统。</p><h1 id="Exactly-Once-难点分析"><a href="#Exactly-Once-难点分析" class="headerlink" title="Exactly-Once 难点分析"></a>Exactly-Once 难点分析</h1><p>由于在分布式系统的进程间协调需要通过网络，而网络情况在很多情况下是不可预知的，通常发送消息要考虑三种情况：正常返回、错误返回和超时，其中错误返回又可以分为可重试错误返回（e.g. 数据库维护暂时不可用）和不可重试错误返回（e.g. 认证错误），而可重试错误返回和超时都会导致重发消息，导致下游可能接收到重复的消息，也就是 at-least-once 的投递语义。而 exactly-once 是在 at-least-once 的基础之上加上了可以识别出重发数据或者将消息包装为为幂等操作的机制。</p><p>其实消息的 exactly-once 投递并不是一个分布式系统产生的新课题（虽然它一般特指分布式领域的 exactly-once），早在计算网络发展初期的 TCP 协议已经实现了网络的可靠传输。TCP 协议的 exactly-once 实现方式是将消息传递变为有状态的：首先同步建立连接，然后发送的每个数据包加上递增的序列号（sequence number），发送完毕后再同步释放连接。由于发送端和接受端都保存了状态信息（已发送数据包的序列号/已接收数据包的序列号），它们可以知道哪些数据包是缺失或重复的。</p><p>而在分布式环境下 exactly-once 则更为复杂，最大的不同点在于分布式系统需要容忍进程崩溃和节点丢失，这会带来许多问题，比如下面常见的几个：</p><ol><li>进程状态需要持续化到可靠的分布式存储，以防止节点丢失带来状态的丢失。</li><li>由于发送消息是一个两阶段的操作（即发送消息和收到对方的确认），重启之后的进程没有办法判断崩溃前是否已经使用当前序列号发送过消息，因此可能会导致重复使用序列号的问题。</li><li>被认为崩溃的进程有可能并没有退出，随后再次连上来变为 zombie 进程继续发送数据。</li></ol><p>第2点和第3点其实是同一个问题，即需要区分出原本进程和重启后的进程。对此业界已经有比较成熟的解决方案: 引入 epoch 表示进程的不同世代并用分布式协调系统来负责管理。虽然还有一些衍生的细节问题，但总体来说问题都不大。但是第1点问题造成了一个比较深远的影响，即为了减低 IO 成本，状态的保存必然是微批量（micro-batching）的而不是流式的，这会导致状态的保存总是落后于流计算进度，因而为了保证 exactly-once 流计算引擎需要实现事务回滚。</p><h1 id="状态-Exactly-Once-和端到端-Exactly-Once"><a href="#状态-Exactly-Once-和端到端-Exactly-Once" class="headerlink" title="状态 Exactly-Once 和端到端 Exactly-Once"></a>状态 Exactly-Once 和端到端 Exactly-Once</h1><p>Flink 提供 exactly-once 的状态（state）投递语义，这为有状态的（stateful）计算提供了准确性保证。其中比较容易令人混淆的一点是状态投递语义和更加常见的端到端（end to end）投递语义，而实现前者是实现后者的前置条件。</p><p>Flink 从 0.9 版本开始提供 State API，标志着 Flink 进入了 Stateful Streaming 的时代。State API 简单来说是“不受进程重启影响的“数据结构，其命名规范也与常见的数据结构一致，比如 MapState、ListState。Flink 官方提供的算子（比如 KafkaSource）和用户开发的算子都可以使用  State API 来保存状态信息。和大多数分布式系统一样 Flink 采用快照的方式来将整个作业的状态定期同步到外部存储，也就是将 State API 保存的信息以序列化的形式存储，作业恢复的时候只要读取外部存储即可将作业恢复到先前某个时间点的状态。由于从快照恢复同时会回滚数据流的处理进度，所以 State 是天然的 exactly-once 投递。</p><p>而端到端的一致性则需要上下游的外部系统配合，因为 Flink 无法将它们的状态也保存到快照并独立地回滚它们，否则就不叫作外部系统了。通常来说 Flink 的上游是可以重复读取或者消费的 pull-based 持续化存储，所以要实现 source 端的 exactly-once 只需要回滚 source 的读取进度即可（e.g. Kafka 的 offset）。而 sink 端的 exactly-once 则比较复杂，因为 sink 是 push-based 的。所谓覆水难收，要撤回发出去的消息是并不是容易的事情，因为这要求下游根据消息作出的一系列反应都是可撤回的。这就需要用 State API 来保存已发出消息的元数据，记录哪些数据是重启后需要回滚的。</p><p>下面将分析 Flink 是如何实现 exactly-once Sink 的。</p><h1 id="Exactly-Once-Sink-原理"><a href="#Exactly-Once-Sink-原理" class="headerlink" title="Exactly-Once Sink 原理"></a>Exactly-Once Sink 原理</h1><p>Flink 的 exactly-once sink 均基于快照机制，按照实现原理可以分为幂等（Idempotent） sink 和事务性（Transactional） sink 两种。</p><h2 id="幂等-Sink"><a href="#幂等-Sink" class="headerlink" title="幂等 Sink"></a>幂等 Sink</h2><p>幂等性是分布式领域里十分有用的特性，它意味着相同的操作执行一次和执行多次可以获得相同的结果，因此 at-least-once 自然等同于 exactly-once。如此一来，在从快照恢复的时候幂等 sink 便不需要对外部系统撤回已发消息，相当于回避了外部系统的状态回滚问题。比如写入 KV 数据库的 sink，由于插入一行的操作是幂等的，因此 sink 可以无状态的，在错误恢复时也不需要关心外部系统的状态。从某种意义来讲，上文提到的 TCP 协议也是利用了发送数据包幂等性来保证 exactly-once。</p><p>然而幂等 sink 的适用场景依赖于业务逻辑，如果下游业务本来就无法保证幂等性，这时就需要应用事务性 sink。</p><h2 id="事务性-Sink"><a href="#事务性-Sink" class="headerlink" title="事务性 Sink"></a>事务性 Sink</h2><p>事务性 sink 顾名思义类似于传统 DBMS 的事务，将一系列（一般是一个 checkpoint 内）的所有输出包装为一个逻辑单元，理想的情况下提供 ACID 的事务保证。之所以说是“理想的情况下”，主要是因为 sink 依赖于目标输出系统的事务保证，而分布式系统对于事务的支持并不一定很完整，比如 HBase 就不支持跨行事务，再比如 HDFS 等文件系统是不提供事务的，这种情况下 sink 只可以在客户端的基础上再包装一层来尽最大努力地提供事务保证。</p><p>然而仅有下游系统本身提供的事务保证对于 exactly-once sink 来说是不够的，因为同一个 sink 的子任务（subtask）会有多个，对于下游系统来说它们是处在不同会话和事务中的，并不能保证操作的原子性，因此 exactly-once sink 还需要实现分布式事务来达到所有 subtask 的一致 commit 或 rollback。由于 sink 事务生命周期是与 checkpoint 一一对应的，或者说 checkpoint 本来就是实现作业状态持久化的分布式事务，sink 的分布式事务也理所当然可以通过 checkpoint 机制提供的 hook 来实现。</p><p>Checkpoint 提供给算子的 hook 有 CheckpointedFunction 和 CheckpointListener 两个，前者在算子进行 checkpoint 快照时被调用，后者在 checkpoint 成功后调用。为了简单起见 Flink 结合上述两个接口抽象出 exactly-once sink 的通用逻辑抽象 <code>TwoPhaseCommitSinkFunction</code> 接口，从命名即可看出这是对两阶段提交协议的一个实现，其主要方法如下:</p><ul><li>beginTransaction: 初始化一个事务。在有新数据到达并且当前事务为空时调用。</li><li>preCommit: 预提交数据，即不再写入当前事务并准好提交当前事务。在 sink 算子进行快照的时候调用。</li><li>commit: 正式提交数据，将准备好的事务提交。在作业的 checkpoint 完成时调用。</li><li>abort: 放弃事务。在作业 checkpoint 失败的时候调用。</li></ul><p>下面以 Bucketing File Sink 作为例子来说明如何基于异步 checkpoint 来实现事务性 sink。</p><p>Bucketing File Sink 是 Flink 提供的一个 FileSystem Connector，用于将数据流写到固定大小的文件里。Bucketing File Sink 将文件分为三种状态，in-progress/pending/committed，分别表示正在写的文件、写完准备提交的文件和已经提交的文件。</p><center><img src="/img/bucketing-file-sink.png" alt="Bucketing File Sink 状态剖析" title="图1. Bucketing File Sink 状态剖析"></center><p>运行时，Bucketing File Sink 首先会打开一个临时文件并不断地将收到的数据写入（相当于事务的 beginTransaction 步骤），这时文件处于 in-progress。直到这个文件因为大小超过阈值或者一段时间内没有新数据写入，这时文件关闭并变为 pending 状态（相当于事务的 pre-commit 步骤）。由于 Flink checkpoint 是异步的，可能有多个并发的 checkpoint，Bucketing File Sink 会记录 pending 文件对应的 checkpoint epoch，当某个 epoch 的 checkpoint 完成后，Bucketing File Sink 会收到 callback 并将对应的文件改为 committed 状态。这是通过原子操作重命名来完成的，因此可以保证 pre-commit 的事务要么 commit 成功要么 commit 失败，不会出现其他中间状态。</p><p>Commit 出现错误会导致作业自动重启，重启后 Bucketing File Sink 本身已被恢复为上次 checkpoint 时的状态，不过仍需要将文件系统的状态也恢复以保证一致性。从 checkpoint 恢复后对应的事务会再次重试 commit，它会将记录的 pending 文件改为 committed 状态，记录的 in-progress 文件 truncate 到 checkpoint 记录下来的 offset，而其余未被记录的 pending 文件和 in-progress 文件都将被删除。</p><p>上面主要围绕事务保证的 AC 两点（Atomicity 和 Consistency），而在 I（Isolation）上 Flink exactly-once sink 也有不同的实现方式。实际上由于 Flink 的流计算特性，当前事务的未 commit 数据是一直在积累的，根据缓存未 commit 数据的地方的不同，可以将事务性 sink 分为两种实现方式。</p><ul><li>在 sink 端缓存未 commit 数据，等 checkpoint 完成以后将缓存的数据 flush 到下游。这种方式可以提供 read-committed 的事务隔离级别，但同时由于未 commit 的数据不会发往下游（与 checkpoint 同步），sink 端缓存会带来一定的延迟，相当于退化为与 checkpoint 同步的 micro-batching 模式。</li><li>在下游系统缓存未 commit 数据，等 checkpoint 完成后通知下游 commit。这样的好处是数据是流式发往下游的，不会在每次 checkpoint 完成后出现网络 IO 的高峰，并且事务隔离级别可以由下游设置，下游可以选择低延迟弱一致性的 read-uncommitted 或高延迟强一致性的 read-committed。</li></ul><p>在 Bucketing File Sink 的例子中，处于 in-progress 和 pending 状态的文件默认情况下都是隐藏文件（在实践中是使用下划线作为文件名前缀，HDFS 的 FileInputFormat 会将其过滤掉），只有 commit 成功后文件才对用户是可见的，即提供了 read-committed 的事务隔离性。理想的情况下 exactly-once sink 都应该使用在下游系统缓存未 commit 数据的方式，因为这最为符合流式计算的理念。最为典型的是下游系统本来就支持事务，那么未 commit 的数据很自然地就是缓存在下游系统的，否则 sink 可以选择像上例的 Bucketing File Sink 一样在下游系统的用户层面实现自己的事务，或者 fallback 到等待数据变为 committed 再发出的 micro-batching 模式。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Exactly-once 是实时系统最为关键的准确性要求，也是当前限制大部分分布式实时系统应用到准确性要求更高的业务场景（比如在线事务处理 OLTP）的问题之一。目前来说流式计算的 exactly-once 在理论上已经有了很大的突破，而 Flink 社区也在积极汲取最先进的思想和实践经验。随着 Flink 在 exactly-once 上的技术愈发成熟，结合 Flink 本身的流处理特性，相信在不远的将来，除了构造数据分析、数据管道应用， Flink 也可以在微服务领域占有一席之地。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/connectors/guarantees.html" target="_blank" rel="external">Fault Tolerance Guarantees of Data Sources and Sinks</a><br>2.<a href="https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html" target="_blank" rel="external">An Overview of End-to-End Exactly-Once Processing in Apache Flink</a><br>3.<a href="http://www.vldb.org/pvldb/vol10/p1718-carbone.pdf" target="_blank" rel="external">State Management in Apache Flink</a><br>4.<a href="https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html" target="_blank" rel="external">An Overview of End-to-End Exactly-Once Processing in Apache Flink (with Apache Kafka, too!)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着近来越来越多的业务迁移到 Flink 上，对 Flink 作业的准确性要求也随之进一步提高，其中最为关键的是如何在不同业务场景下保证 exactly-once 的投递语义。虽然不少实时系统（e.g. 实时计算/消息队列）都宣称支持 exactly-once，exactly-once 投递似乎是一个已被解决的问题，但是其实它们更多是针对内部模块之间的信息投递，比如 Kafka 生产（producer 到 Kafka broker）和消费（broker 到 consumer）的 exactly-once。而 Flink 作为实时计算引擎，在实际场景业务会涉及到很多不同组件，由于组件特性和定位的不同，Flink 并不是对所有组件都支持 exactly-once（见[1]），而且不同组件实现 exactly-once 的方法也有所差异，有些实现或许会带来副作用或者用法上的局限性，因此深入了解 Flink exactly-once 的实现机制对于设计稳定可靠的架构有十分重要的意义。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink Checkpoint/Savepoint 差异</title>
    <link href="https://link3280.github.io/2018/09/06/Flink-Checkpoint-Savepoint-%E5%B7%AE%E5%BC%82/"/>
    <id>https://link3280.github.io/2018/09/06/Flink-Checkpoint-Savepoint-差异/</id>
    <published>2018-09-06T11:50:09.000Z</published>
    <updated>2019-04-14T13:05:18.365Z</updated>
    
    <content type="html"><![CDATA[<p>Flink 为作业的容错提供 Checkpoint 和 Savepoint 两种机制，而这两者在无论在命名还是使用上都十分相似，很容易令用户混淆，因此 Checkpoint 和 Savepoint 有何区别也是 Flink 社区常见的问题之一。除开生产环境不常见的内存 Checkpoint，External Checkpoint 和 Savepoint 都是作业状态（State）的持久化副本，也理所当然地可以用于作业恢复，甚至在提交作业时指定状态的参数都可以两者通用。那么它们究竟有什么不同呢？</p><a id="more"></a><center><img src="/img/flink-fault-tolerance/flink-fault-tolerance-4_savepoint-and-checkpoint.png" alt="Flink Checkpoint &amp; Savepoint" title="Flink Checkpoint &amp; Savepoint"></center><p>先来看 Checkpoint 的定义: </p><blockquote><p>Checkpoints make state in Flink fault tolerant by allowing state and the corresponding stream positions to be recovered, thereby giving the application the same semantics as a failure-free execution.</p></blockquote><p>再比较 Savepoint 的定义: </p><blockquote><p>Savepoints are externally stored self-contained checkpoints that you can use to stop-and-resume or update your Flink programs. They use Flink’s checkpointing mechanism to create a (non-incremental) snapshot of the state of your streaming program and write the checkpoint data and meta data out to an external file system.</p></blockquote><p>首先最容易注意到的是 Savepoint 是一种特殊的 Checkpoint，实际上它们的存储格式也是一致的，它们主要的不同在于定位。Checkpoint 机制的目标在于保证 Flink 作业意外崩溃重启不影响 exactly once 准确性，通常是配合作业重启策略使用的。而 Savepoint 的目的在于在 Flink 作业维护（比如更新作业代码）时将作业状态写到外部系统，以便维护结束后重新提交作业可以到恢复原本的状态。换句话讲，Checkpoint 是为 Flink runtime 准备的，Savepoint 是为 Flink 用户准备的。因此 Checkpoint 是由 Flink runtime 定时触发并根据运行配置自动清理的，一般不需要用户介入，而 Savepoint 的触发和清理都由用户掌控。</p><p>其次，由于 Checkpoint 的频率远远大于 Savepoint，Flink 对 Checkpoint 格式进行了针对不同 StateBackend 的优化，因此它在底层储存效率更高，而代价是耦合性更强，比如不保证 rescaling （即改变作业并行度）的特性和跨版本兼容。这里说”不保证”而不是”不支持”，原因是实际上 RocksDB 的 Checkpoint 是支持 rescaling 的，”不保证”更多是从系统设计出发而言。跨版本兼容性也同理。</p><p>最后，Savepoint 的定义有提及它是 non-incremental 的，这是相对于 incremental Checkpoint 来说。因为 Checkpoint 是秒级频繁触发的，两个连续 Checkpoint 通常高度相似，因此对于 State 特别大的作业来说，每次 Checkpoint 只增量地补充 diff 可以大大地节约成本，这就是 incremental Checkpoint 的由来。而 Savepoint 并不会连续地触发，而且比起效率，Savepoint 更关注的是可移植性和版本兼容性，所以不支持 incremental 也是理所当然。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Flink 为作业的容错提供 Checkpoint 和 Savepoint 两种机制，而这两者在无论在命名还是使用上都十分相似，很容易令用户混淆，因此 Checkpoint 和 Savepoint 有何区别也是 Flink 社区常见的问题之一。除开生产环境不常见的内存 Checkpoint，External Checkpoint 和 Savepoint 都是作业状态（State）的持久化副本，也理所当然地可以用于作业恢复，甚至在提交作业时指定状态的参数都可以两者通用。那么它们究竟有什么不同呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink/Spark 如何实现动态更新作业配置</title>
    <link href="https://link3280.github.io/2018/08/19/Flink-Spark-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E6%9B%B4%E6%96%B0%E4%BD%9C%E4%B8%9A%E5%8F%98%E9%87%8F/"/>
    <id>https://link3280.github.io/2018/08/19/Flink-Spark-如何实现动态更新作业变量/</id>
    <published>2018-08-19T12:47:21.000Z</published>
    <updated>2019-04-14T13:05:07.105Z</updated>
    
    <content type="html"><![CDATA[<p>由于实时场景对可用性十分敏感，实时作业通常需要避免频繁重启，因此动态加载作业配置（变量）是实时计算里十分常见的需求，比如通常复杂事件处理 (CEP) 的规则或者在线机器学习的模型。尽管常见，实现起来却并没有那么简单，其中最难点在于如何确保节点状态在变更期间的一致性。目前来说一般有两种实现方式：</p><ul><li>轮询拉取方式，即作业算子定时检测在外部系统的配置是否有变更，若有则同步配置。</li><li>控制流方式，即作业除了用于计算的一个或多个普通数据流以外，还有提供一个用于改变作业算子状态的元数据流，也就是控制流。</li></ul><a id="more"></a><p>轮询拉取方式基于 pull 模式，一般实现是用户在 Stateful 算子(比如 RichMap)里实现后台线程定时从外部系统同步变量。这种方式对于一般作业或许足够，但存在两个缺点分别限制了作业的实时性和准确性的进一步提高：首先，轮询总是有一定的延迟，因此变量的变更不能第一时间生效；其次，这种方式依赖于节点本地时间来进行校准。如果在同一时间有的节点已经检测到变更并更新状态，而有的节点还没有检测到或者还未更新，就会造成短时间内的不一致。</p><p>控制流方式基于 push 模式，变更的检测和节点更新的一致性都由计算框架负责，从用户视角看只需要定义如何更新算子状态并负责将控制事件丢入控制流，后续工作计算框架会自动处理。控制流不同于其他普通数据流的地方在于控制流是以广播形式流动的，否则在有 Keyby 或者 rebalance 等提高并行度分流的算子的情况下就无法将控制事件传达给所有的算子。</p><p>以目前最流行的两个实时计算框架 Spark Streaming 和 Flink 来说，前者是以类似轮询的方式来实现实时作业的更新，而后者则是基于控制流的方式。</p><h2 id="Spark-Streaming-Broadcast-Variable"><a href="#Spark-Streaming-Broadcast-Variable" class="headerlink" title="Spark Streaming Broadcast Variable"></a>Spark Streaming Broadcast Variable</h2><p>Spark Streaming 为用户提供了 Broadcast Varialbe，可以用于节点算子状态的初始化和后续更新。Broacast Variable 是一组只读的变量，它在作业初始化时由 Spark Driver 生成并广播到每个 Executor 节点，随后该节点的 Task 可以复用同一份变量。</p><p>Broadcast Variable 的设计初衷是为了避免大文件，比如 NLP 常用的分词词典，随序列化后的作业对象一起分发，造成重复分发的网络资源浪费和启动时间延长。这类文件的更新频率是相对低的，扮演的角色类似于只读缓存，通过设置 TTL 来定时更新，缓存过期之后 Executor 节点会重新向 Driver 请求最新的变量。</p><p>Broadcast Variable 并不是从设计理念上就支持低延迟的作业状态更新，因此用户想出了不少 Hack 的方法，其中最为常见的方式是：一方面在 Driver 实现后台线程不断更新 Broadcast Variavle，另一方面在作业运行时通过显式地删除 Broadcast Variable 来迫使 Executor 重新从 Driver 拉取最新的 Broadcast Variable。这个过程会发生在两个 micro batch 计算之间，以确保每个 micro batch 计算过程中状态是一致的。</p><p>比起用户在算子内访问外部系统实现更新变量，这种方式的优点在于一致性更有保证。因为 Broadcast Variable 是统一由 Driver 更新并推到 Executor 的，这就保证不同节点的更新时间是一致的。然而相对地，缺点是会给 Driver 带来比较大的负担，因为需要不断分发全量的 Broadcast Variable (试想下一个巨大的 Map，每次只会更新少数 Entry，却要整个 Map 重新分发)。在 Spark 2.0 版本以后，Broadcast Variable 的分发已经从 Driver 单点改为基于 BitTorrent 的 P2P 分发，这一定程度上缓解了随着集群规模提升 Driver 分发变量的压力，但我个人对这种方式能支持到多大规模的部署还是持怀疑态度。另外一点是重新分发 Broadcast Variable 需要阻塞作业进行，这也会使作业的吞吐量和延迟受到比较大的影响。</p><h2 id="Flink-Broadcast-State-amp-Stream"><a href="#Flink-Broadcast-State-amp-Stream" class="headerlink" title="Flink Broadcast State &amp; Stream"></a>Flink Broadcast State &amp; Stream</h2><p>Broadcast Stream 是 Flink 1.5.0 发布的新特性，基于控制流的方式实现了实时作业的状态更新。Broadcast Stream 的创建方式与普通数据流相同，例如从 Kafka Topic 读取，特别之处在于它承载的是控制事件流，会以广播形式将数据发给下游算子的每个实例。Broadcast Stream 需要在作业拓扑的某个节点和普通数据流 (Main Stream) join 到一起。</p><p><img src="/img/control-stream.png" alt="Control Stream Topo"></p><p>该节点的算子需要同时处理普通数据流和控制流：一方面它需要读取控制流以更新本地状态 (Broadcast State)，另外一方面需要读取 Main Stream 并根据 Broadcast State 来进行数据转换。由于每个算子实例读到的控制流都是相同的，它们生成的 Broadcast State 也是相同的，从而达到通过控制消息来更新所有算子实例的效果。</p><p>目前 Flink 的 Broadcast Stream 从效果上实现了控制流的作业状态更新，不过在编程模型上有点和一般直觉不同。原因主要在于 Flink 对控制流的处理方式和普通数据流保持了一致，最为明显的一点是控制流除了改变本地 State 还可以产生 output，这很大程度上影响了 Broadcast Stream 的使用方式。Broadcast Stream 的使用方式与普通的 DataStream 差别比较大，即需要和 DataStream 连接成为 BroadcastConnectedStream 后，再通过特殊的 BroadcastProcessFunction 来处理，而 BroadcastProcessFunction 目前只支持 类似于 RichCoFlatMap 效果的操作。RichCoFlatMap 可以间接实现对 Main Stream 的 Map 转换（返回一只有一个元素的集合）和 Filter 转换（返回空集合），但无法实现 Window 类计算。这意味着如果用户希望改变 Window 算子的状态，那么需要将状态管理提前到上游的 BroadcastProcessFunction，然后再通过 BroadcastProcessFunction 的输出来将影响下游 Window 算子的行为。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>实时作业运行时动态加载变量可以令大大提升实时作业的灵活性和适应更多应用场景，目前无论是 Flink 还是 Spark Streaming 对动态加载变量的支持都不是特别完美。Spark Streaming 受限于 Micro Batch 的计算模型（虽然现在 2.3 版本引入 Continuous Streaming 来支持流式处理，但离成熟还需要一定时间），将作业变量作为一致性和实时性要求相对低的节点本地缓存，并不支持低延迟地、低成本地更新作业变量。Flink 将变量更新视为特殊的控制事件流，符合 Even Driven 的流式计算框架定位，目前在业界已有比较成熟的应用。不过美中不足的是编程模型的易用性上有提高空间：控制流目前只能用于和数据流的 join，这意味着下游节点无法继续访问控制流或者需要把控制流数据插入到数据流中（这种方式并不优雅），从而降低了编程模型的灵活性。个人认为最好的情况是大部分的算子都可以被拓展为具有 BroadcastOperator，就像 RichFunction 一样，它们可以接收一个数据流和一个至多个控制流，并维护对应的 BroadcastState，这样控制流的接入成本将显著下降。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-17+Side+Inputs+for+DataStream+API" target="_blank" rel="external">FLIP-17 Side Inputs for DataStream API</a><br>2.<a href="https://data-artisans.com/blog/bettercloud-dynamic-alerting-apache-flink" target="_blank" rel="external">Dynamically Configured Stream Processing: How BetterCloud Built an Alerting System with Apache Flink®</a><br>3.<a href="https://mux.com/blog/using-control-streams-to-manage-apache-flink-applications/" target="_blank" rel="external">Using Control Streams to Manage Apache Flink Applications</a><br>4.<a href="https://stackoverflow.com/questions/33372264/how-can-i-update-a-broadcast-variable-in-spark-streaming" target="_blank" rel="external">StackOverFlow - ow can I update a broadcast variable in spark streaming?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于实时场景对可用性十分敏感，实时作业通常需要避免频繁重启，因此动态加载作业配置（变量）是实时计算里十分常见的需求，比如通常复杂事件处理 (CEP) 的规则或者在线机器学习的模型。尽管常见，实现起来却并没有那么简单，其中最难点在于如何确保节点状态在变更期间的一致性。目前来说一般有两种实现方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;轮询拉取方式，即作业算子定时检测在外部系统的配置是否有变更，若有则同步配置。&lt;/li&gt;
&lt;li&gt;控制流方式，即作业除了用于计算的一个或多个普通数据流以外，还有提供一个用于改变作业算子状态的元数据流，也就是控制流。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Spark" scheme="https://link3280.github.io/tags/Spark/"/>
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 内存管理机制</title>
    <link href="https://link3280.github.io/2018/07/07/Flink-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/"/>
    <id>https://link3280.github.io/2018/07/07/Flink-内存管理机制/</id>
    <published>2018-07-07T03:31:02.000Z</published>
    <updated>2019-04-14T13:05:26.808Z</updated>
    
    <content type="html"><![CDATA[<p>Flink 作为一个基于内存的分布式计算引擎，其内存管理模块很大程度上决定了系统的效率和稳定性，尤其对于实时流式计算，JVM GC 带来的微小延迟也有可能被业务感知到。针对这个问题，Flink 实现了一套较为优雅的内存管理机制，可以在引入小量访问成本的情况下提高内存的使用效率并显著降低 GC 成本和 OOM 风险，令用户可以通过少量的简单配置即可建立一个健壮的数据处理系统。</p><a id="more"></a><h2 id="大数据时代的-JVM"><a href="#大数据时代的-JVM" class="headerlink" title="大数据时代的 JVM"></a>大数据时代的 JVM</h2><p>众所周知，以 Hadoop 为核心的开源大数据生技术栈组件绝大多数运行在 JVM 之上，而为了性能考虑越来越多的后起之秀选择将数据存储到内存（比如 Spark、HBase、Drill，当然还有 Flink ），常常是一个进程占用十几GB甚至上百GB内存，这就触及并突出了 JVM 在 heap 内存管理上的几个短板:   </p><ol><li>潜在的 OOM 风险。在有大量对象被创建和清理的情况下，监控 JVM heap 内存的使用并不是一件轻松的事情，稍有不慎可能会触发 OOM 错误导致进程崩溃。  </li><li>GC 成本高，调优困难。Java 垃圾回收器普遍会将 heap 分为 young 和 old 两个区并基于新生代区存放短期对象、老年代区存放长期对象的假设来分别实施不同的 GC 策略。然而各区的比例是启发性和基于经验估计的，对于有数十 GB 甚至上百 GB 并伴随着大量新生对象的 JVM 来说，新生对象可能会溢出到老年代，这可能导致 GC 成本消耗50%甚至更高的性能。虽然垃圾回收器提供数十个参数给用户进行调优，然而并没有静态的参数可以完美适应运行时动态的负载。  </li><li>对象的存储密度低。Java 对象的 header、padding 都需要占用额外的空间，在存储大量小对象的情况下尤其低效。  </li></ol><p>针对这些问题的解决方案根据组件不同有所差异，不过核心思想是一致的：在 JVM 堆内或堆外实现显式的内存管理，即用自定义内存池来进行内存块的分配和回收，并将对象序列化后存储到内存块，比如 Spark 的 Project Tungsten 和 HBase 的 BlockCache。因为内存可以被精准地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以组件可以对内存有更好的掌控，这种内存管理方式也被认为是相比 Java 更像 C 语言化的做法。  </p><p>Flink 的内存管理也并不例外，采用了显式的内存管理并用序列化方式存储对象，同时支持 on-heap 和 off-heap。在这点上 Flink 与 Spark 的<code>MEMORY_ONLY_SER</code>存储级别十分相似，不同点在于 Spark 仍以 on-heap 对象存储为主，而 Flink 则只支持序列化的对象存储。</p><h2 id="Flink-内存管理概览"><a href="#Flink-内存管理概览" class="headerlink" title="Flink 内存管理概览"></a>Flink 内存管理概览</h2><p>Flink 内存主要指 TaskManager 运行时提供的内存资源。TaskManager 主要由几个内部组件构成: 负责和 JobManager 等进程通信的 actor 系统，负责在内存不足时将数据溢写到磁盘和读回的 IOManager，还有负责内存管理的 MemoryManager。其中 actor 系统和 MemoryManager 会要求大量的内存。相应地，Flink 将 TaskManager 的运行时 JVM heap 分为 Network Buffers、MemoryManager 和 Free 三个区域（在 streaming 模式下只存在 Network Buffers 和 Free 两个区域，因为算子不需要缓存一次读入的大量数据）。</p><center><p><img src="/img/flink-memory-management/HeapDivision.png" alt=""></p></center><p>各个区域的功能如下:  </p><ul><li>Network Buffers 区: 网络模块用于网络传输的一组缓存块对象，单个缓存块对象默认是32KB大小。Flink 会根据 TaskManager 的最大内存来计算该区大小，默认范围是64MB至1GB。  </li><li>Memory Manager 区: 用于为算子缓存运行时消息记录的大缓存池（比如 Sort、Join 这类耗费大量内存的操作），消息记录会被序列化之后存进这些缓存块对象。这部分区域默认占最大 heap 内存减去 Network Buffers 后的70%，单个缓存块同样默认是32KB。  </li><li>Free 区: 除去上述两个区域的内存剩余部分便是 Free heap，这个区域用于存放用户代码所产生的数据结构，比如用户定义的 State。  </li></ul><p>目前 Memory Manager 的内存初始化方式有两种: 第一种是启动时即为 Network Buffers 区和 MemoryManager 区分配全部内存，这样 TaskManager 启动过程中会产生一次到多次的 full GC，导致 TaskManager 的启动慢一点，但是节省了后续执行作业时的 GC 时长。第二种方式是采用”懒分配”的方法，在内存紧张时再增量向操作系统申请内存，避免一下吃完所有的内存导致后续的其他操作内存不足，例如流计算作业的 StateBackend 保存在内存的 State 对象。  </p><p>Network Buffers 和 MemoryManager 的存在会贯穿 TaskManager 的整个生命周期。它们管理的 Memory Segment 不断被重用，因此不会被 JVM 回收。经过若干次 GC 之后它们会进入老年代，变成常驻的对象。  </p><h2 id="Memory-Segment"><a href="#Memory-Segment" class="headerlink" title="Memory Segment"></a>Memory Segment</h2><p>Memory Segment 是 Flink 内存管理的核心概念，是在 JVM 内存上的进一步抽象（包括 on-heap 和 off-heap），代表了 Flink Managed Memory 分配的单元。每个 Memory Segment 默认占32KB，支持存储和访问不同类型的数据，包括 long、int、byte、数组等。你可以将 Memory Segment 想象为 Flink 版本的 <code>java.nio.ByteBuffer</code>。  </p><p>不管消息数据实际存储在 on-heap 还在是 off-heap，Flink 都会将它序列化成为一个或多个的 Memory Segment（内部又称 page）。系统可能会在其他的数据结构里存指向条消息的指针（一条消息通常会被构造成一个 Memory Segment 的集合），这意味着 Flink 需要依赖于一个有 page 概念并支持跨 page 消息的高效率序列化器。因此 Flink 实现了自己的类型信息系统（Type Information System）和序列化栈。  </p><center><p><img src="/img/flink-memory-management/RecordsInPages.png" alt=""></p></center><p>序列化的格式由 Flink 序列化器定义，并且可以描述到消息记录的单个字段，这意味着 Flink 只需要序列化目标字段而不是整个对象。这点非常重要，因为 Flink 的输入数据是以序列化方式存储的，在访问时需要反序列化，这在减小了存储空间的同时会带来一定的计算开销，所以提升序列化和反序列化(SerDe)效率可以明显提高整体性能。  </p><p>熟悉 Spark 的同学看到这里可能会联想到 Spark 的 Catalyst，它可以通过运行时编译来生成 Java bytecode 并提供了直接访问二进制对象字段的能力。然而 Flink 的开发语言是 Java，不便利用 Scala 的 quasiquotes 特性，因此 Flink 采用的办法是使用定制化的序列化机制和类型系统。</p><h2 id="Flink-序列化机制-amp-类型信息系统"><a href="#Flink-序列化机制-amp-类型信息系统" class="headerlink" title="Flink 序列化机制 &amp; 类型信息系统"></a>Flink 序列化机制 &amp; 类型信息系统</h2><p>Java 生态圈实际上已经有不少出色的序列化库，包括 Kryo、Apache Avro、Apache Thrift 和 Google 的 ProtoBuf，然而 Flink 毅然重新造了一套轮子以定制数据的二进制格式。这带来了三点重要的优势：其一，掌握了对序列化后的数据结构信息，使得二进制数据间的比较甚至于直接操作二进制数据成为可能；其二，Flink 依据计划要执行的操作来提前优化序列化结构，极大地提高了性能；其三，Flink 可以在作业执行之前确定对象的类型，并在序列化时利用这个信息进行优化。  </p><p>Flink 作业可以处理任意 Java 或 Scala 对象类型的数据。要对作业执行计划进行优化，首先要识别出作业数据流的每一个步骤的数据类型。对于 Java 程序，Flink 实现了基于反射的类型提取器来分析用户定义函数的返回类型；对于 Scala 程序，Flink 可以直接利用 Scala 编译器的类型推导特性。其后 Flink 会用<code>TypeInformation</code>作为类型描述符来表示每种数据类型。  </p><p>Flink 类型系统存下以下内建分类:  </p><ul><li>基础类型: 所有 Java 原始类型和它们的包装类型（包括 Hadoop Writable 类型），加上<code>void</code>、<code>String</code>、<code>Date</code>、<code>BigDecimal</code>和<code>BigInteger</code>。  </li><li>原始类型的数组和对象数组。  </li><li>复合类型:   <ul><li>Flink Java Tuple (Flink Java API 的一部分): 最多支持25个字段，不支持 null 字段。</li><li>Scala case classe (包括 Scala 元组): 最多支持22个字段，不支持 null 字段。</li><li>Row: Flink API 对象，代表任意数量字段的元组，支持 null 字段。</li><li>POJO: 遵循 Java Bean 模式的类。  </li></ul></li><li>辅助类型: <code>Option</code>、<code>Either</code>、<code>Lists</code>、<code>Maps</code>等等。</li><li>通用类型: 不能被识别为以上任意类型的数据类型。Flink 不会自己序列化这些类型，而是交由 Kyro 序列化。</li></ul><p>每个数据类型都会有专属的序列化器。举个例子，<code>BasicTypeInfo</code>(基础类型)的序列化器为<code>TypeSerializer&lt;T&gt;</code>，用于序列化 T 原始类型的数据；<code>WritableTypeInfo</code>(Hadoop Writable类型)的序列化器为<code>WritableSerializer&lt;T&gt;</code>，用于序列化实现类 Writable 接口的 T 类型的数据。<br>其中比较特别的是<code>GenericTypeInfo</code>，作为后备计划的它会委托 Kyro 进行序列化。</p><p>对象序列化后会写到 DataOutput (由 MemorySegement 所支持)，并高效地自动通过 Java 的 Unsafe API 操作写到内存。对于作为 key 的数据类型，<code>TypeInformation</code>还会提供类型比较器(<code>TypeComparator</code>)。类型比较器负责对对象进行哈希和比较，取决于具体的数据类型还可以高效地比较二进制形式的对象和提取固定长度的 key 前缀。  </p><p>复合类型的对象可能包含内嵌的数据类型，在这种情况下，它们的序列化器和类型比较器同样是复合的。它们会将内嵌类型的序列化和比较大小任务委托给对应类型的序列化器和类型比较器。下图描述了一个典型的复合类型对象<code>Tuple&lt;Integer, Double, Person&gt;</code>是如何序列化和存储的:  </p><center><p><img src="/img/flink-memory-management/data-serialization.png" alt=""></p></center><p>可以看出序列化后的对象存储是非常紧凑的，POJO 首部序列化后仅占1字节的空间，String 这种不固定长度的对象也以实际长度来存储。如同上文所讲，一个对象并不要求完整地存放在一个 MemorySegment 内，而是可以跨 MemorySegment 存放，这意味着将不会有内存碎片产生。</p><p>最后，如果内建的数据类型和序列化方式不能满足你的需求，Flink 的类型信息系统也支持用户拓展。用户只需要实现<code>TypeInformation</code>、<code>TypeSerializer</code>和<code>TypeComparator</code>即可定制自己类型的序列化和比较大小方式。  </p><h2 id="对-GC-的影响"><a href="#对-GC-的影响" class="headerlink" title="对 GC 的影响"></a>对 GC 的影响</h2><p>作为简单回顾，Flink 不会将消息记录当作对象直接放到 heap 上，而是序列化后存在长期缓存对象里。这意味着将不会出现低效的短期对象，消息对象只用来在用户函数内传递和被序列化。而长期对象是 MemorySegment 本身，它们并不会被 GC 清理。  </p><p>因此在 JVM 内存结构规划上，Flink 也作了相应的调整: MemoryManager 和 Network Buffers 两个实现了显式内存管理的子系统分配到老年代，而留给用户代码的 Free 区域分配到新生代，见下图。</p><center><p><img src="/img/flink-memory-management/GC_Pools.png" alt=""></p></center><p>JVM 参数中控制新生代和老年代比例的参数是<code>-XX:NewRatio</code>，这表示了老年代空间与新生代空间之比，默认为2，即（不计Meta区）新生代占 heap 的三分之一，老年代占 heap 的三分之二。  </p><p>为了证明 Flink 内存管理和序列化器的优势，Flink 官方对 Object-on-Heap (直接 Java 对象存储)、Flink-Serialized (内建序列化器 + 显式内存管理)和 Kryo-Serialized (Kryo 序列化器 + 显式内存管理)三种方案进行了 GC 表现的对比测试。</p><p>测试方法是对一千万个 <code>Tuple2&lt;Integer, String&gt;</code> 对象进行排序，其中 Integer 字段的值是均匀分布的，String 字段是长度为12的字符串并服从长尾分布。测试的作业跑在 heap 为900MB的 JVM 内，这恰好是排序一千万个对象所需要的最低内存。</p><p>测试在 JVM GC 上的表现如下图: </p><center><p><img src="/img/flink-memory-management/GC-performance.png" alt=""></p></center><p>显而易见，使用显式内存管理可以显著地减少 GC 频率。在 Object-on-Heap 的测试中，GC 频繁地被触发并导致 CPU 峰值达到90%。在测试中使用的8核机器上单线程的作业最多只占用12.5%的 CPU ，机器花费在 GC 的成本显然超过了实际运行作业的成本。而在另外两个依赖显式内存管理和序列化的测试中，GC 很少被触发，CPU 使用率也一直稳定在较低的水平。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Flink 在内存管理上较大程度上借鉴了 Spark 的方案，包括存储不足时的溢写机制和内存区域的划分。不过 Flink 的管理粒度更细更精确，最为典型的一点是 Spark 的序列化存储是以 RDD 的一个 Partiton 为单位，而 Flink 的序列化则是以消息记录为单位，这也体现了两者最大的区别： Spark 的哲学是将 Streaming 视作 Batch 的特例 (micro batch) ，相反地，Flink 的哲学是将 Batch 视作 Streaming 的特例。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html" target="_blank" rel="external">Juggling-with-Bits-and-Bytes</a><br>2.<a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=53741525" target="_blank" rel="external">Memory Management (Batch API)</a><br>3.<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.5/dev/types_serialization.html#flinks-typeinformation-class" target="_blank" rel="external">flinks-typeinformation-class</a><br>4.<a href="http://wuchong.me/blog/2016/04/29/flink-internals-memory-manage/" target="_blank" rel="external">Flink 原理与实现：内存管理</a><br>5.<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="external">Apache Spark 内存管理详解</a><br>6.<a href="http://hbasefly.com/2016/04/08/hbase-blockcache-1/" target="_blank" rel="external">HBase BlockCache系列 – 走进BlockCache</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Flink 作为一个基于内存的分布式计算引擎，其内存管理模块很大程度上决定了系统的效率和稳定性，尤其对于实时流式计算，JVM GC 带来的微小延迟也有可能被业务感知到。针对这个问题，Flink 实现了一套较为优雅的内存管理机制，可以在引入小量访问成本的情况下提高内存的使用效率并显著降低 GC 成本和 OOM 风险，令用户可以通过少量的简单配置即可建立一个健壮的数据处理系统。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="https://link3280.github.io/categories/Flink/"/>
    
    
      <category term="Flink" scheme="https://link3280.github.io/tags/Flink/"/>
    
  </entry>
  
</feed>
